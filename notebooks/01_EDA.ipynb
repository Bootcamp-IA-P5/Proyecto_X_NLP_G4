{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - Hate Speech Detection\n",
    "\n",
    "Este notebook contiene el an\u00e1lisis exploratorio de datos (EDA) para el proyecto de detecci\u00f3n de mensajes de odio en comentarios de YouTube.\n",
    "\n",
    "## Objetivos del EDA:\n",
    "1. Cargar y explorar el dataset de comentarios\n",
    "2. Analizar la distribuci\u00f3n de clases (mensajes de odio vs. mensajes normales)\n",
    "3. Explorar estad\u00edsticas de texto (longitud, palabras, caracteres)\n",
    "4. Identificar patrones y caracter\u00edsticas del texto\n",
    "5. Detectar valores faltantes y problemas de calidad de datos\n",
    "6. Visualizar las palabras m\u00e1s frecuentes\n",
    "7. Analizar n-gramas\n",
    "8. Identificar insights para el preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar Librer\u00edas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer\u00edas b\u00e1sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuraci\u00f3n de visualizaci\u00f3n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Tama\u00f1o de figuras por defecto\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\u2713 Librer\u00edas importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer\u00edas de NLP\n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLTK (descomentar para descargar recursos necesarios)\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "print(\"\u2713 Librer\u00edas NLP importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar Datos\n",
    "\n",
    "**Nota:** Aseg\u00farate de colocar tu dataset en la carpeta `data/raw/` con el nombre adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al dataset\n",
    "# Ajusta el nombre del archivo seg\u00fan tu dataset\n",
    "DATA_PATH = Path('../data/raw/')\n",
    "\n",
    "# Ejemplo de carga (ajustar seg\u00fan el formato real del dataset)\n",
    "# df = pd.read_csv(DATA_PATH / 'youtube_comments.csv')\n",
    "\n",
    "# Para este ejemplo, mostraremos c\u00f3mo ser\u00eda la estructura esperada\n",
    "print(f\"Ruta de datos: {DATA_PATH}\")\n",
    "print(\"\\nEstructura esperada del dataset:\")\n",
    "print(\"- Columna 'text' o 'comment': texto del comentario\")\n",
    "print(\"- Columna 'label' o 'class': etiqueta (0: normal, 1: hate speech)\")\n",
    "\n",
    "# Descomenta y ajusta seg\u00fan tu dataset:\n",
    "# df = pd.read_csv(DATA_PATH / 'tu_archivo.csv')\n",
    "# print(f\"\\n\u2713 Dataset cargado: {len(df)} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploraci\u00f3n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeras filas\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci\u00f3n general del dataset\n",
    "# print(\"=\" * 50)\n",
    "# print(\"INFORMACI\u00d3N DEL DATASET\")\n",
    "# print(\"=\" * 50)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad\u00edsticas descriptivas\n",
    "# df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones del dataset\n",
    "# print(f\"N\u00famero de filas: {df.shape[0]}\")\n",
    "# print(f\"N\u00famero de columnas: {df.shape[1]}\")\n",
    "# print(f\"\\nColumnas: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. An\u00e1lisis de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores faltantes\n",
    "# print(\"=\" * 50)\n",
    "# print(\"VALORES FALTANTES\")\n",
    "# print(\"=\" * 50)\n",
    "# missing = df.isnull().sum()\n",
    "# missing_pct = (missing / len(df)) * 100\n",
    "# missing_df = pd.DataFrame({\n",
    "#     'Missing_Count': missing,\n",
    "#     'Missing_Percentage': missing_pct\n",
    "# })\n",
    "# print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Visualizaci\u00f3n de valores faltantes\n",
    "# if missing.sum() > 0:\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "#     plt.title('Mapa de Valores Faltantes')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distribuci\u00f3n de Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An\u00e1lisis de la distribuci\u00f3n de clases\n",
    "# print(\"=\" * 50)\n",
    "# print(\"DISTRIBUCI\u00d3N DE CLASES\")\n",
    "# print(\"=\" * 50)\n",
    "# class_dist = df['label'].value_counts()\n",
    "# print(class_dist)\n",
    "# print(f\"\\nPorcentaje de cada clase:\")\n",
    "# print(df['label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualizaci\u00f3n\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# # Gr\u00e1fico de barras\n",
    "# class_dist.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "# axes[0].set_title('Distribuci\u00f3n de Clases', fontsize=14, fontweight='bold')\n",
    "# axes[0].set_xlabel('Clase')\n",
    "# axes[0].set_ylabel('Cantidad')\n",
    "# axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# # Gr\u00e1fico de pastel\n",
    "# axes[1].pie(class_dist, labels=class_dist.index, autopct='%1.1f%%',\n",
    "#             colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "# axes[1].set_title('Proporci\u00f3n de Clases', fontsize=14, fontweight='bold')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Balance de clases\n",
    "# imbalance_ratio = class_dist.max() / class_dist.min()\n",
    "# print(f\"\\n\u26a0\ufe0f Ratio de desbalance: {imbalance_ratio:.2f}\")\n",
    "# if imbalance_ratio > 2:\n",
    "#     print(\"El dataset est\u00e1 desbalanceado. Considerar t\u00e9cnicas de balanceo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An\u00e1lisis de Texto\n",
    "\n",
    "### 6.1 Estad\u00edsticas B\u00e1sicas del Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares para an\u00e1lisis de texto\n",
    "def get_text_stats(text):\n",
    "    \"\"\"Obtiene estad\u00edsticas b\u00e1sicas de un texto\"\"\"\n",
    "    return {\n",
    "        'char_count': len(str(text)),\n",
    "        'word_count': len(str(text).split()),\n",
    "        'avg_word_length': np.mean([len(word) for word in str(text).split()]),\n",
    "        'sentence_count': len(str(text).split('.')),\n",
    "        'uppercase_count': sum(1 for c in str(text) if c.isupper()),\n",
    "        'special_char_count': sum(1 for c in str(text) if not c.isalnum() and not c.isspace())\n",
    "    }\n",
    "\n",
    "# # Aplicar funciones\n",
    "# df['char_count'] = df['text'].apply(lambda x: len(str(x)))\n",
    "# df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "# df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\n",
    "\n",
    "# print(\"\u2713 Estad\u00edsticas de texto calculadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad\u00edsticas descriptivas de las m\u00e9tricas de texto\n",
    "# print(\"=\" * 50)\n",
    "# print(\"ESTAD\u00cdSTICAS DE TEXTO\")\n",
    "# print(\"=\" * 50)\n",
    "# text_stats = df[['char_count', 'word_count', 'avg_word_length']].describe()\n",
    "# print(text_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci\u00f3n de distribuciones de texto\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# # Distribuci\u00f3n de longitud de caracteres\n",
    "# axes[0, 0].hist(df['char_count'], bins=50, edgecolor='black', alpha=0.7)\n",
    "# axes[0, 0].set_title('Distribuci\u00f3n de Longitud de Caracteres', fontweight='bold')\n",
    "# axes[0, 0].set_xlabel('N\u00famero de Caracteres')\n",
    "# axes[0, 0].set_ylabel('Frecuencia')\n",
    "\n",
    "# # Distribuci\u00f3n de cantidad de palabras\n",
    "# axes[0, 1].hist(df['word_count'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "# axes[0, 1].set_title('Distribuci\u00f3n de Cantidad de Palabras', fontweight='bold')\n",
    "# axes[0, 1].set_xlabel('N\u00famero de Palabras')\n",
    "# axes[0, 1].set_ylabel('Frecuencia')\n",
    "\n",
    "# # Boxplot de longitud por clase\n",
    "# df.boxplot(column='char_count', by='label', ax=axes[1, 0])\n",
    "# axes[1, 0].set_title('Longitud de Caracteres por Clase', fontweight='bold')\n",
    "# axes[1, 0].set_xlabel('Clase')\n",
    "# axes[1, 0].set_ylabel('N\u00famero de Caracteres')\n",
    "# plt.suptitle('')\n",
    "\n",
    "# # Boxplot de palabras por clase\n",
    "# df.boxplot(column='word_count', by='label', ax=axes[1, 1])\n",
    "# axes[1, 1].set_title('Cantidad de Palabras por Clase', fontweight='bold')\n",
    "# axes[1, 1].set_xlabel('Clase')\n",
    "# axes[1, 1].set_ylabel('N\u00famero de Palabras')\n",
    "# plt.suptitle('')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 An\u00e1lisis de Palabras Frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci\u00f3n para limpiar y tokenizar texto\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpia el texto para an\u00e1lisis b\u00e1sico\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-z\u00e1\u00e9\u00ed\u00f3\u00fa\u00f1\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def get_top_words(texts, n=20):\n",
    "    \"\"\"Obtiene las palabras m\u00e1s frecuentes\"\"\"\n",
    "    words = ' '.join(texts).split()\n",
    "    word_freq = Counter(words)\n",
    "    return word_freq.most_common(n)\n",
    "\n",
    "# # Aplicar limpieza\n",
    "# df['text_clean'] = df['text'].apply(clean_text)\n",
    "# print(\"\u2713 Texto limpiado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top palabras generales\n",
    "# print(\"=\" * 50)\n",
    "# print(\"TOP 20 PALABRAS M\u00c1S FRECUENTES\")\n",
    "# print(\"=\" * 50)\n",
    "# top_words = get_top_words(df['text_clean'])\n",
    "# for word, count in top_words:\n",
    "#     print(f\"{word:20s}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci\u00f3n de palabras frecuentes\n",
    "# top_words_df = pd.DataFrame(top_words, columns=['word', 'count'])\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.barplot(data=top_words_df, x='count', y='word', palette='viridis')\n",
    "# plt.title('Top 20 Palabras M\u00e1s Frecuentes', fontsize=14, fontweight='bold')\n",
    "# plt.xlabel('Frecuencia')\n",
    "# plt.ylabel('Palabra')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 An\u00e1lisis por Clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palabras m\u00e1s frecuentes por clase\n",
    "# for label in df['label'].unique():\n",
    "#     print(\"\\n\" + \"=\" * 50)\n",
    "#     print(f\"TOP PALABRAS - CLASE {label}\")\n",
    "#     print(\"=\" * 50)\n",
    "#     class_texts = df[df['label'] == label]['text_clean']\n",
    "#     top_words_class = get_top_words(class_texts, n=15)\n",
    "#     for word, count in top_words_class:\n",
    "#         print(f\"{word:20s}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Nubes de Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci\u00f3n para generar WordCloud\n",
    "def generate_wordcloud(texts, title):\n",
    "    \"\"\"Genera y muestra una nube de palabras\"\"\"\n",
    "    text_combined = ' '.join(texts)\n",
    "    wordcloud = WordCloud(width=800, height=400,\n",
    "                         background_color='white',\n",
    "                         max_words=100,\n",
    "                         colormap='viridis').generate(text_combined)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# # WordCloud general\n",
    "# generate_wordcloud(df['text_clean'], 'Nube de Palabras - Todos los Comentarios')\n",
    "\n",
    "# # WordCloud por clase\n",
    "# for label in df['label'].unique():\n",
    "#     class_texts = df[df['label'] == label]['text_clean']\n",
    "#     generate_wordcloud(class_texts, f'Nube de Palabras - Clase {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 An\u00e1lisis de N-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci\u00f3n para obtener n-gramas\n",
    "def get_ngrams(texts, n=2, top=20):\n",
    "    \"\"\"Obtiene los n-gramas m\u00e1s frecuentes\"\"\"\n",
    "    ngrams_list = []\n",
    "    for text in texts:\n",
    "        words = str(text).split()\n",
    "        ngrams = zip(*[words[i:] for i in range(n)])\n",
    "        ngrams_list.extend([' '.join(ngram) for ngram in ngrams])\n",
    "    \n",
    "    ngram_freq = Counter(ngrams_list)\n",
    "    return ngram_freq.most_common(top)\n",
    "\n",
    "# # Bigramas\n",
    "# print(\"=\" * 50)\n",
    "# print(\"TOP 15 BIGRAMAS\")\n",
    "# print(\"=\" * 50)\n",
    "# bigrams = get_ngrams(df['text_clean'], n=2, top=15)\n",
    "# for bigram, count in bigrams:\n",
    "#     print(f\"{bigram:30s}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trigramas\n",
    "# print(\"=\" * 50)\n",
    "# print(\"TOP 15 TRIGRAMAS\")\n",
    "# print(\"=\" * 50)\n",
    "# trigrams = get_ngrams(df['text_clean'], n=3, top=15)\n",
    "# for trigram, count in trigrams:\n",
    "#     print(f\"{trigram:40s}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci\u00f3n de n-gramas\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# # Bigramas\n",
    "# bigrams_df = pd.DataFrame(bigrams, columns=['ngram', 'count'])\n",
    "# sns.barplot(data=bigrams_df, x='count', y='ngram', ax=axes[0], palette='mako')\n",
    "# axes[0].set_title('Top 15 Bigramas', fontsize=14, fontweight='bold')\n",
    "# axes[0].set_xlabel('Frecuencia')\n",
    "# axes[0].set_ylabel('Bigrama')\n",
    "\n",
    "# # Trigramas\n",
    "# trigrams_df = pd.DataFrame(trigrams, columns=['ngram', 'count'])\n",
    "# sns.barplot(data=trigrams_df, x='count', y='ngram', ax=axes[1], palette='rocket')\n",
    "# axes[1].set_title('Top 15 Trigramas', fontsize=14, fontweight='bold')\n",
    "# axes[1].set_xlabel('Frecuencia')\n",
    "# axes[1].set_ylabel('Trigrama')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An\u00e1lisis de Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar duplicados\n",
    "# print(\"=\" * 50)\n",
    "# print(\"AN\u00c1LISIS DE DUPLICADOS\")\n",
    "# print(\"=\" * 50)\n",
    "# duplicates = df.duplicated(subset=['text']).sum()\n",
    "# print(f\"Comentarios duplicados: {duplicates}\")\n",
    "# print(f\"Porcentaje de duplicados: {(duplicates/len(df))*100:.2f}%\")\n",
    "\n",
    "# if duplicates > 0:\n",
    "#     print(\"\\nEjemplos de textos duplicados:\")\n",
    "#     dup_examples = df[df.duplicated(subset=['text'], keep=False)].sort_values('text').head(10)\n",
    "#     print(dup_examples[['text', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusiones y Pr\u00f3ximos Pasos\n",
    "\n",
    "### Hallazgos Principales:\n",
    "1. **Distribuci\u00f3n de clases**: [Describir si est\u00e1 balanceado o desbalanceado]\n",
    "2. **Calidad de datos**: [Comentar sobre valores faltantes, duplicados]\n",
    "3. **Caracter\u00edsticas del texto**: [Longitud promedio, palabras m\u00e1s comunes]\n",
    "4. **Patrones identificados**: [Diferencias entre clases]\n",
    "\n",
    "### Recomendaciones para Preprocesamiento:\n",
    "1. Limpieza de texto (eliminar URLs, menciones, caracteres especiales)\n",
    "2. Normalizaci\u00f3n (min\u00fasculas, acentos)\n",
    "3. Eliminaci\u00f3n de stopwords\n",
    "4. Tokenizaci\u00f3n\n",
    "5. Lematizaci\u00f3n o stemming\n",
    "6. Manejo de duplicados\n",
    "7. T\u00e9cnicas de balanceo si es necesario (oversampling/undersampling)\n",
    "\n",
    "### Pr\u00f3ximas Fases:\n",
    "1. Preprocesamiento de texto\n",
    "2. Feature engineering (TF-IDF, embeddings)\n",
    "3. Entrenamiento de modelos baseline\n",
    "4. Optimizaci\u00f3n y ajuste de hiperpar\u00e1metros\n",
    "5. Evaluaci\u00f3n y selecci\u00f3n del modelo final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Guardar Resultados del EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar dataset con nuevas features\n",
    "# output_path = Path('../data/processed/data_with_features.csv')\n",
    "# output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "# df.to_csv(output_path, index=False)\n",
    "# print(f\"\u2713 Dataset con features guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar estad\u00edsticas clave\n",
    "# stats_summary = {\n",
    "#     'total_records': len(df),\n",
    "#     'class_distribution': df['label'].value_counts().to_dict(),\n",
    "#     'missing_values': df.isnull().sum().to_dict(),\n",
    "#     'avg_char_count': df['char_count'].mean(),\n",
    "#     'avg_word_count': df['word_count'].mean(),\n",
    "#     'duplicates': duplicates\n",
    "# }\n",
    "\n",
    "# import json\n",
    "# with open('../data/processed/eda_summary.json', 'w') as f:\n",
    "#     json.dump(stats_summary, f, indent=4)\n",
    "# print(\"\u2713 Resumen de EDA guardado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}