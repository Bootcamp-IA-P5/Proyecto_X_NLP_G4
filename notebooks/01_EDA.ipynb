{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - Hate Speech Detection\n",
    "\n",
    "Este notebook contiene el análisis exploratorio de datos (EDA) para el proyecto de detección de mensajes de odio en comentarios de YouTube.\n",
    "\n",
    "## Objetivos del EDA:\n",
    "1. Cargar y explorar el dataset de comentarios\n",
    "2. Analizar la distribución de clases (mensajes de odio vs. mensajes normales)\n",
    "3. Explorar estadísticas de texto (longitud, palabras, caracteres)\n",
    "4. Identificar patrones y características del texto\n",
    "5. Detectar valores faltantes y problemas de calidad de datos\n",
    "6. Visualizar las palabras más frecuentes\n",
    "7. Analizar n-gramas\n",
    "8. Identificar insights para el preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías básicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Tamaño de figuras por defecto\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías de NLP\n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLTK (descomentar para descargar recursos necesarios)\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "print(\"✓ Librerías NLP importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cargar Datos\n",
    "\n",
    "**Nota:** Asegúrate de colocar tu dataset en la carpeta `data/raw/` con el nombre adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al dataset\n",
    "# Ajusta el nombre del archivo según tu dataset\n",
    "DATA_PATH = Path('../data/raw/')\n",
    "\n",
    "# Ejemplo de carga (ajustar según el formato real del dataset)\n",
    "# df = pd.read_csv(DATA_PATH / 'youtube_comments.csv')\n",
    "\n",
    "# Para este ejemplo, mostraremos cómo sería la estructura esperada\n",
    "print(f\"Ruta de datos: {DATA_PATH}\")\n",
    "print(\"\\nEstructura esperada del dataset:\")\n",
    "print(\"- Columna 'text' o 'comment': texto del comentario\")\n",
    "print(\"- Columna 'label' o 'class': etiqueta (0: normal, 1: hate speech)\")\n",
    "\n",
    "# Descomenta y ajusta según tu dataset:\n",
    "# df = pd.read_csv(DATA_PATH / 'tu_archivo.csv')\n",
    "# print(f\"\\n✓ Dataset cargado: {len(df)} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploración Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeras filas\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general del dataset\n",
    "# print(\"=\" * 50)\n",
    "# print(\"INFORMACIÓN DEL DATASET\")\n",
    "# print(\"=\" * 50)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas\n",
    "# df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones del dataset\n",
    "# print(f\"Número de filas: {df.shape[0]}\")\n",
    "# print(f\"Número de columnas: {df.shape[1]}\")\n",
    "# print(f\"\\nColumnas: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Análisis de Valores Faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores faltantes\n",
    "# print(\"=\" * 50)\n",
    "# print(\"VALORES FALTANTES\")\n",
    "# print(\"=\" * 50)\n",
    "# missing = df.isnull().sum()\n",
    "# missing_pct = (missing / len(df)) * 100\n",
    "# missing_df = pd.DataFrame({\n",
    "#     'Missing_Count': missing,\n",
    "#     'Missing_Percentage': missing_pct\n",
    "# })\n",
    "# print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Visualización de valores faltantes\n",
    "# if missing.sum() > 0:\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "#     plt.title('Mapa de Valores Faltantes')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Distribución de Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de la distribución de clases\n",
    "# print(\"=\" * 50)\n",
    "# print(\"DISTRIBUCIÓN DE CLASES\")\n",
    "# print(\"=\" * 50)\n",
    "# class_dist = df['label'].value_counts()\n",
    "# print(class_dist)\n",
    "# print(f\"\\nPorcentaje de cada clase:\")\n",
    "# print(df['label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualización\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# # Gráfico de barras\n",
    "# class_dist.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "# axes[0].set_title('Distribución de Clases', fontsize=14, fontweight='bold')\n",
    "# axes[0].set_xlabel('Clase')\n",
    "# axes[0].set_ylabel('Cantidad')\n",
    "# axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# # Gráfico de pastel\n",
    "# axes[1].pie(class_dist, labels=class_dist.index, autopct='%1.1f%%',\n",
    "#             colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "# axes[1].set_title('Proporción de Clases', fontsize=14, fontweight='bold')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Balance de clases\n",
    "# imbalance_ratio = class_dist.max() / class_dist.min()\n",
    "# print(f\"\\n⚠️ Ratio de desbalance: {imbalance_ratio:.2f}\")\n",
    "# if imbalance_ratio > 2:\n",
    "#     print(\"El dataset está desbalanceado. Considerar técnicas de balanceo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análisis de Texto\n",
    "\n",
    "### 6.1 Estadísticas Básicas del Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares para análisis de texto\n",
    "def get_text_stats(text):\n",
    "    \"\"\"Obtiene estadísticas básicas de un texto\"\"\"\n",
    "    return {\n",
    "        'char_count': len(str(text)),\n",
    "        'word_count': len(str(text).split()),\n",
    "        'avg_word_length': np.mean([len(word) for word in str(text).split()]),\n",
    "        'sentence_count': len(str(text).split('.')),\n",
    "        'uppercase_count': sum(1 for c in str(text) if c.isupper()),\n",
    "        'special_char_count': sum(1 for c in str(text) if not c.isalnum() and not c.isspace())\n",
    "    }\n",
    "\n",
    "# # Aplicar funciones\n",
    "# df['char_count'] = df['text'].apply(lambda x: len(str(x)))\n",
    "# df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "# df['avg_word_length'] = df['text'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\n",
    "\n",
    "# print(\"✓ Estadísticas de texto calculadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas de las métricas de texto\n",
    "# print(\"=\" * 50)\n",
    "# print(\"ESTADÍSTICAS DE TEXTO\")\n",
    "# print(\"=\" * 50)\n",
    "# text_stats = df[['char_count', 'word_count', 'avg_word_length']].describe()\n",
    "# print(text_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de distribuciones de texto\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# # Distribución de longitud de caracteres\n",
    "# axes[0, 0].hist(df['char_count'], bins=50, edgecolor='black', alpha=0.7)\n",
    "# axes[0, 0].set_title('Distribución de Longitud de Caracteres', fontweight='bold')\n",
    "# axes[0, 0].set_xlabel('Número de Caracteres')\n",
    "# axes[0, 0].set_ylabel('Frecuencia')\n",
    "\n",
    "# # Distribución de cantidad de palabras\n",
    "# axes[0, 1].hist(df['word_count'], bins=50, edgecolor='black', alpha=0.7, color='orange')\n",
    "# axes[0, 1].set_title('Distribución de Cantidad de Palabras', fontweight='bold')\n",
    "# axes[0, 1].set_xlabel('Número de Palabras')\n",
    "# axes[0, 1].set_ylabel('Frecuencia')\n",
    "\n",
    "# # Boxplot de longitud por clase\n",
    "# df.boxplot(column='char_count', by='label', ax=axes[1, 0])\n",
    "# axes[1, 0].set_title('Longitud de Caracteres por Clase', fontweight='bold')\n",
    "# axes[1, 0].set_xlabel('Clase')\n",
    "# axes[1, 0].set_ylabel('Número de Caracteres')\n",
    "# plt.suptitle('')\n",
    "\n",
    "# # Boxplot de palabras por clase\n",
    "# df.boxplot(column='word_count', by='label', ax=axes[1, 1])\n",
    "# axes[1, 1].set_title('Cantidad de Palabras por Clase', fontweight='bold')\n",
    "# axes[1, 1].set_xlabel('Clase')\n",
    "# axes[1, 1].set_ylabel('Número de Palabras')\n",
    "# plt.suptitle('')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Análisis de Palabras Frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para limpiar y tokenizar texto\n",
    "def clean_text(text):\n",
    "    \"\"\"Limpia el texto para análisis básico\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-záéíóúñ\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def get_top_words(texts, n=20):\n",
    "    \"\"\"Obtiene las palabras más frecuentes\"\"\"\n",
    "    words = ' '.join(texts).split()\n",
    "    word_freq = Counter(words)\n",
    "    return word_freq.most_common(n)\n",
    "\n",
    "# # Aplicar limpieza\n",
    "# df['text_clean'] = df['text'].apply(clean_text)\n",
    "# print(\"✓ Texto limpiado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top palabras generales\n",
    "# print(\"=\" * 50)\n",
    "# print(\"TOP 20 PALABRAS MÁS FRECUENTES\")\n",
    "# print(\"=\" * 50)\n",
    "# top_words = get_top_words(df['text_clean'])\n",
    "# for word, count in top_words:\n",
    "#     print(f\"{word:20s}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de palabras frecuentes\n",
    "# top_words_df = pd.DataFrame(top_words, columns=['word', 'count'])\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.barplot(data=top_words_df, x='count', y='word', palette='viridis')\n",
    "# plt.title('Top 20 Palabras Más Frecuentes', fontsize=14, fontweight='bold')\n",
    "# plt.xlabel('Frecuencia')\n",
    "# plt.ylabel('Palabra')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Análisis por Clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palabras más frecuentes por clase\n",
    "# for label in df['label'].unique():\n",
    "#     print(\"\\n\" + \"=\" * 50)\n",
    "#     print(f\"TOP PALABRAS - CLASE {label}\")\n",
    "#     print(\"=\" * 50)\n",
    "#     class_texts = df[df['label'] == label]['text_clean']\n",
    "#     top_words_class = get_top_words(class_texts, n=15)\n",
    "#     for word, count in top_words_class:\n",
    "#         print(f\"{word:20s}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Nubes de Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar WordCloud\n",
    "def generate_wordcloud(texts, title):\n",
    "    \"\"\"Genera y muestra una nube de palabras\"\"\"\n",
    "    text_combined = ' '.join(texts)\n",
    "    wordcloud = WordCloud(width=800, height=400,\n",
    "                         background_color='white',\n",
    "                         max_words=100,\n",
    "                         colormap='viridis').generate(text_combined)\n",
    "    \n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# # WordCloud general\n",
    "# generate_wordcloud(df['text_clean'], 'Nube de Palabras - Todos los Comentarios')\n",
    "\n",
    "# # WordCloud por clase\n",
    "# for label in df['label'].unique():\n",
    "#     class_texts = df[df['label'] == label]['text_clean']\n",
    "#     generate_wordcloud(class_texts, f'Nube de Palabras - Clase {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Análisis de N-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener n-gramas\n",
    "def get_ngrams(texts, n=2, top=20):\n",
    "    \"\"\"Obtiene los n-gramas más frecuentes\"\"\"\n",
    "    ngrams_list = []\n",
    "    for text in texts:\n",
    "        words = str(text).split()\n",
    "        ngrams = zip(*[words[i:] for i in range(n)])\n",
    "        ngrams_list.extend([' '.join(ngram) for ngram in ngrams])\n",
    "    \n",
    "    ngram_freq = Counter(ngrams_list)\n",
    "    return ngram_freq.most_common(top)\n",
    "\n",
    "# # Bigramas\n",
    "# print(\"=\" * 50)\n",
    "# print(\"TOP 15 BIGRAMAS\")\n",
    "# print(\"=\" * 50)\n",
    "# bigrams = get_ngrams(df['text_clean'], n=2, top=15)\n",
    "# for bigram, count in bigrams:\n",
    "#     print(f\"{bigram:30s}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trigramas\n",
    "# print(\"=\" * 50)\n",
    "# print(\"TOP 15 TRIGRAMAS\")\n",
    "# print(\"=\" * 50)\n",
    "# trigrams = get_ngrams(df['text_clean'], n=3, top=15)\n",
    "# for trigram, count in trigrams:\n",
    "#     print(f\"{trigram:40s}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de n-gramas\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# # Bigramas\n",
    "# bigrams_df = pd.DataFrame(bigrams, columns=['ngram', 'count'])\n",
    "# sns.barplot(data=bigrams_df, x='count', y='ngram', ax=axes[0], palette='mako')\n",
    "# axes[0].set_title('Top 15 Bigramas', fontsize=14, fontweight='bold')\n",
    "# axes[0].set_xlabel('Frecuencia')\n",
    "# axes[0].set_ylabel('Bigrama')\n",
    "\n",
    "# # Trigramas\n",
    "# trigrams_df = pd.DataFrame(trigrams, columns=['ngram', 'count'])\n",
    "# sns.barplot(data=trigrams_df, x='count', y='ngram', ax=axes[1], palette='rocket')\n",
    "# axes[1].set_title('Top 15 Trigramas', fontsize=14, fontweight='bold')\n",
    "# axes[1].set_xlabel('Frecuencia')\n",
    "# axes[1].set_ylabel('Trigrama')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Análisis de Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar duplicados\n",
    "# print(\"=\" * 50)\n",
    "# print(\"ANÁLISIS DE DUPLICADOS\")\n",
    "# print(\"=\" * 50)\n",
    "# duplicates = df.duplicated(subset=['text']).sum()\n",
    "# print(f\"Comentarios duplicados: {duplicates}\")\n",
    "# print(f\"Porcentaje de duplicados: {(duplicates/len(df))*100:.2f}%\")\n",
    "\n",
    "# if duplicates > 0:\n",
    "#     print(\"\\nEjemplos de textos duplicados:\")\n",
    "#     dup_examples = df[df.duplicated(subset=['text'], keep=False)].sort_values('text').head(10)\n",
    "#     print(dup_examples[['text', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusiones y Próximos Pasos\n",
    "\n",
    "### Hallazgos Principales:\n",
    "1. **Distribución de clases**: [Describir si está balanceado o desbalanceado]\n",
    "2. **Calidad de datos**: [Comentar sobre valores faltantes, duplicados]\n",
    "3. **Características del texto**: [Longitud promedio, palabras más comunes]\n",
    "4. **Patrones identificados**: [Diferencias entre clases]\n",
    "\n",
    "### Recomendaciones para Preprocesamiento:\n",
    "1. Limpieza de texto (eliminar URLs, menciones, caracteres especiales)\n",
    "2. Normalización (minúsculas, acentos)\n",
    "3. Eliminación de stopwords\n",
    "4. Tokenización\n",
    "5. Lematización o stemming\n",
    "6. Manejo de duplicados\n",
    "7. Técnicas de balanceo si es necesario (oversampling/undersampling)\n",
    "\n",
    "### Próximas Fases:\n",
    "1. Preprocesamiento de texto\n",
    "2. Feature engineering (TF-IDF, embeddings)\n",
    "3. Entrenamiento de modelos baseline\n",
    "4. Optimización y ajuste de hiperparámetros\n",
    "5. Evaluación y selección del modelo final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Guardar Resultados del EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar dataset con nuevas features\n",
    "# output_path = Path('../data/processed/data_with_features.csv')\n",
    "# output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "# df.to_csv(output_path, index=False)\n",
    "# print(f\"✓ Dataset con features guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar estadísticas clave\n",
    "# stats_summary = {\n",
    "#     'total_records': len(df),\n",
    "#     'class_distribution': df['label'].value_counts().to_dict(),\n",
    "#     'missing_values': df.isnull().sum().to_dict(),\n",
    "#     'avg_char_count': df['char_count'].mean(),\n",
    "#     'avg_word_count': df['word_count'].mean(),\n",
    "#     'duplicates': duplicates\n",
    "# }\n",
    "\n",
    "# import json\n",
    "# with open('../data/processed/eda_summary.json', 'w') as f:\n",
    "#     json.dump(stats_summary, f, indent=4)\n",
    "# print(\"✓ Resumen de EDA guardado\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
