# Retrospectiva Scrum - Proyecto NLP Toxicity Detection

---

## 1. Contexto del Proyecto

**Equipo**: Grupo 4 - PX_NLP_G4  
**Proyecto**: Sistema de Detecci√≥n de Comentarios T√≥xicos  
**Repositorio**: https://github.com/[tu-org]/PX_NLP_G4  
**Duraci√≥n del Proyecto**: 18 d√≠as  
**Fecha Retrospectiva**: 12 de Diciembre de 2025  
**Participantes**: M√≥nica, Yeder, Alfonso.
---

### Stack Tecnol√≥gico Implementado

- **Machine Learning**: Naive Bayes, Random Forest, SVM, Logistic Regression, RNN-BiGRU, DistilBERT
- **Backend**: FastAPI + PostgreSQL
- **Frontend**: React + Vite
- **MLOps**: MLflow + Docker + GitHub Actions (CI/CD)
- **Control de Versiones**: Git con Git Flow (dev, feature branches)

---

### T√©cnica Utilizada: Barco de Vela / Barco Pirata (Met√°fora Visual)

**Descripci√≥n**: Esta t√©cnica de retrospectiva utiliza la met√°fora de un barco (nuestro equipo) navegando hacia una isla (nuestra meta). Identificamos:

- **‚õµ Viento a Favor / Motor**: ¬øQu√© nos impulsa? Las fuerzas positivas que aceleran nuestro progreso
- **‚öì Anclas**: ¬øQu√© nos frena? Los impedimentos y obst√°culos que nos ralentizan
- **ü™® Rocas / Icebergs**: ¬øQu√© riesgos vemos en el horizonte? Amenazas potenciales
- **‚≠ê Estrella / Isla**: ¬øHacia d√≥nde queremos ir? Nuestros objetivos de mejora

**Raz√≥n de elecci√≥n**: Seleccionamos esta t√©cnica por su naturaleza visual y creativa, ideal para equipos t√©cnicos. La met√°fora del viaje nos ayuda a identificar claramente los impulsores y frenos sin se√±alar culpables. Es especialmente efectiva para visualizar el estado actual del proyecto y trazar el rumbo de mejora.

---

## 2. Desarrollo de la Sesi√≥n

### 2.1 Organizaci√≥n de la Retrospectiva

**Facilitador**: Alfonso Berm√∫dez  
**Herramienta utilizada**: Miro para post-its, Zoom, Discord, CANVA/GAMMA.
**Duraci√≥n total**: 60 minutos  
**Modalidad**: Remota

---

### 2.2 Agenda Ejecutada

| Tiempo | Actividad | Descripci√≥n |
|--------|-----------|-------------|
| **0-5 min** | **Introducci√≥n** | Explicaci√≥n de la met√°fora del Barco de Vela y establecimiento de reglas: ambiente seguro, sin culpas, todos participan, focus en el viaje del equipo |
| **5-15 min** | **Dibujo y reflexi√≥n individual** | Se dibuja un barco (f√≠sico o digital). Cada miembro escribe en silencio sus ideas en post-its y los coloca en: Viento (impulsa), Anclas (frena), Rocas (riesgos), o Isla (destino) |
| **15-40 min** | **Compartir y discutir** | Cada participante comparte sus puntos. Se agrupan ideas similares en cada categor√≠a de la met√°fora |
| **40-55 min** | **Votaci√≥n y ruta a seguir** | Votaci√≥n democr√°tica (3 votos por persona) para priorizar: qu√© anclas soltar, qu√© vientos potenciar, qu√© rocas evitar |
| **55-60 min** | **Trazar el rumbo** | Definir 5 acciones concretas que nos llevar√°n a la isla (objetivo), con responsables y plazos |

---

### 2.3 Preparaci√≥n Previa

Antes de la sesi√≥n, el equipo revis√≥:

- **Repositorio Git**: An√°lisis de commits, Pull Requests, branches activas
- **M√©tricas del proyecto**:
  - 6 modelos de ML comparados
  - Backend con API REST funcional
  - Frontend desplegable con React
  - Integraci√≥n MLflow en fase final
- **Bloqueos identificados**: Dependencias entre m√≥dulos, integraciones tard√≠as, falta de tests
- **Preparaci√≥n del tablero**: Se dibuj√≥ un barco en el centro, con secciones para: Viento, Anclas, Rocas e Isla

**Materiales utilizados**:
- Tablero f√≠sico o Miro con dibujo del barco
- Post-its de 4 colores diferentes (uno por categor√≠a)
- Timer para mantener el timeboxing

---

## 3. Resultados de la Retrospectiva

### 3.1 ‚õµ VIENTO A FAVOR / MOTOR - ¬øQu√© nos impulsa?

*Las fuerzas positivas que aceleran nuestro progreso y nos hacen avanzar r√°pido*

#### **1. Comparativa exhaustiva de modelos con m√©tricas claras**

- **Impulso identificado**: 6 algoritmos comparados (Naive Bayes, Random Forest, SVM, Logistic Regression, RNN-BiGRU, DistilBERT) con evaluaci√≥n sistem√°tica.
- **Valor**: El archivo `comparison_models.ipynb` permite tomar decisiones basadas en datos objetivos, no intuiciones.
- **Viento que debemos mantener**: Siempre comparar m√∫ltiples enfoques antes de elegir el modelo final. Esta pr√°ctica nos da confianza en las decisiones t√©cnicas.

#### **2. Uso de Docker para servicios de MLOps (MLflow + MinIO/Azure)**

- **Impulso identificado**: Configuraci√≥n dockerizada reproducible.
- **Valor**: Facilita onboarding de nuevos miembros (setup en 5 minutos), elimina problemas de "en mi m√°quina funciona".
- **Viento que debemos mantener**: Extender Docker a toda la aplicaci√≥n, e incluir MLflow.

#### **3. Separaci√≥n clara Frontend/Backend con APIs REST**

- **Impulso identificado**: Arquitectura limpia con FastAPI (backend) y React (frontend) comunic√°ndose por API REST bien definida.
- **Valor**: Escalabilidad futura. Equipos pueden trabajar en paralelo sin interferencias.
- **Viento que debemos mantener**: Continuar con arquitectura desacoplada y documentar contratos de API.

#### **4. EDA (Exploratory Data Analysis) detallado antes de modelar**

- **Impulso identificado**: Notebooks `eda.ipynb` y `preprocessing_eda.ipynb` con an√°lisis profundo del dataset.
- **Valor**: Entender los datos evit√≥ problemas posteriores (desbalanceo de clases, outliers, distribuciones an√≥malas).
- **Viento que debemos mantener**: Nunca saltarse la fase de exploraci√≥n, es tiempo bien invertido.

#### **5. Git Branching Strategy (Feature branches + Pull Requests)**

- **Impulso identificado**: Uso correcto de feature branches como `21-feature-api`, `15-feature-ml_logisticregression`, permitiendo trabajo paralelo sin conflictos.
- **Valor**: Historial limpio de Git. F√°cil hacer rollback si algo falla. Trazabilidad de qui√©n hizo qu√© y por qu√©.
- **Viento que debemos mantener**: Seguir usando Git Flow rigurosamente en futuros proyectos.

---

### 3.2 ‚öì ANCLAS - ¬øQu√© nos frena?

*Los impedimentos y obst√°culos que nos ralentizan y debemos soltar para avanzar m√°s r√°pido*

#### **1. Falta de tests automatizados (especialmente en Backend)**

- **Ancla identificada**: No hay tests unitarios en backend (`main.py`, routers, schemas). Toda la validaci√≥n es manual.
- **Impacto**: Miedo a refactorizar c√≥digo porque no sabemos si romperemos algo. Bugs que llegan hasta el final del desarrollo.
- **C√≥mo soltar el ancla**: Implementar pytest con al menos 60% de cobertura en endpoints cr√≠ticos antes de continuar con nuevas features.

#### **2. Integraci√≥n tard√≠a de componentes**

- **Ancla identificada**: Frontend y backend se integraron casi al final. MLflow lleg√≥ en las √∫ltimas semanas.
- **Impacto**: Retrabajos masivos de √∫ltima hora. Estr√©s innecesario. Descubrimiento tard√≠o de incompatibilidades.
- **C√≥mo soltar el ancla**: Integraci√≥n continua desde el Sprint 1. "Integrar temprano, integrar frecuentemente".

#### **3. Hardcodeo de credenciales y configuraciones**

- **Ancla identificada**: Encontramos API keys, contrase√±as de BD y rutas absolutas directamente en el c√≥digo.
- **Impacto**: Riesgo de seguridad grave. C√≥digo no portable entre m√°quinas. Credenciales expuestas en Git.
- **C√≥mo soltar el ancla**: Usar `.env` desde el d√≠a 1, con `.env.example` como plantilla y `.env` en `.gitignore`.

#### **4. Notebooks sin estructura modular (mezcla de fases)**

- **Ancla identificada**: Notebooks muy largos que duplican c√≥digo.
- **Impacto**: Dif√≠cil reutilizar c√≥digo. Ejecutar todo el notebook cada vez que cambias una l√≠nea. Dificulta colaboraci√≥n.
- **C√≥mo soltar el ancla**: Separar notebooks por fase (01_eda, 02_preprocessing, 04_evaluation) y extraer funciones comunes a m√≥dulos Python.

---

### 3.3 ü™® ROCAS / ICEBERGS - ¬øQu√© riesgos vemos en el horizonte?

*Amenazas potenciales que debemos evitar para no chocar y hundir el barco*

#### **1. Falta de CI/CD automatizado**

- **Riesgo detectado**: Sin pipeline de CI/CD, cada merge a `dev` es un salto al vac√≠o. No sabemos si el c√≥digo compila o pasa tests hasta que alguien lo ejecuta manualmente.
- **Impacto potencial**: Bug cr√≠tico en producci√≥n que pudo haberse detectado con tests automatizados. Deployment manual propenso a errores humanos.
- **C√≥mo evitar la roca**: Implementar GitHub Actions ANTES del pr√≥ximo proyecto, no despu√©s.

#### **2. Ausencia de monitoreo en producci√≥n**

- **Riesgo detectado**: Si desplegamos el modelo a producci√≥n, no tenemos manera de detectar degradaci√≥n de performance, drift de datos o errores en runtime.
- **Impacto potencial**: Modelo dando predicciones err√≥neas durante semanas sin que nadie lo note. P√©rdida de confianza del usuario final.
- **C√≥mo evitar la roca**: Implementar logging estructurado y alertas b√°sicas (ej: Prometheus + Grafana).

#### **3. Dependencia de conocimiento individual (no documentado)**

- **Riesgo detectado**: Mucho conocimiento del proyecto est√° en la cabeza de miembros individuales, no en documentaci√≥n.
- **Impacto potencial**: Si un miembro clave no est√° disponible, el resto del equipo no puede continuar su trabajo. "Bus factor" muy bajo.
- **C√≥mo evitar la roca**: README por m√≥dulo, sesiones de pair programming para compartir conocimiento.

#### **4. Escalabilidad no considerada en dise√±o inicial**

- **Riesgo detectado**: No tenemos el c√°lculo de petici√≥n/segundo de la API.
- **Impacto potencial**: Sistema ca√≠do en producci√≥n por carga inesperada. Refactorizaci√≥n completa necesaria.
- **C√≥mo evitar la roca**: Load testing antes de desplegar. Implementar rate limiting y caching.

#### **5. Deuda t√©cnica acumulada**

- **Riesgo detectado**: C√≥digo duplicado, TODOs en comentarios.

- **Impacto potencial**: Mantenimiento cada vez m√°s dif√≠cil. 

- **C√≥mo evitar la roca**: Dedicar 20% de cada sprint a refactorizaci√≥n.

---

### 3.4 ‚≠ê ESTRELLA / ISLA - ¬øHacia d√≥nde queremos ir?

*Nuestro objetivo de mejora, el destino al que navegamos como equipo*

#### **Visi√≥n de la Isla (Estado deseado)**

> "Queremos ser un equipo de alto rendimiento en ML/NLP que entrega proyectos con calidad de producci√≥n, no solo prototipos acad√©micos. Un equipo donde cualquier miembro puede retomar el trabajo de otro sin bloquearse, donde el c√≥digo es auto-documentado y testeado, y donde los deployments son aburridos porque siempre funcionan."

#### **Caracter√≠sticas de nuestra Isla**

**üèùÔ∏è En lo T√©cnico:**
- ‚úÖ Aplicaci√≥n completamente dockerizada (un comando para levantar todo)
- ‚úÖ CI/CD automatizado que detecta bugs antes de merge
- ‚úÖ Cobertura de tests > 70% en backend, > 50% en frontend
- ‚úÖ MLflow integrado desde el Sprint 1, no al final
- ‚úÖ Documentaci√≥n clara que cualquier desarrollador nuevo puede seguir

**üèùÔ∏è En lo Metodol√≥gico:**
- ‚úÖ Dailies de 15 minutos (Lun/Mi√©/Vie) para sincronizar
- ‚úÖ Code reviews obligatorios antes de merge (m√≠nimo 1 aprobaci√≥n)
- ‚úÖ Retrospectivas cada 2 semanas para ajustar el rumbo
- ‚úÖ Definition of Done establecida y respetada

**üèùÔ∏è En lo Cultural:**
- ‚úÖ Ambiente seguro donde es OK decir "no s√©" o "comet√≠ un error"
- ‚úÖ Celebrar peque√±os logros al final de cada sprint
- ‚úÖ Aprendizaje continuo (compartir art√≠culos, t√©cnicas nuevas)
- ‚úÖ Equilibrio trabajo-vida (no crunchs de √∫ltima hora)
- ‚úÖ Orgullo por la calidad del c√≥digo que escribimos

#### **Coordenadas de la Isla (M√©tricas)**

Para saber si llegamos a la isla, mediremos:

1. **Velocidad de onboarding**: < 2 horas para que un nuevo miembro est√© productivo
2. **Tiempo de deployment**: < 15 minutos desde merge a producci√≥n
3. **Tasa de bugs en producci√≥n**: < 1 bug cr√≠tico por sprint
4. **Cobertura de tests**: > 70% backend, > 50% frontend
5. **Satisfacci√≥n del equipo**: Score > 8/10 en retrospectivas

---

## 4. Acciones de Mejora Priorizadas

### 4.1 Top 5 Acciones (Votadas por el equipo)

#### **Acci√≥n #1: Implementar CI/CD con GitHub Actions**

- **Descripci√≥n**: Pipeline automatizado que ejecute tests, linters y build al hacer push a `dev` o PR.
- **Responsable**: Yeder
- **Plazo**: Sprint 1 del pr√≥ximo proyecto (Semana 1-2)
- **M√©trica de √©xito**: 
  - Pipeline configurado con al menos 3 jobs (lint, test, build)
  - 80% de los PRs pasan los checks antes de revisi√≥n manual
- **Prioridad**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (M√°xima - evita bugs en producci√≥n)

---

#### **Acci√≥n #2: Dockerizar aplicaci√≥n completa (Frontend + Backend + BD)**

- **Descripci√≥n**: Crear `docker-compose.yml` en la ra√≠z con servicios: FastAPI, React, PostgreSQL, MLflow.
- **Responsable**: Alfonso
- **Plazo**: Semana 1 del pr√≥ximo proyecto
- **M√©trica de √©xito**: 
  - Comando √∫nico `docker-compose up -d` levanta toda la aplicaci√≥n
  - README con instrucciones de setup 
- **Prioridad**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (M√°xima - mejora reproducibilidad)

---

#### **Acci√≥n #3: Integrar MLflow desde el inicio del proyecto**

- **Descripci√≥n**: Configurar tracking de experimentos en el Sprint 1, no al final.
- **Responsable**: Alfonso
- **Plazo**: D√≠a 1 de entrenamiento de modelos
- **M√©trica de √©xito**: 
  - Todos los experimentos registrados en MLflow desde el primer modelo
  - No perder trazabilidad de ning√∫n experimento
- **Prioridad**: ‚≠ê‚≠ê‚≠ê‚≠ê (Alta - evita p√©rdida de experimentos)

---

#### **Acci√≥n #4: Establecer Dailies de 15 minutos (Lun/Mi√©/Vie)**

- **Descripci√≥n**: Reuniones cortas de sincronizaci√≥n para compartir progreso y bloqueos.
- **Responsable**: Todo el equipo (rotar facilitador cada semana)
- **Plazo**: Inmediato (pr√≥ximo proyecto)
- **M√©trica de √©xito**: 
  - 90% de asistencia a las dailies
  - Reducci√≥n de bloqueos reportados al final del sprint
- **Prioridad**: ‚≠ê‚≠ê‚≠ê‚≠ê (Alta - mejora comunicaci√≥n)

---

#### **Acci√≥n #5: Crear plantilla de README por m√≥dulo**

- **Descripci√≥n**: Template estandarizado con secciones: Objetivo, Instalaci√≥n, Uso, Arquitectura, Decisiones T√©cnicas.
- **Responsable**: M√≥nica
- **Plazo**: Semana 1 del pr√≥ximo proyecto
- **M√©trica de √©xito**: 
  - 100% de los m√≥dulos nuevos tienen README siguiendo el template
  - Tiempo de onboarding de nuevos miembros < 2 horas
- **Prioridad**: ‚≠ê‚≠ê‚≠ê (Media-Alta - facilita mantenimiento)

---

### 4.2 Acciones Consideradas pero No Priorizadas (Importante pero menos urgente)

Las siguientes acciones fueron discutidas pero no entraron en el Top 5:

- **Implementar logging estructurado** (ELK Stack o similar) ‚Üí Dejar para cuando la aplicaci√≥n est√© en producci√≥n
- **A√±adir autenticaci√≥n JWT en el backend** ‚Üí Feature para versi√≥n 2.0
- **Optimizaci√≥n de hiperpar√°metros con Transformers** ‚Üí Ya se probaron buenos defaults, no cr√≠tico

---

### 4.3 Plan de Seguimiento

**Revisi√≥n intermedia de acciones**: Fecha: 2 semanas despu√©s de la retrospectiva

- **Formato**: Reuni√≥n de 30 minutos
- **Objetivo**: Verificar progreso de las 5 acciones. ¬øCu√°les est√°n completadas? ¬øCu√°les necesitan ajuste?

**Pr√≥xima retrospectiva completa**: Fecha: Fin del siguiente sprint/proyecto

**Canal de comunicaci√≥n**: 
- Slack/Discord para updates r√°pidos
- Issues de GitHub para tracking de cada acci√≥n
- GitHub Wiki para notas

---

## 5. Reflexi√≥n Final del Equipo

### 5.1 Sobre la Aplicaci√≥n de Scrum en el Proyecto

**Lo que funcion√≥**:

- El uso de **feature branches** nos permiti√≥ trabajar en paralelo sin pisarnos. Cada miembro pudo avanzar en su m√≥dulo (modelos, backend, frontend) de forma independiente.
- La **estructura del backlog** en GitHub (issues por funcionalidad) nos dio visibilidad de qu√© faltaba y qui√©n estaba trabajando en qu√©.
- Los **sprints impl√≠citos** (aunque no formales) nos ayudaron a mantener momentum: primero EDA, luego modelos cl√°sicos, luego deep learning, luego backend, finalmente frontend.

**Lo que no funcionamos bien**:

- **No aplicamos ceremonias Scrum formalmente**: No tuvimos dailies regularmente, sprint plannings ni sprint reviews estructurados. Todo fue ad-hoc.
- **Sprints sin duraci√≥n fija**: No hubo timeboxing. Algunos m√≥dulos se alargaron m√°s de lo necesario.
- **Integraci√≥n tard√≠a**: Frontend-backend y MLflow se integraron casi al final, generando estr√©s innecesario.

**Aprendizajes clave**:

> "Scrum no es solo tener un repositorio Git organizado. Es comunicaci√≥n constante, priorizaci√≥n clara y adaptaci√≥n continua. En nuestro pr√≥ximo proyecto, queremos implementar las ceremonias formales."

---

### 5.2 Sobre la T√©cnica de Retrospectiva "Barco de Vela / Barco Pirata"

**Opini√≥n del equipo**:

La t√©cnica fue **muy efectiva** para identificar impulsos, frenos y riesgos de forma visual y colaborativa. La met√°fora ayud√≥ a que todos aportaran sin buscar culpables, enfoc√°ndonos en el rumbo del equipo.

**Aspectos destacados**:

- **Simplicidad visual**: F√°cil de entender, no requiere formaci√≥n previa.
- **Actionable**: Cada punto identificado se convierte naturalmente en una acci√≥n.
- **Equilibrio**: Permite resaltar tanto lo que impulsa (viento) como lo que frena (anclas) y los riesgos (rocas).

**Sorpresa del equipo**:

> "Nos sorprendi√≥ la cantidad de mejoras peque√±as que, sumadas, pueden tener un gran impacto. Por ejemplo, commits descriptivos o habr integrado mlflow nos hubiera permitido llevar un mejor control de las versiones de los par√°metros de los notebooks."

---

### 5.3 Compromiso para el Futuro

**Compromisos concretos del equipo**:

1. ‚úÖ **Aplicar las 5 acciones priorizadas** en nuestro pr√≥ximo proyecto desde el d√≠a 1.
2. ‚úÖ **Hacer retrospectivas m√°s frecuentes**: Cada 2 semanas (al final de cada sprint de 2 semanas).
3. ‚úÖ **Implementar dailies de 15 minutos** (Lunes, Mi√©rcoles, Viernes a las 9:00 AM).
4. ‚úÖ **Crear un backlog priorizado** con un Product Owner rotativo cada sprint.
5. ‚úÖ **Celebrar los peque√±os logros**: Al terminar cada sprint, reconocer el trabajo del equipo.

**Aprendizaje m√°s valioso**:

> "La retrospectiva nos hizo conscientes de que, aunque el proyecto fue exitoso t√©cnicamente (buenos modelos, arquitectura limpia, frontend funcional), nuestros **procesos de equipo tienen margen de mejora**. La tecnolog√≠a es solo una parte del √©xito; la comunicaci√≥n y organizaci√≥n son igual de importantes."

**Frase que resume nuestra experiencia**:

> *"No se trata de hacer Scrum perfecto desde el inicio, sino de mejorar continuamente. Esta retrospectiva es el primer paso de muchos hacia convertirnos en un equipo de alto rendimiento."*

---

## Anexos

### A. M√©tricas del Proyecto

| M√©trica | Valor |
|---------|-------|
| **Commits totales** | 102 commits |
| **Pull Requests mergeados** | 7 PRs |
| **Issues cerrados** | 23 issues |
| **Modelos de ML entrenados** | 6 (NB, RF, SVM, LR, RNN-BiGRU, DistilBERT) |
| **Accuracy del mejor modelo** | [0.80%] |
| **L√≠neas de c√≥digo (Backend)** | 1007043 total |
| **Componentes de Frontend** | 10 |
| **Cobertura de tests** | 0% (a mejorar en pr√≥ximo proyecto) |

---

### B. Timeline del Proyecto

```
Sprint 1-2: EDA y Preprocesamiento
‚îú‚îÄ eda.ipynb
‚îú‚îÄ preprocessing_eda.ipynb
‚îî‚îÄ Limpieza de datos, an√°lisis de distribuciones

Sprint 3-4: Modelado Cl√°sico
‚îú‚îÄ Naive Bayes
‚îú‚îÄ Random Forest
‚îú‚îÄ SVM
‚îî‚îÄ Logistic Regression

Sprint 5-6: Deep Learning
‚îú‚îÄ RNN-BiGRU
‚îî‚îÄ DistilBERT (fine-tuning)

Sprint 7-8: Backend
‚îú‚îÄ FastAPI setup
‚îú‚îÄ PostgreSQL integration
‚îú‚îÄ Endpoints de predicci√≥n
‚îî‚îÄ Schemas y validaciones

Sprint 9-10: Frontend
‚îú‚îÄ React + Vite setup
‚îú‚îÄ P√°ginas (Home, Results, Model comparisons)
‚îî‚îÄ Integraci√≥n con backend

Sprint 11: MLOps (MLflow)
‚îú‚îÄ Docker setup (MinIO y Azure)
‚îî‚îÄ Tracking de experimentos (integraci√≥n tard√≠a)
```

---

### C. Evidencias de la Sesi√≥n de Retrospectiva

**Tablero de Start-Stop-Continue**:

- **Tablero Ideaci√≥n**: https://miro.com/app/board/uXjVGcMw7E8=/

---

### D. Recursos Consultados

- **T√©cnica utilizada**: https://www.funretrospectives.com/start-stop-continue/
- **Gu√≠a de Scrum**: https://scrumguides.org/
- **Git Workflow**: https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow

---

## Conclusi√≥n

Esta retrospectiva ha sido **extremadamente valiosa** para el equipo. Nos ha permitido:

‚úÖ Identificar **15 puntos de mejora concretos** (5 START, 5 STOP, 5 CONTINUE)  
‚úÖ Priorizar **5 acciones de alto impacto** con responsables y plazos  
‚úÖ Reflexionar sobre nuestra aplicaci√≥n de Scrum y c√≥mo mejorarla  
‚úÖ Celebrar nuestros logros t√©cnicos (comparativa de 6 modelos, arquitectura limpia)  
‚úÖ Comprometernos a hacer retrospectivas m√°s frecuentes  

**El proyecto NLP Toxicity Detection fue un √©xito t√©cnico**, pero ahora sabemos que podemos ser a√∫n mejores como equipo si aplicamos estas lecciones aprendidas.

---

**Equipo PX_NLP_G4**  
*"C√≥digo que aprende, equipo que mejora"*

Fecha: 12 de Diciembre de 2025
