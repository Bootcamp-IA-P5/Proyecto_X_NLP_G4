{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86dd179",
   "metadata": {},
   "source": [
    "# Fine-tuning de DistilBERT para detección de hate speech\n",
    "\n",
    "En este notebook vamos a entrenar un modelo basado en transformers (DistilBERT)\n",
    "para clasificar comentarios de YouTube como **tóxicos (1)** o **no tóxicos (0)**.\n",
    "\n",
    "Objetivos:\n",
    "\n",
    "- Cargar el dataset preprocesado (`text_basic` + `IsToxic`).\n",
    "- Preparar los datos para HuggingFace (tokenizer + `Dataset`).\n",
    "- Fine-tuning de `distilbert-base-uncased` para clasificación binaria.\n",
    "- Evaluar el modelo en el conjunto de test (accuracy, precision, recall, F1, ROC-AUC).\n",
    "- Guardar el modelo en `backend/models/distilbert_toxic_v1`.\n",
    "- Guardar las métricas en `data/results/distilbert_toxic_v1.json`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44ab2a",
   "metadata": {},
   "source": [
    "## 1. Configuración inicial y rutas del proyecto\n",
    "\n",
    "En esta sección definimos las rutas relativas dentro del repositorio y hacemos\n",
    "las importaciones básicas que necesitaremos más adelante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d29c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook dir: c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\backend\\notebooks\n",
      "CSV path: c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\data\\preprocessing_data\\youtoxic_english_1000_clean.csv\n",
      "Metrics JSON: c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\data\\results\\distilbert_toxic_v1.json\n",
      "Model dir: c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\backend\\models\\distilbert_toxic_v1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta del repo (este notebook está en backend/notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "BACKEND_DIR = NOTEBOOK_DIR.parent           # backend/\n",
    "ROOT_DIR = BACKEND_DIR.parent               # raíz del proyecto\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "PREPROC_DIR = DATA_DIR / \"preprocessing_data\"\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "MODELS_DIR = BACKEND_DIR / \"models\"\n",
    "\n",
    "# Ficheros concretos\n",
    "CSV_PATH = PREPROC_DIR / \"youtoxic_english_1000_clean.csv\"\n",
    "METRICS_JSON_PATH = RESULTS_DIR / \"distilbert_toxic_v1.json\"\n",
    "DISTILBERT_MODEL_DIR = MODELS_DIR / \"distilbert_toxic_v1\"\n",
    "\n",
    "print(\"Notebook dir:\", NOTEBOOK_DIR)\n",
    "print(\"CSV path:\", CSV_PATH)\n",
    "print(\"Metrics JSON:\", METRICS_JSON_PATH)\n",
    "print(\"Model dir:\", DISTILBERT_MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d336d60",
   "metadata": {},
   "source": [
    "## 2. Instalación de dependencias\n",
    "\n",
    "Instalamos las librerías necesarias para trabajar con transformers y los\n",
    "datasets de HuggingFace. Esta celda solo es necesario ejecutarla la primera vez\n",
    "(en el entorno del proyecto).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf9c42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"transformers>=4.40.0\" \"datasets>=2.19.0\" \"evaluate\" \"accelerate\" \"scikit-learn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e048c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"tf_keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd711132",
   "metadata": {},
   "source": [
    "## 3. Imports principales\n",
    "\n",
    "Importamos las librerías de HuggingFace, sklearn y utilidades varias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2c0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645caddd",
   "metadata": {},
   "source": [
    "## 4. Carga del dataset preprocesado\n",
    "\n",
    "Cargamos el CSV que generamos en el notebook de preprocesado.  \n",
    "Suponemos que tiene al menos estas columnas:\n",
    "\n",
    "- `text_basic`: texto preparado para modelos modernos.\n",
    "- `IsToxic`: etiqueta binaria (0 = no tóxico, 1 = tóxico).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e076a5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              CommentId      VideoId  \\\n",
      "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
      "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
      "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
      "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
      "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
      "\n",
      "                                                Text  IsToxic  IsAbusive  \\\n",
      "0  If only people would just take a step back and...    False      False   \n",
      "1  Law enforcement is not trained to shoot to app...     True       True   \n",
      "2  \\r\\nDont you reckon them 'black lives matter' ...     True       True   \n",
      "3  There are a very large number of people who do...    False      False   \n",
      "4  The Arab dude is absolutely right, he should h...    False      False   \n",
      "\n",
      "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  \\\n",
      "0     False          False      False         False     False   \n",
      "1     False          False      False         False     False   \n",
      "2     False          False       True         False     False   \n",
      "3     False          False      False         False     False   \n",
      "4     False          False      False         False     False   \n",
      "\n",
      "   IsReligiousHate                                         text_basic  \\\n",
      "0            False  If only people would just take a step back and...   \n",
      "1            False  Law enforcement is not trained to shoot to app...   \n",
      "2            False  Dont you reckon them 'black lives matter' bann...   \n",
      "3            False  There are a very large number of people who do...   \n",
      "4            False  The Arab dude is absolutely right, he should h...   \n",
      "\n",
      "                                        text_classic  text_len_classic  \\\n",
      "0  people would take step back make case wasnt an...               850   \n",
      "1  law enforcement trained shoot apprehend traine...                90   \n",
      "2  dont reckon black life matter banner held whit...               252   \n",
      "3  large number people like police officer called...               339   \n",
      "4  arab dude absolutely right shot extra time sho...               138   \n",
      "\n",
      "   word_count_classic  uppercase_ratio  exclamation_count  hate_words_count  \n",
      "0                 129         0.014121                  0                 2  \n",
      "1                  13         0.036232                  0                 3  \n",
      "2                  40         0.002375                  0                 1  \n",
      "3                  49         0.015464                  0                 0  \n",
      "4                  23         0.020576                  0                 1  \n",
      "\n",
      "Columnas: ['CommentId', 'VideoId', 'Text', 'IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist', 'IsReligiousHate', 'text_basic', 'text_classic', 'text_len_classic', 'word_count_classic', 'uppercase_ratio', 'exclamation_count', 'hate_words_count']\n",
      "\n",
      "Distribución de IsToxic:\n",
      "IsToxic\n",
      "False    0.539619\n",
      "True     0.460381\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(df.head())\n",
    "print(\"\\nColumnas:\", df.columns.tolist())\n",
    "print(\"\\nDistribución de IsToxic:\")\n",
    "print(df[\"IsToxic\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b7b94",
   "metadata": {},
   "source": [
    "### 4.1 Limpieza ligera y renombrado de la columna label\n",
    "\n",
    "HuggingFace `Trainer` espera normalmente una columna `labels`.  \n",
    "Renombramos `IsToxic` → `label` y nos aseguramos de que sea un entero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a609032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_basic</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If only people would just take a step back and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Law enforcement is not trained to shoot to app...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dont you reckon them 'black lives matter' bann...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There are a very large number of people who do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Arab dude is absolutely right, he should h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_basic  label\n",
       "0  If only people would just take a step back and...      0\n",
       "1  Law enforcement is not trained to shoot to app...      1\n",
       "2  Dont you reckon them 'black lives matter' bann...      1\n",
       "3  There are a very large number of people who do...      0\n",
       "4  The Arab dude is absolutely right, he should h...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminamos filas con textos o etiquetas nulas por seguridad\n",
    "df = df.dropna(subset=[\"text_basic\", \"IsToxic\"]).reset_index(drop=True)\n",
    "\n",
    "# Renombrar la columna de target a 'label'\n",
    "df = df.rename(columns={\"IsToxic\": \"label\"})\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "df[[\"text_basic\", \"label\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa984c",
   "metadata": {},
   "source": [
    "## 5. División en train y test\n",
    "\n",
    "Dividimos el dataset en entrenamiento (80%) y test (20%), estratificando por la\n",
    "etiqueta para mantener la proporción de tóxicos y no tóxicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b534d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(797, 200)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "len(train_df), len(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0927a5",
   "metadata": {},
   "source": [
    "## 6. Tokenizer y conversión a `datasets.Dataset`\n",
    "\n",
    "Usamos el modelo **`distilbert-base-uncased`** y su tokenizer oficial.  \n",
    "Luego convertimos los `DataFrame` de pandas a objetos `Dataset` y aplicamos\n",
    "la tokenización con padding y truncado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab6f541b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc81fd6022474873a1d797dd8805e40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yeder\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9bf236583b84ec78d2e04c4ad9fdebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58913762badd4234a5056b39443ea962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bf658b903a45d69e2478c85a840460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9021abef60b4ca9b51c8fc370b99f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/797 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ad61ef41874092be24955f50e7a752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text_basic', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 797\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text_basic', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 200\n",
       " }))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text_basic\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "# Creamos Dataset a partir de pandas\n",
    "train_ds = Dataset.from_pandas(train_df[[\"text_basic\", \"label\"]])\n",
    "test_ds = Dataset.from_pandas(test_df[[\"text_basic\", \"label\"]])\n",
    "\n",
    "# Aplicamos tokenización\n",
    "train_encoded = train_ds.map(tokenize_batch, batched=True)\n",
    "test_encoded = test_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Renombramos 'label' -> 'labels' y preparamos tensores para PyTorch\n",
    "train_encoded = train_encoded.rename_column(\"label\", \"labels\")\n",
    "test_encoded = test_encoded.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_encoded.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "test_encoded.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "train_encoded, test_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a5961",
   "metadata": {},
   "source": [
    "## 7. Modelo DistilBERT para clasificación binaria\n",
    "\n",
    "Cargamos `AutoModelForSequenceClassification` con `num_labels=2` para que\n",
    "DistilBERT aprenda a distinguir entre comentarios tóxicos y no tóxicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679b0152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2820c52684a84f289c2bcf3a6bf628d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654e780",
   "metadata": {},
   "source": [
    "## 8. Métricas de evaluación\n",
    "\n",
    "Usamos las métricas de HuggingFace `evaluate` para:\n",
    "\n",
    "- `accuracy`\n",
    "- `precision`\n",
    "- `recall`\n",
    "- `f1`\n",
    "\n",
    "Después, calcularemos también `ROC-AUC` manualmente con `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "322aa7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23990aa9aa648bb8d72dad4fe0ad821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f90f716efb4d3aa86c27d08653daf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e68c8fd81c446183dbebba30977437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9745708e3b477fb5ed4e8bbe3d0e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    prec = precision_metric.compute(predictions=preds, references=labels, average=\"binary\")[\"precision\"]\n",
    "    rec = recall_metric.compute(predictions=preds, references=labels, average=\"binary\")[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
