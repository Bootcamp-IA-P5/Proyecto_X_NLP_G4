{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86dd179",
   "metadata": {},
   "source": [
    "# Fine-tuning de DistilBERT para detecci√≥n de hate speech\n",
    "\n",
    "En este notebook vamos a entrenar un modelo basado en transformers (DistilBERT)\n",
    "para clasificar comentarios de YouTube como **t√≥xicos (1)** o **no t√≥xicos (0)**.\n",
    "\n",
    "Objetivos:\n",
    "\n",
    "- Cargar el dataset preprocesado (`text_basic` + `IsToxic`).\n",
    "- Preparar los datos para HuggingFace (tokenizer + `Dataset`).\n",
    "- Fine-tuning de `distilbert-base-uncased` para clasificaci√≥n binaria.\n",
    "- Evaluar el modelo en el conjunto de test (accuracy, precision, recall, F1, ROC-AUC).\n",
    "- Guardar el modelo en `backend/models/distilbert_toxic_v1`.\n",
    "- Guardar las m√©tricas en `data/results/distilbert_toxic_v1.json`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44ab2a",
   "metadata": {},
   "source": [
    "## 1. Configuraci√≥n inicial y rutas del proyecto\n",
    "\n",
    "En esta secci√≥n definimos las rutas relativas dentro del repositorio y hacemos\n",
    "las importaciones b√°sicas que necesitaremos m√°s adelante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2d29c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook dir: c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\backend\\notebooks\n",
      "Root dir: c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\n",
      "Backend dir: c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\backend\n",
      "CSV path: c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\data\\preprocessing_data\\youtoxic_english_1000_clean.csv\n",
      "CSV exists? True\n",
      "Metrics JSON: c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\data\\results\\distilbert_toxic_v1.json\n",
      "Model dir: c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\backend\\models\\distilbert_toxic_v1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ruta del repo (este notebook est√° en backend/notebooks/)\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "# Detectar la ra√≠z del proyecto de forma robusta\n",
    "if NOTEBOOK_DIR.name == \"notebooks\":\n",
    "    # Estamos en .../backend/notebooks\n",
    "    BACKEND_DIR = NOTEBOOK_DIR.parent\n",
    "    ROOT_DIR = BACKEND_DIR.parent\n",
    "elif NOTEBOOK_DIR.name == \"backend\":\n",
    "    # Estamos en .../backend\n",
    "    BACKEND_DIR = NOTEBOOK_DIR\n",
    "    ROOT_DIR = BACKEND_DIR.parent\n",
    "else:\n",
    "    # Asumimos que estamos en la ra√≠z del proyecto\n",
    "    ROOT_DIR = NOTEBOOK_DIR\n",
    "    BACKEND_DIR = ROOT_DIR / \"backend\"\n",
    "\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "PREPROC_DIR = DATA_DIR / \"preprocessing_data\"\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "MODELS_DIR = BACKEND_DIR / \"models\"\n",
    "\n",
    "# Ficheros concretos\n",
    "CSV_PATH = PREPROC_DIR / \"youtoxic_english_1000_clean.csv\"\n",
    "METRICS_JSON_PATH = RESULTS_DIR / \"distilbert_toxic_v1.json\"\n",
    "DISTILBERT_MODEL_DIR = MODELS_DIR / \"distilbert_toxic_v1\"\n",
    "\n",
    "print(\"Notebook dir:\", NOTEBOOK_DIR)\n",
    "print(\"Root dir:\", ROOT_DIR)\n",
    "print(\"Backend dir:\", BACKEND_DIR)\n",
    "print(\"CSV path:\", CSV_PATH)\n",
    "print(\"CSV exists?\", CSV_PATH.exists())\n",
    "print(\"Metrics JSON:\", METRICS_JSON_PATH)\n",
    "print(\"Model dir:\", DISTILBERT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d336d60",
   "metadata": {},
   "source": [
    "## 2. Instalaci√≥n de dependencias\n",
    "\n",
    "Instalamos las librer√≠as necesarias para trabajar con transformers y los\n",
    "datasets de HuggingFace. Esta celda solo es necesario ejecutarla la primera vez\n",
    "(en el entorno del proyecto).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcf9c42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"transformers>=4.40.0\" \"datasets>=2.19.0\" \"evaluate\" \"accelerate\" \"scikit-learn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e048c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"tf_keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd711132",
   "metadata": {},
   "source": [
    "## 3. Imports principales\n",
    "\n",
    "Importamos las librer√≠as de HuggingFace, sklearn y utilidades varias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc2c0bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645caddd",
   "metadata": {},
   "source": [
    "## 4. Carga del dataset preprocesado\n",
    "\n",
    "Cargamos el CSV que generamos en el notebook de preprocesado.  \n",
    "Suponemos que tiene al menos estas columnas:\n",
    "\n",
    "- `text_basic`: texto preparado para modelos modernos.\n",
    "- `IsToxic`: etiqueta binaria (0 = no t√≥xico, 1 = t√≥xico).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e076a5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              CommentId      VideoId  \\\n",
      "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
      "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
      "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
      "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
      "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
      "\n",
      "                                                Text  IsToxic  IsAbusive  \\\n",
      "0  If only people would just take a step back and...    False      False   \n",
      "1  Law enforcement is not trained to shoot to app...     True       True   \n",
      "2  \\r\\nDont you reckon them 'black lives matter' ...     True       True   \n",
      "3  There are a very large number of people who do...    False      False   \n",
      "4  The Arab dude is absolutely right, he should h...    False      False   \n",
      "\n",
      "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  \\\n",
      "0     False          False      False         False     False   \n",
      "1     False          False      False         False     False   \n",
      "2     False          False       True         False     False   \n",
      "3     False          False      False         False     False   \n",
      "4     False          False      False         False     False   \n",
      "\n",
      "   IsReligiousHate                                         text_basic  \\\n",
      "0            False  If only people would just take a step back and...   \n",
      "1            False  Law enforcement is not trained to shoot to app...   \n",
      "2            False  Dont you reckon them 'black lives matter' bann...   \n",
      "3            False  There are a very large number of people who do...   \n",
      "4            False  The Arab dude is absolutely right, he should h...   \n",
      "\n",
      "                                        text_classic  text_len_classic  \\\n",
      "0  people would take step back make case wasnt an...               850   \n",
      "1  law enforcement trained shoot apprehend traine...                90   \n",
      "2  dont reckon black life matter banner held whit...               252   \n",
      "3  large number people like police officer called...               339   \n",
      "4  arab dude absolutely right shot extra time sho...               138   \n",
      "\n",
      "   word_count_classic  uppercase_ratio  exclamation_count  hate_words_count  \n",
      "0                 129         0.014121                  0                 2  \n",
      "1                  13         0.036232                  0                 3  \n",
      "2                  40         0.002375                  0                 1  \n",
      "3                  49         0.015464                  0                 0  \n",
      "4                  23         0.020576                  0                 1  \n",
      "\n",
      "Columnas: ['CommentId', 'VideoId', 'Text', 'IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative', 'IsObscene', 'IsHatespeech', 'IsRacist', 'IsReligiousHate', 'text_basic', 'text_classic', 'text_len_classic', 'word_count_classic', 'uppercase_ratio', 'exclamation_count', 'hate_words_count']\n",
      "\n",
      "Distribuci√≥n de IsToxic:\n",
      "IsToxic\n",
      "False    0.539619\n",
      "True     0.460381\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(df.head())\n",
    "print(\"\\nColumnas:\", df.columns.tolist())\n",
    "print(\"\\nDistribuci√≥n de IsToxic:\")\n",
    "print(df[\"IsToxic\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b7b94",
   "metadata": {},
   "source": [
    "### 4.1 Limpieza ligera y renombrado de la columna label\n",
    "\n",
    "HuggingFace `Trainer` espera normalmente una columna `labels`.  \n",
    "Renombramos `IsToxic` ‚Üí `label` y nos aseguramos de que sea un entero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a609032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_basic</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If only people would just take a step back and...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Law enforcement is not trained to shoot to app...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dont you reckon them 'black lives matter' bann...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>There are a very large number of people who do...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Arab dude is absolutely right, he should h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_basic  label\n",
       "0  If only people would just take a step back and...      0\n",
       "1  Law enforcement is not trained to shoot to app...      1\n",
       "2  Dont you reckon them 'black lives matter' bann...      1\n",
       "3  There are a very large number of people who do...      0\n",
       "4  The Arab dude is absolutely right, he should h...      0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminamos filas con textos o etiquetas nulas por seguridad\n",
    "df = df.dropna(subset=[\"text_basic\", \"IsToxic\"]).reset_index(drop=True)\n",
    "\n",
    "# Renombrar la columna de target a 'label'\n",
    "df = df.rename(columns={\"IsToxic\": \"label\"})\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "df[[\"text_basic\", \"label\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa984c",
   "metadata": {},
   "source": [
    "## 5. Divisi√≥n en train y test\n",
    "\n",
    "Dividimos el dataset en entrenamiento (80%) y test (20%), estratificando por la\n",
    "etiqueta para mantener la proporci√≥n de t√≥xicos y no t√≥xicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b534d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(797, 200)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "len(train_df), len(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0927a5",
   "metadata": {},
   "source": [
    "## 6. Tokenizer y conversi√≥n a `datasets.Dataset`\n",
    "\n",
    "Usamos el modelo **`distilbert-base-uncased`** y su tokenizer oficial.  \n",
    "Luego convertimos los `DataFrame` de pandas a objetos `Dataset` y aplicamos\n",
    "la tokenizaci√≥n con padding y truncado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab6f541b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b389034af19440dd80aae1458f2ac67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/797 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6a0f0a0b384ea5bade10df63ad93fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text_basic', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 797\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text_basic', 'labels', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 200\n",
       " }))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text_basic\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "# Creamos Dataset a partir de pandas\n",
    "train_ds = Dataset.from_pandas(train_df[[\"text_basic\", \"label\"]])\n",
    "test_ds = Dataset.from_pandas(test_df[[\"text_basic\", \"label\"]])\n",
    "\n",
    "# Aplicamos tokenizaci√≥n\n",
    "train_encoded = train_ds.map(tokenize_batch, batched=True)\n",
    "test_encoded = test_ds.map(tokenize_batch, batched=True)\n",
    "\n",
    "# Renombramos 'label' -> 'labels' y preparamos tensores para PyTorch\n",
    "train_encoded = train_encoded.rename_column(\"label\", \"labels\")\n",
    "test_encoded = test_encoded.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_encoded.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "test_encoded.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "\n",
    "train_encoded, test_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a5961",
   "metadata": {},
   "source": [
    "## 7. Modelo DistilBERT para clasificaci√≥n binaria\n",
    "\n",
    "Cargamos `AutoModelForSequenceClassification` con `num_labels=2` para que\n",
    "DistilBERT aprenda a distinguir entre comentarios t√≥xicos y no t√≥xicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "679b0152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c654e780",
   "metadata": {},
   "source": [
    "## 8. M√©tricas de evaluaci√≥n\n",
    "\n",
    "Usamos las m√©tricas de HuggingFace `evaluate` para:\n",
    "\n",
    "- `accuracy`\n",
    "- `precision`\n",
    "- `recall`\n",
    "- `f1`\n",
    "\n",
    "Despu√©s, calcularemos tambi√©n `ROC-AUC` manualmente con `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "322aa7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    prec = precision_metric.compute(predictions=preds, references=labels, average=\"binary\")[\"precision\"]\n",
    "    rec = recall_metric.compute(predictions=preds, references=labels, average=\"binary\")[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"]\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d21026",
   "metadata": {},
   "source": [
    "## 9. Par√°metros de entrenamiento\n",
    "\n",
    "Configuramos los `TrainingArguments` para el fine-tuning:\n",
    "\n",
    "- 3 √©pocas\n",
    "- batch size 16\n",
    "- aprendizaje 5e-5\n",
    "- evaluaci√≥n al final de cada √©poca\n",
    "- guardar el mejor modelo seg√∫n F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce56f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(ROOT_DIR / \"distilbert_training_outputs\"),\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # üîß Cambios para compatibilidad\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8033ff8",
   "metadata": {},
   "source": [
    "## 10. Entrenamiento del modelo\n",
    "\n",
    "Creamos el objeto `Trainer` pasando:\n",
    "\n",
    "- modelo\n",
    "- argumentos de entrenamiento\n",
    "- datasets codificados\n",
    "- tokenizer\n",
    "- funci√≥n de m√©tricas\n",
    "\n",
    "Luego lanzamos el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "664e6e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeder\\AppData\\Local\\Temp\\ipykernel_17516\\3246262577.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 08:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.557900</td>\n",
       "      <td>0.503095</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>0.712871</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.746114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.481691</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.790123</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.739884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.171300</td>\n",
       "      <td>0.531706</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.782609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=0.3737753423055013, metrics={'train_runtime': 507.0915, 'train_samples_per_second': 4.715, 'train_steps_per_second': 0.296, 'total_flos': 79182387546624.0, 'train_loss': 0.3737753423055013, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_encoded,\n",
    "    eval_dataset=test_encoded,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "train_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c9800",
   "metadata": {},
   "source": [
    "## 11. Evaluaci√≥n final en el conjunto de test\n",
    "\n",
    "Usamos el `Trainer` para obtener predicciones en el conjunto de test y\n",
    "calculamos m√©tricas detalladas:\n",
    "\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1\n",
    "- ROC-AUC\n",
    "- Matriz de confusi√≥n (TN, FP, FN, TP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f54cbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8,\n",
       " 'precision': 0.782608695652174,\n",
       " 'recall': 0.782608695652174,\n",
       " 'f1': 0.782608695652174,\n",
       " 'roc_auc': 0.8649355877616747,\n",
       " 'tn': 88,\n",
       " 'fp': 20,\n",
       " 'fn': 20,\n",
       " 'tp': 72}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicciones crudas\n",
    "pred_output = trainer.predict(test_encoded)\n",
    "\n",
    "logits = pred_output.predictions\n",
    "labels = pred_output.label_ids\n",
    "\n",
    "# Predicci√≥n final (clase 0/1)\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Probabilidad de la clase positiva para ROC-AUC\n",
    "probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "pos_probs = probs[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision = precision_score(labels, preds)\n",
    "recall = recall_score(labels, preds)\n",
    "f1 = f1_score(labels, preds)\n",
    "roc_auc = roc_auc_score(labels, pos_probs)\n",
    "\n",
    "cm = confusion_matrix(labels, preds)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "metrics_dict = {\n",
    "    \"accuracy\": float(accuracy),\n",
    "    \"precision\": float(precision),\n",
    "    \"recall\": float(recall),\n",
    "    \"f1\": float(f1),\n",
    "    \"roc_auc\": float(roc_auc),\n",
    "    \"tn\": int(tn),\n",
    "    \"fp\": int(fp),\n",
    "    \"fn\": int(fn),\n",
    "    \"tp\": int(tp),\n",
    "}\n",
    "\n",
    "metrics_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518748e",
   "metadata": {},
   "source": [
    "## 12. Guardar el modelo fine-tuneado\n",
    "\n",
    "Guardamos el modelo y el tokenizer en la carpeta de modelos del backend:\n",
    "\n",
    "`backend/models/distilbert_toxic_v1`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8eba6ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo guardado en: c:\\Users\\yeder\\Documents\\Factoria F5 Bootcamp IA\\Proyecto_X_NLP_G4\\backend\\models\\distilbert_toxic_v1\n"
     ]
    }
   ],
   "source": [
    "DISTILBERT_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(DISTILBERT_MODEL_DIR)\n",
    "tokenizer.save_pretrained(DISTILBERT_MODEL_DIR)\n",
    "\n",
    "print(f\"Modelo guardado en: {DISTILBERT_MODEL_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9355c",
   "metadata": {},
   "source": [
    "## 13. Exportar m√©tricas a JSON para la app\n",
    "\n",
    "Generamos un JSON con la informaci√≥n del modelo y las m√©tricas para poder:\n",
    "\n",
    "- compararlo en `comparison_models.ipynb`\n",
    "- visualizarlo en el frontend (pesta√±a Resultados)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c7390c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'distilbert_toxic_v1',\n",
       " 'task': 'binary_classification',\n",
       " 'target_label': 'IsToxic',\n",
       " 'data': {'n_samples': 997,\n",
       "  'n_train': 797,\n",
       "  'n_test': 200,\n",
       "  'train_size': 0.7993981945837513,\n",
       "  'test_size': 0.20060180541624875,\n",
       "  'random_state': 42},\n",
       " 'metrics': {'accuracy': 0.8,\n",
       "  'precision': 0.782608695652174,\n",
       "  'recall': 0.782608695652174,\n",
       "  'f1': 0.782608695652174,\n",
       "  'roc_auc': 0.8649355877616747},\n",
       " 'confusion_matrix': {'tn': 88, 'fp': 20, 'fn': 20, 'tp': 72},\n",
       " 'timestamp': '2025-12-10T13:43:13.161030',\n",
       " 'notes': 'DistilBERT fine-tuned on text_basic for hate speech detection'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "METRICS_JSON_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_json = {\n",
    "    \"model_name\": \"distilbert_toxic_v1\",\n",
    "    \"task\": \"binary_classification\",\n",
    "    \"target_label\": \"IsToxic\",\n",
    "    \"data\": {\n",
    "        \"n_samples\": int(len(df)),\n",
    "        \"n_train\": int(len(train_df)),\n",
    "        \"n_test\": int(len(test_df)),\n",
    "        \"train_size\": len(train_df) / len(df),\n",
    "        \"test_size\": len(test_df) / len(df),\n",
    "        \"random_state\": 42,\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": metrics_dict[\"accuracy\"],\n",
    "        \"precision\": metrics_dict[\"precision\"],\n",
    "        \"recall\": metrics_dict[\"recall\"],\n",
    "        \"f1\": metrics_dict[\"f1\"],\n",
    "        \"roc_auc\": metrics_dict[\"roc_auc\"],\n",
    "    },\n",
    "    \"confusion_matrix\": {\n",
    "        \"tn\": metrics_dict[\"tn\"],\n",
    "        \"fp\": metrics_dict[\"fp\"],\n",
    "        \"fn\": metrics_dict[\"fn\"],\n",
    "        \"tp\": metrics_dict[\"tp\"],\n",
    "    },\n",
    "    \"timestamp\": datetime.utcnow().isoformat(),\n",
    "    \"notes\": \"DistilBERT fine-tuned on text_basic for hate speech detection\",\n",
    "}\n",
    "\n",
    "with open(METRICS_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_json, f, indent=2)\n",
    "\n",
    "output_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8659af",
   "metadata": {},
   "source": [
    "## 14. Funci√≥n de inferencia r√°pida\n",
    "\n",
    "Definimos una peque√±a funci√≥n para probar el modelo con un texto suelto.\n",
    "Esto viene muy bien para hacer pruebas desde el notebook y tambi√©n como base\n",
    "para el endpoint del backend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4179b406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'You are disgusting, go away!',\n",
       " 'predicted_label': 1,\n",
       " 'label_name': 'TOXIC (1)',\n",
       " 'score': 0.9710241556167603}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {0: \"NO TOXIC (0)\", 1: \"TOXIC (1)\"}\n",
    "\n",
    "def predict_text(text: str):\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1).numpy()[0]\n",
    "        pred = int(np.argmax(probs))\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"predicted_label\": pred,\n",
    "        \"label_name\": id2label[pred],\n",
    "        \"score\": float(probs[pred]),\n",
    "    }\n",
    "\n",
    "test_example = \"You are disgusting, go away!\"\n",
    "predict_text(test_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b325676",
   "metadata": {},
   "source": [
    "## 15. ¬øQu√© es un transformer? Explicaci√≥n intuitiva\n",
    "\n",
    "Imagina una frase:\n",
    "\n",
    "> \"This video is sick!\"\n",
    "\n",
    "La palabra **\"sick\"** puede significar _enfermo_ o _muy bueno_.  \n",
    "Para entenderlo bien, necesitamos mirar el **contexto completo**.\n",
    "\n",
    "### Modelos cl√°sicos (RNN / LSTM)\n",
    "\n",
    "- Leen la frase palabra a palabra: `This ‚Üí video ‚Üí is ‚Üí sick!`\n",
    "- Intentan recordar el contexto en una \"memoria\" interna.\n",
    "- Si la frase es larga, esa memoria se degrada y se olvidan cosas.\n",
    "\n",
    "### Transformers: atenci√≥n a todo\n",
    "\n",
    "Un transformer hace algo distinto:  \n",
    "para cada palabra, mira a **todas las dem√°s** y decide qu√© tan importantes son.\n",
    "\n",
    "Para la palabra `sick`, puede aprender:\n",
    "\n",
    "- que `video` es muy relevante,\n",
    "- que `This` o `is` importan menos,\n",
    "- y que el signo `!` puede indicar emoci√≥n.\n",
    "\n",
    "Esto se llama **self-attention (auto-atenci√≥n)**.\n",
    "\n",
    "Matem√°ticamente, se calculan pesos del tipo:\n",
    "\n",
    "- attention(`sick`, `video`) = 0.91  \n",
    "- attention(`sick`, `this`) = 0.13\n",
    "\n",
    "Con esos pesos, el modelo mezcla la informaci√≥n de todas las palabras y\n",
    "construye una representaci√≥n muy rica de la frase completa.\n",
    "\n",
    "### ¬øPor qu√© es tan potente?\n",
    "\n",
    "- No lee el texto solo de izquierda a derecha, sino que ve la frase **entera de golpe**.\n",
    "- Puede capturar relaciones entre palabras lejanas.\n",
    "- Funciona genial para:\n",
    "  - detecci√≥n de toxicidad\n",
    "  - an√°lisis de sentimiento\n",
    "  - traducci√≥n\n",
    "  - chatbots (como este proyecto üí¨)\n",
    "\n",
    "### ¬øY DistilBERT qu√© es?\n",
    "\n",
    "- Es una versi√≥n **reducida y m√°s r√°pida** de BERT.\n",
    "- Conserva la mayor parte de su rendimiento (~95% de la calidad).\n",
    "- Es ideal para proyectos de producci√≥n o demos donde queremos:\n",
    "  - buen rendimiento,\n",
    "  - pero sin un modelo gigantesco.\n",
    "\n",
    "En nuestro caso:\n",
    "\n",
    "- DistilBERT ya viene **pre-entrenado** en much√≠simo texto.\n",
    "- Solo le hemos hecho _fine-tuning_ para que aprenda una tarea muy concreta:\n",
    "  decidir si un comentario de YouTube es **t√≥xico (1)** o **no t√≥xico (0)**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
