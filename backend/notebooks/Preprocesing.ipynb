{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4f41ab",
   "metadata": {},
   "source": [
    "1. Carga de datos y librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffa5b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c65e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv(\"../../data/processed_eda/dataset_eda.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328471c",
   "metadata": {},
   "source": [
    "2. Limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b093edf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üßº LIMPIEZA DE TEXTO\n",
      "============================================================\n",
      "üîÑ Limpiando textos...\n",
      "‚úÖ 997 textos procesados correctamente\n",
      "\n",
      "============================================================\n",
      "üìã EJEMPLOS DE TRANSFORMACI√ìN\n",
      "============================================================\n",
      "\n",
      "[Ejemplo 1]\n",
      "Original (1558 chars): If only people would just take a step back and not make this case about them, because it wasn't about anyone except the \n",
      "Limpio   (1518 chars): if only people would just take a step back and not make this case about them because it wasn t about anyone except the t\n",
      "Palabras: 287 -> 288 (+1)\n",
      "\n",
      "[Ejemplo 2]\n",
      "Original (138 chars): Law enforcement is not trained to shoot to apprehend. ¬†They are trained to shoot to kill. ¬†And I thank Wilson for killin\n",
      "Limpio   (133 chars): law enforcement is not trained to shoot to apprehend they are trained to shoot to kill and i thank wilson for killing th\n",
      "Palabras: 25 -> 25 (+0)\n",
      "\n",
      "[Ejemplo 3]\n",
      "Original (420 chars): \n",
      "Dont you reckon them 'black lives matter' banners being held by white cunts is ¬†kinda patronizing and ironically racist\n",
      "Limpio   (406 chars): dont you reckon them black lives matter banners being held by white cunts is kinda patronizing and ironically racist cou\n",
      "Palabras: 77 -> 77 (+0)\n",
      "\n",
      "[Ejemplo 4]\n",
      "Original (582 chars): There are a very large number of people who do not like police officers. They are called Criminals and its the reason we\n",
      "Limpio   (574 chars): there are a very large number of people who do not like police officers they are called criminals and its the reason we \n",
      "Palabras: 107 -> 107 (+0)\n",
      "\n",
      "[Ejemplo 5]\n",
      "Original (243 chars): The Arab dude is absolutely right, he should have not been shot 6 extra time. Shoot him once if hes attacking you and th\n",
      "Limpio   (234 chars): the arab dude is absolutely right he should have not been shot extra time shoot him once if hes attacking you and that w\n",
      "Palabras: 47 -> 47 (+0)\n",
      "\n",
      "============================================================\n",
      "üóëÔ∏è DETECCI√ìN DE TEXTOS VAC√çOS\n",
      "============================================================\n",
      "‚ö†Ô∏è Textos vac√≠os tras limpieza: 0\n",
      "‚úÖ No hay textos vac√≠os\n",
      "\n",
      "============================================================\n",
      "üìä ESTAD√çSTICAS DE TEXTOS LIMPIOS\n",
      "============================================================\n",
      "count    997.000000\n",
      "mean      34.199599\n",
      "std       49.066868\n",
      "min        1.000000\n",
      "25%        9.000000\n",
      "50%       19.000000\n",
      "75%       40.000000\n",
      "max      805.000000\n",
      "Name: length_clean, dtype: float64\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è AN√ÅLISIS DE TEXTOS CORTOS\n",
      "============================================================\n",
      "Textos con ‚â§2 palabras: 36 (3.6%)\n",
      "\n",
      "Ejemplos de textos muy cortos:\n",
      "1. Original: '1 word: Provocateur........'\n",
      "   Limpio  : 'word provocateur'\n",
      "2. Original: 'Wow!!!'\n",
      "   Limpio  : 'wow'\n",
      "3. Original: 'CHRISTIANS ROCK!'\n",
      "   Limpio  : 'christians rock'\n",
      "4. Original: 'techno party!!!'\n",
      "   Limpio  : 'techno party'\n",
      "5. Original: 'Perfect police'\n",
      "   Limpio  : 'perfect police'\n",
      "\n",
      "============================================================\n",
      "üìè COMPARACI√ìN: ORIGINAL VS LIMPIO\n",
      "============================================================\n",
      "Longitud promedio ORIGINAL: 33.87 palabras\n",
      "Longitud promedio LIMPIO  : 34.20 palabras\n",
      "Reducci√≥n promedio        : -1.0%\n",
      "\n",
      "Longitud m√°xima ORIGINAL  : 815 palabras\n",
      "Longitud m√°xima LIMPIA    : 805 palabras\n",
      "\n",
      "Longitud m√≠nima ORIGINAL  : 1 palabras\n",
      "Longitud m√≠nima LIMPIA    : 1 palabras\n",
      "\n",
      "============================================================\n",
      "üòÄ VERIFICACI√ìN DE CONVERSI√ìN DE EMOJIS\n",
      "============================================================\n",
      "Textos con posibles emojis: 606\n",
      "\n",
      "Ejemplos de conversi√≥n de emojis:\n",
      "\n",
      "1. Original: If only people would just take a step back and not make this case about them, because it wasn't abou\n",
      "   Limpio  : if only people would just take a step back and not make this case about them because it wasn t about\n",
      "\n",
      "2. Original: \n",
      "Dont you reckon them 'black lives matter' banners being held by white cunts is ¬†kinda patronizing a\n",
      "   Limpio  : dont you reckon them black lives matter banners being held by white cunts is kinda patronizing and i\n",
      "\n",
      "3. Original: The Arab dude is absolutely right, he should have not been shot 6 extra time. Shoot him once if hes \n",
      "   Limpio  : the arab dude is absolutely right he should have not been shot extra time shoot him once if hes atta\n",
      "\n",
      "============================================================\n",
      "üìö AN√ÅLISIS DE VOCABULARIO\n",
      "============================================================\n",
      "Total de palabras (con repeticiones): 34,097\n",
      "Palabras √∫nicas (vocabulario)       : 4,459\n",
      "Ratio de diversidad                 : 0.1308\n",
      "\n",
      "üîù Top 20 palabras m√°s frecuentes:\n",
      " 1. the             ->  1,549 veces (4.54%)\n",
      " 2. and             ->    818 veces (2.40%)\n",
      " 3. to              ->    817 veces (2.40%)\n",
      " 4. a               ->    791 veces (2.32%)\n",
      " 5. of              ->    606 veces (1.78%)\n",
      " 6. is              ->    541 veces (1.59%)\n",
      " 7. that            ->    490 veces (1.44%)\n",
      " 8. you             ->    489 veces (1.43%)\n",
      " 9. i               ->    473 veces (1.39%)\n",
      "10. in              ->    440 veces (1.29%)\n",
      "11. this            ->    393 veces (1.15%)\n",
      "12. it              ->    347 veces (1.02%)\n",
      "13. are             ->    321 veces (0.94%)\n",
      "14. they            ->    280 veces (0.82%)\n",
      "15. was             ->    261 veces (0.77%)\n",
      "16. he              ->    260 veces (0.76%)\n",
      "17. people          ->    258 veces (0.76%)\n",
      "18. black           ->    254 veces (0.74%)\n",
      "19. for             ->    253 veces (0.74%)\n",
      "20. t               ->    239 veces (0.70%)\n",
      "\n",
      "============================================================\n",
      "‚úÖ LIMPIEZA COMPLETADA\n",
      "============================================================\n",
      "üìä Dataset final        : 997 comentarios\n",
      "üìù Vocabulario          : 4,459 palabras √∫nicas\n",
      "üìè Longitud promedio    : 34.20 palabras\n",
      "üìâ Reducci√≥n de tama√±o  : -1.0%\n",
      "\n",
      "üöÄ Datos listos para vectorizaci√≥n y modelado NLP\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üßº LIMPIEZA DE TEXTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpieza de texto enfocada en hate speech detection\n",
    "    \n",
    "    Proceso:\n",
    "    1. Min√∫sculas\n",
    "    2. Emojis a texto en INGL√âS (üòÇ -> face with tears of joy)\n",
    "    3. Eliminar URLs y menciones\n",
    "    4. Mantener hashtags como palabras\n",
    "    5. Mantener solo letras y espacios (espa√±ol + ingl√©s)\n",
    "    6. Reducir repeticiones de letras\n",
    "    7. Normalizar espacios\n",
    "    \"\"\"\n",
    "    # Convertir a string y min√∫sculas\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Codificaci√≥n UTF-8 segura\n",
    "    text = text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
    "    \n",
    "    # Convertir emojis a texto EN INGL√âS\n",
    "    # üòÇ -> :face_with_tears_of_joy:\n",
    "    text = emoji.demojize(text, language='en')\n",
    "    \n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    \n",
    "    # Eliminar menciones (@usuario)\n",
    "    text = re.sub(r\"@\\w+\", \" \", text)\n",
    "    \n",
    "    # Convertir hashtags a palabras (#libertad -> libertad)\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "    \n",
    "    # OPCI√ìN 2: Mantener SOLO letras y espacios\n",
    "    # Esto convierte :face_with_tears_of_joy: -> face with tears of joy\n",
    "    # Mantiene espa√±ol (√°√©√≠√≥√∫√±√º) + ingl√©s (a-z)\n",
    "    text = re.sub(r\"[^a-z√°√©√≠√≥√∫√±√º\\s]\", \" \", text)\n",
    "    \n",
    "    # Reducir letras repetidas (hooola -> hola, soooo -> so)\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\", text)\n",
    "    \n",
    "    # Normalizar espacios m√∫ltiples y quitar espacios al inicio/fin\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Aplicar limpieza\n",
    "print(\"üîÑ Limpiando textos...\")\n",
    "df[\"text_clean\"] = df[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Verificar que la limpieza funcion√≥\n",
    "print(f\"‚úÖ {len(df)} textos procesados correctamente\")\n",
    "\n",
    "# ==========================================\n",
    "# MOSTRAR EJEMPLOS DE TRANSFORMACI√ìN\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã EJEMPLOS DE TRANSFORMACI√ìN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\n[Ejemplo {i+1}]\")\n",
    "    original = df['Text'].iloc[i]\n",
    "    limpio = df['text_clean'].iloc[i]\n",
    "    \n",
    "    # Mostrar hasta 120 caracteres\n",
    "    print(f\"Original ({len(original)} chars): {original[:120]}\")\n",
    "    print(f\"Limpio   ({len(limpio)} chars): {limpio[:120]}\")\n",
    "    \n",
    "    # Mostrar diferencia de longitud\n",
    "    original_words = len(original.split())\n",
    "    clean_words = len(limpio.split())\n",
    "    print(f\"Palabras: {original_words} -> {clean_words} ({clean_words-original_words:+d})\")\n",
    "\n",
    "# ==========================================\n",
    "# DETECTAR Y ELIMINAR TEXTOS VAC√çOS\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üóëÔ∏è DETECCI√ìN DE TEXTOS VAC√çOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "empty_after_clean = df[df['text_clean'].str.strip() == '']\n",
    "print(f\"‚ö†Ô∏è Textos vac√≠os tras limpieza: {len(empty_after_clean)}\")\n",
    "\n",
    "if len(empty_after_clean) > 0:\n",
    "    print(\"\\nEjemplos de textos que quedaron vac√≠os:\")\n",
    "    for idx, original in empty_after_clean['Text'].head(5).items():\n",
    "        print(f\"  - '{original[:80]}'\")\n",
    "    \n",
    "    print(f\"\\nüóëÔ∏è Eliminando {len(empty_after_clean)} textos vac√≠os...\")\n",
    "    df = df[df['text_clean'].str.strip() != ''].reset_index(drop=True)\n",
    "    print(f\"‚úÖ Dataset final: {len(df)} filas\")\n",
    "else:\n",
    "    print(\"‚úÖ No hay textos vac√≠os\")\n",
    "\n",
    "# ==========================================\n",
    "# RECALCULAR LONGITUD DE TEXTOS LIMPIOS\n",
    "# ==========================================\n",
    "df['length_clean'] = df['text_clean'].str.split().str.len()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä ESTAD√çSTICAS DE TEXTOS LIMPIOS\")\n",
    "print(\"=\" * 60)\n",
    "print(df['length_clean'].describe())\n",
    "\n",
    "# ==========================================\n",
    "# AN√ÅLISIS DE TEXTOS MUY CORTOS\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚ö†Ô∏è AN√ÅLISIS DE TEXTOS CORTOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "very_short = df[df['length_clean'] <= 2]\n",
    "print(f\"Textos con ‚â§2 palabras: {len(very_short)} ({len(very_short)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(very_short) > 0:\n",
    "    print(\"\\nEjemplos de textos muy cortos:\")\n",
    "    for i, (original, clean) in enumerate(zip(very_short['Text'].head(5), \n",
    "                                               very_short['text_clean'].head(5)), 1):\n",
    "        print(f\"{i}. Original: '{original[:60]}'\")\n",
    "        print(f\"   Limpio  : '{clean}'\")\n",
    "\n",
    "# ==========================================\n",
    "# COMPARACI√ìN ORIGINAL VS LIMPIO\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìè COMPARACI√ìN: ORIGINAL VS LIMPIO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcular longitud original\n",
    "df['length_original'] = df['Text'].str.split().str.len()\n",
    "\n",
    "print(f\"Longitud promedio ORIGINAL: {df['length_original'].mean():.2f} palabras\")\n",
    "print(f\"Longitud promedio LIMPIO  : {df['length_clean'].mean():.2f} palabras\")\n",
    "\n",
    "reduccion = ((df['length_original'].mean() - df['length_clean'].mean()) / df['length_original'].mean()) * 100\n",
    "print(f\"Reducci√≥n promedio        : {reduccion:.1f}%\")\n",
    "\n",
    "print(f\"\\nLongitud m√°xima ORIGINAL  : {df['length_original'].max()} palabras\")\n",
    "print(f\"Longitud m√°xima LIMPIA    : {df['length_clean'].max()} palabras\")\n",
    "\n",
    "print(f\"\\nLongitud m√≠nima ORIGINAL  : {df['length_original'].min()} palabras\")\n",
    "print(f\"Longitud m√≠nima LIMPIA    : {df['length_clean'].min()} palabras\")\n",
    "\n",
    "# ==========================================\n",
    "# VERIFICAR CONVERSI√ìN DE EMOJIS\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üòÄ VERIFICACI√ìN DE CONVERSI√ìN DE EMOJIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Buscar textos que originalmente ten√≠an emojis\n",
    "textos_con_emoji = df[df['Text'].str.contains(r'[^\\w\\s,.]', regex=True, na=False)]\n",
    "print(f\"Textos con posibles emojis: {len(textos_con_emoji)}\")\n",
    "\n",
    "if len(textos_con_emoji) > 0:\n",
    "    print(\"\\nEjemplos de conversi√≥n de emojis:\")\n",
    "    for i, (original, clean) in enumerate(zip(textos_con_emoji['Text'].head(3), \n",
    "                                               textos_con_emoji['text_clean'].head(3)), 1):\n",
    "        print(f\"\\n{i}. Original: {original[:100]}\")\n",
    "        print(f\"   Limpio  : {clean[:100]}\")\n",
    "\n",
    "# ==========================================\n",
    "# AN√ÅLISIS DE VOCABULARIO\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìö AN√ÅLISIS DE VOCABULARIO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Contar palabras √∫nicas\n",
    "all_words = ' '.join(df['text_clean']).split()\n",
    "unique_words = set(all_words)\n",
    "\n",
    "print(f\"Total de palabras (con repeticiones): {len(all_words):,}\")\n",
    "print(f\"Palabras √∫nicas (vocabulario)       : {len(unique_words):,}\")\n",
    "print(f\"Ratio de diversidad                 : {len(unique_words)/len(all_words):.4f}\")\n",
    "\n",
    "# Palabras m√°s frecuentes\n",
    "from collections import Counter\n",
    "word_freq = Counter(all_words)\n",
    "print(\"\\nüîù Top 20 palabras m√°s frecuentes:\")\n",
    "for i, (word, count) in enumerate(word_freq.most_common(20), 1):\n",
    "    print(f\"{i:2}. {word:15} -> {count:6,} veces ({count/len(all_words)*100:.2f}%)\")\n",
    "\n",
    "# ==========================================\n",
    "# RESUMEN FINAL\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ LIMPIEZA COMPLETADA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Dataset final        : {len(df):,} comentarios\")\n",
    "print(f\"üìù Vocabulario          : {len(unique_words):,} palabras √∫nicas\")\n",
    "print(f\"üìè Longitud promedio    : {df['length_clean'].mean():.2f} palabras\")\n",
    "print(f\"üìâ Reducci√≥n de tama√±o  : {reduccion:.1f}%\")\n",
    "print(f\"\\nüöÄ Datos listos para vectorizaci√≥n y modelado NLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52a329",
   "metadata": {},
   "source": [
    "3. Normalizaci√≥n con spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "163f5c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizando textos en ingl√©s con spaCy (manteniendo negaciones)...\n",
      "\n",
      "[Ejemplo 1]\n",
      "Limpio     : if only people would just take a step back and not make this case about them because it wasn t about anyone except the two people in that situation to lump yourself into this mess and take matters into your own hands makes these kinds of protests selfish and without rational thought and investigation the guy in this video is heavily emotional and hyped up and wants to be heard and when he gets heard he just presses more and more he was never out to have a reasonable discussion kudos to the smerconish for keeping level the whole time and letting masri make himself out to be a fool how dare he and those that tore that city down in protest make this about themselves and to dishonor the entire incident with their own hate by the way since when did police brutality become an epidemic i wish everyone would just stop pretending like they were there and they knew exactly what was going on because there s no measurable amount of people that honestly witnessed this incident so none of us have a clue on which way this whole issue should have swung the grand jury were the most informed we have to trust the majority rule was the right course of action and let it be also thank you to the of police officers in america that actually serve protect even if you re a bit of a jerk when you pull me over i respect your job and know that someone has to do it and that many people are going to pout about being held accountable to their actions people hate police until they need an officer or two around in an emergency\n",
      "Normalizado: people step not case wasn t people situation lump mess matter hand make kind protest selfish rational thought investigation guy video heavily emotional hype want hear get hear press never reasonable discussion kudo smerconish keep level time let masri fool dare tear city protest dishonor entire incident hate way police brutality epidemic wish stop pretend like know exactly go s no measurable people honestly witness incident none clue way issue swing grand jury informed trust majority rule right course action let thank police officer america actually serve protect bit jerk pull respect job know people go pout hold accountable action people hate police need officer emergency\n",
      "\n",
      "[Ejemplo 2]\n",
      "Limpio     : law enforcement is not trained to shoot to apprehend they are trained to shoot to kill and i thank wilson for killing that punk bitch\n",
      "Normalizado: law enforcement not train shoot apprehend train shoot kill thank wilson kill punk bitch\n",
      "\n",
      "[Ejemplo 3]\n",
      "Limpio     : dont you reckon them black lives matter banners being held by white cunts is kinda patronizing and ironically racist could they have not come up with somethin better or is it just what white folks do to give them selves pride o look at me im being nice for the black people why does it always have to be about race actually the whole world is pussyfootin around for fear of being racist its fuckin daft man\n",
      "Normalizado: not reckon black life matter banner hold white cunt kinda patronizing ironically racist not come somethin well white folk self pride o look m nice black people race actually world pussyfootin fear racist fuckin daft man\n",
      "\n",
      "[Ejemplo 4]\n",
      "Limpio     : there are a very large number of people who do not like police officers they are called criminals and its the reason we have police officers the fact that criminals do not like police officers is a testament to the good work that police officers do in protecting the public when our children or our family are in danger we do not hessitate to call for help and we call the police its about time people stopped complaining and started to give the police some respect for the hard work and dedication that often requires them to put their lives on the line to serve the public\n",
      "Normalizado: large number people not like police officer call criminal reason police officer fact criminal not like police officer testament good work police officer protect public child family danger not hessitate help police time people stop complain start police respect hard work dedication require life line serve public\n",
      "\n",
      "[Ejemplo 5]\n",
      "Limpio     : the arab dude is absolutely right he should have not been shot extra time shoot him once if hes attacking you and that would stop his attack shoot him twice if he s still attacking you but six times that is shoot to kill in my opinion\n",
      "Normalizado: arab dude absolutely right not shoot extra time shoot s attack stop attack shoot twice s attack time shoot kill opinion\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar modelo spaCy en ingl√©s\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Lista de negaciones que queremos conservar\n",
    "negations = {\"no\", \"not\", \"never\", \"none\", \"nobody\", \"nothing\", \"neither\", \"nor\", \"cannot\"}\n",
    "\n",
    "def normalize_text_spacy(text):\n",
    "    \"\"\"\n",
    "    Normalizaci√≥n en ingl√©s con spaCy:\n",
    "    1. Tokenizar\n",
    "    2. Lematizar\n",
    "    3. Eliminar stopwords (excepto negaciones)\n",
    "    4. Mantener solo tokens alfab√©ticos\n",
    "    \"\"\"\n",
    "    doc = nlp_en(text)\n",
    "\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.is_alpha and (not token.is_stop or token.text.lower() in negations)\n",
    "    ]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Aplicar normalizaci√≥n sobre tu columna ya limpia\n",
    "print(\"üîÑ Normalizando textos en ingl√©s con spaCy (manteniendo negaciones)...\")\n",
    "df[\"text_norm\"] = df[\"text_clean\"].apply(normalize_text_spacy)\n",
    "\n",
    "# Ejemplos de verificaci√≥n\n",
    "for i in range(5):\n",
    "    print(f\"\\n[Ejemplo {i+1}]\")\n",
    "    print(\"Limpio     :\", df[\"text_clean\"].iloc[i])\n",
    "    print(\"Normalizado:\", df[\"text_norm\"].iloc[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa5003",
   "metadata": {},
   "source": [
    "5. Guardado del Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b5553d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Verificaci√≥n previa al guardado\n",
      "                                          text_clean  \\\n",
      "0  if only people would just take a step back and...   \n",
      "1  law enforcement is not trained to shoot to app...   \n",
      "2  dont you reckon them black lives matter banner...   \n",
      "3  there are a very large number of people who do...   \n",
      "4  the arab dude is absolutely right he should ha...   \n",
      "5  here people his facebook is he has ties with i...   \n",
      "6  check out this you tube post black man goes on...   \n",
      "7  i would love to see this pussy go to staten is...   \n",
      "8                         i agree with the protestor   \n",
      "9     mike browns father was made to say that boshit   \n",
      "\n",
      "                                           text_norm  \n",
      "0  people step not case wasn t people situation l...  \n",
      "1  law enforcement not train shoot apprehend trai...  \n",
      "2  not reckon black life matter banner hold white...  \n",
      "3  large number people not like police officer ca...  \n",
      "4  arab dude absolutely right not shoot extra tim...  \n",
      "5  people facebook tie isis terrorist group musli...  \n",
      "6  check tube post black man go epic rant ferguso...  \n",
      "7  love pussy staten island spit cop love happen ...  \n",
      "8                                    agree protestor  \n",
      "9                          mike browns father boshit  \n",
      "‚úÖ Dataset guardado con columnas text_clean y text_norm en: ../../data/comentarios_preprocesados.csv\n"
     ]
    }
   ],
   "source": [
    "# Verificar que la normalizaci√≥n est√° en el DataFrame\n",
    "print(\"\\nüìã Verificaci√≥n previa al guardado\")\n",
    "print(df[['text_clean','text_norm']].head(10))\n",
    "\n",
    "# Guardar dataset con ambas columnas\n",
    "output_file = \"../../data/comentarios_preprocesados.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Dataset guardado con columnas text_clean y text_norm en: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b81a28",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '..\\..\\data\\processing_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Guardar el dataset normalizado en CSV\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m     11\u001b[39m output_file_csv = \u001b[33m\"\u001b[39m\u001b[33m../../data/processing_data/comentarios_preprocesados.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Dataset guardado con preprocesamiento y columnas objetivo en formato CSV en: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file_csv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Guardar el dataset normalizado en PKL\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andres\\Desktop\\Bootcamp_F5_IA\\1- Proyectos Grupales\\Proyecto_X_NLP_G4\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andres\\Desktop\\Bootcamp_F5_IA\\1- Proyectos Grupales\\Proyecto_X_NLP_G4\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3989\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3978\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3980\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3981\u001b[39m     frame=df,\n\u001b[32m   3982\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3986\u001b[39m     decimal=decimal,\n\u001b[32m   3987\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3989\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4006\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andres\\Desktop\\Bootcamp_F5_IA\\1- Proyectos Grupales\\Proyecto_X_NLP_G4\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andres\\Desktop\\Bootcamp_F5_IA\\1- Proyectos Grupales\\Proyecto_X_NLP_G4\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andres\\Desktop\\Bootcamp_F5_IA\\1- Proyectos Grupales\\Proyecto_X_NLP_G4\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andres\\Desktop\\Bootcamp_F5_IA\\1- Proyectos Grupales\\Proyecto_X_NLP_G4\\Proyecto_X_NLP_G4\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '..\\..\\data\\processing_data'"
     ]
    }
   ],
   "source": [
    "# Seleccionar √∫nicamente las columnas necesarias\n",
    "cols_to_keep = [\n",
    "    \"CommentId\",\"VideoId\",\"text_norm\", \"IsToxic\", \"IsAbusive\", \"IsThreat\",\n",
    "    \"IsProvocative\", \"IsObscene\", \"IsHatespeech\", \"IsRacist\", \"IsNationalist\",\n",
    "    \"IsSexist\", \"IsHomophobic\", \"IsReligiousHate\", \"IsRadicalism\"\n",
    "]\n",
    "df = df[cols_to_keep]\n",
    "# ==========================================\n",
    "# Guardar el dataset normalizado en CSV\n",
    "# ==========================================\n",
    "output_file_csv = \"../../data/preprocessing_data/comentarios_preprocesados.csv\"\n",
    "df.to_csv(output_file_csv, index=False)\n",
    "print(f\"‚úÖ Dataset guardado con preprocesamiento y columnas objetivo en formato CSV en: {output_file_csv}\")\n",
    "\n",
    "# ==========================================\n",
    "# Guardar el dataset normalizado en PKL\n",
    "# ==========================================\n",
    "output_file_pkl = \"../../data/preprocessing_data/comentarios_preprocesados.pkl\"\n",
    "df.to_pickle(output_file_pkl)\n",
    "print(f\"‚úÖ Dataset guardado con preprocesamiento y columnas objetivo en formato PKL en: {output_file_pkl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc712c97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
