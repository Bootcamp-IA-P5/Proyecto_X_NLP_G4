{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4f41ab",
   "metadata": {},
   "source": [
    "1. Carga de datos y librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffa5b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c65e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv(\"../../data/dataset_eda.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328471c",
   "metadata": {},
   "source": [
    "2. Limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b093edf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üßº LIMPIEZA DE TEXTO\n",
      "============================================================\n",
      "üîÑ Limpiando textos...\n",
      "‚úÖ 997 textos procesados correctamente\n",
      "\n",
      "============================================================\n",
      "üìã EJEMPLOS DE TRANSFORMACI√ìN\n",
      "============================================================\n",
      "\n",
      "[Ejemplo 1]\n",
      "Original (1558 chars): If only people would just take a step back and not make this case about them, because it wasn't about anyone except the \n",
      "Limpio   (1518 chars): if only people would just take a step back and not make this case about them because it wasn t about anyone except the t\n",
      "Palabras: 287 -> 288 (+1)\n",
      "\n",
      "[Ejemplo 2]\n",
      "Original (138 chars): Law enforcement is not trained to shoot to apprehend. ¬†They are trained to shoot to kill. ¬†And I thank Wilson for killin\n",
      "Limpio   (133 chars): law enforcement is not trained to shoot to apprehend they are trained to shoot to kill and i thank wilson for killing th\n",
      "Palabras: 25 -> 25 (+0)\n",
      "\n",
      "[Ejemplo 3]\n",
      "Original (420 chars): \n",
      "Dont you reckon them 'black lives matter' banners being held by white cunts is ¬†kinda patronizing and ironically racist\n",
      "Limpio   (406 chars): dont you reckon them black lives matter banners being held by white cunts is kinda patronizing and ironically racist cou\n",
      "Palabras: 77 -> 77 (+0)\n",
      "\n",
      "[Ejemplo 4]\n",
      "Original (582 chars): There are a very large number of people who do not like police officers. They are called Criminals and its the reason we\n",
      "Limpio   (574 chars): there are a very large number of people who do not like police officers they are called criminals and its the reason we \n",
      "Palabras: 107 -> 107 (+0)\n",
      "\n",
      "[Ejemplo 5]\n",
      "Original (243 chars): The Arab dude is absolutely right, he should have not been shot 6 extra time. Shoot him once if hes attacking you and th\n",
      "Limpio   (234 chars): the arab dude is absolutely right he should have not been shot extra time shoot him once if hes attacking you and that w\n",
      "Palabras: 47 -> 47 (+0)\n",
      "\n",
      "============================================================\n",
      "üóëÔ∏è DETECCI√ìN DE TEXTOS VAC√çOS\n",
      "============================================================\n",
      "‚ö†Ô∏è Textos vac√≠os tras limpieza: 0\n",
      "‚úÖ No hay textos vac√≠os\n",
      "\n",
      "============================================================\n",
      "üìä ESTAD√çSTICAS DE TEXTOS LIMPIOS\n",
      "============================================================\n",
      "count    997.000000\n",
      "mean      34.199599\n",
      "std       49.066868\n",
      "min        1.000000\n",
      "25%        9.000000\n",
      "50%       19.000000\n",
      "75%       40.000000\n",
      "max      805.000000\n",
      "Name: length_clean, dtype: float64\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è AN√ÅLISIS DE TEXTOS CORTOS\n",
      "============================================================\n",
      "Textos con ‚â§2 palabras: 36 (3.6%)\n",
      "\n",
      "Ejemplos de textos muy cortos:\n",
      "1. Original: '1 word: Provocateur........'\n",
      "   Limpio  : 'word provocateur'\n",
      "2. Original: 'Wow!!!'\n",
      "   Limpio  : 'wow'\n",
      "3. Original: 'CHRISTIANS ROCK!'\n",
      "   Limpio  : 'christians rock'\n",
      "4. Original: 'techno party!!!'\n",
      "   Limpio  : 'techno party'\n",
      "5. Original: 'Perfect police'\n",
      "   Limpio  : 'perfect police'\n",
      "\n",
      "============================================================\n",
      "üìè COMPARACI√ìN: ORIGINAL VS LIMPIO\n",
      "============================================================\n",
      "Longitud promedio ORIGINAL: 33.87 palabras\n",
      "Longitud promedio LIMPIO  : 34.20 palabras\n",
      "Reducci√≥n promedio        : -1.0%\n",
      "\n",
      "Longitud m√°xima ORIGINAL  : 815 palabras\n",
      "Longitud m√°xima LIMPIA    : 805 palabras\n",
      "\n",
      "Longitud m√≠nima ORIGINAL  : 1 palabras\n",
      "Longitud m√≠nima LIMPIA    : 1 palabras\n",
      "\n",
      "============================================================\n",
      "üòÄ VERIFICACI√ìN DE CONVERSI√ìN DE EMOJIS\n",
      "============================================================\n",
      "Textos con posibles emojis: 606\n",
      "\n",
      "Ejemplos de conversi√≥n de emojis:\n",
      "\n",
      "1. Original: If only people would just take a step back and not make this case about them, because it wasn't abou\n",
      "   Limpio  : if only people would just take a step back and not make this case about them because it wasn t about\n",
      "\n",
      "2. Original: \n",
      "Dont you reckon them 'black lives matter' banners being held by white cunts is ¬†kinda patronizing a\n",
      "   Limpio  : dont you reckon them black lives matter banners being held by white cunts is kinda patronizing and i\n",
      "\n",
      "3. Original: The Arab dude is absolutely right, he should have not been shot 6 extra time. Shoot him once if hes \n",
      "   Limpio  : the arab dude is absolutely right he should have not been shot extra time shoot him once if hes atta\n",
      "\n",
      "============================================================\n",
      "üìö AN√ÅLISIS DE VOCABULARIO\n",
      "============================================================\n",
      "Total de palabras (con repeticiones): 34,097\n",
      "Palabras √∫nicas (vocabulario)       : 4,459\n",
      "Ratio de diversidad                 : 0.1308\n",
      "\n",
      "üîù Top 20 palabras m√°s frecuentes:\n",
      " 1. the             ->  1,549 veces (4.54%)\n",
      " 2. and             ->    818 veces (2.40%)\n",
      " 3. to              ->    817 veces (2.40%)\n",
      " 4. a               ->    791 veces (2.32%)\n",
      " 5. of              ->    606 veces (1.78%)\n",
      " 6. is              ->    541 veces (1.59%)\n",
      " 7. that            ->    490 veces (1.44%)\n",
      " 8. you             ->    489 veces (1.43%)\n",
      " 9. i               ->    473 veces (1.39%)\n",
      "10. in              ->    440 veces (1.29%)\n",
      "11. this            ->    393 veces (1.15%)\n",
      "12. it              ->    347 veces (1.02%)\n",
      "13. are             ->    321 veces (0.94%)\n",
      "14. they            ->    280 veces (0.82%)\n",
      "15. was             ->    261 veces (0.77%)\n",
      "16. he              ->    260 veces (0.76%)\n",
      "17. people          ->    258 veces (0.76%)\n",
      "18. black           ->    254 veces (0.74%)\n",
      "19. for             ->    253 veces (0.74%)\n",
      "20. t               ->    239 veces (0.70%)\n",
      "\n",
      "============================================================\n",
      "‚úÖ LIMPIEZA COMPLETADA\n",
      "============================================================\n",
      "üìä Dataset final        : 997 comentarios\n",
      "üìù Vocabulario          : 4,459 palabras √∫nicas\n",
      "üìè Longitud promedio    : 34.20 palabras\n",
      "üìâ Reducci√≥n de tama√±o  : -1.0%\n",
      "\n",
      "üöÄ Datos listos para vectorizaci√≥n y modelado NLP\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üßº LIMPIEZA DE TEXTO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpieza de texto enfocada en hate speech detection\n",
    "    \n",
    "    Proceso:\n",
    "    1. Min√∫sculas\n",
    "    2. Emojis a texto en INGL√âS (üòÇ -> face with tears of joy)\n",
    "    3. Eliminar URLs y menciones\n",
    "    4. Mantener hashtags como palabras\n",
    "    5. Mantener solo letras y espacios (espa√±ol + ingl√©s)\n",
    "    6. Reducir repeticiones de letras\n",
    "    7. Normalizar espacios\n",
    "    \"\"\"\n",
    "    # Convertir a string y min√∫sculas\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Codificaci√≥n UTF-8 segura\n",
    "    text = text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n",
    "    \n",
    "    # Convertir emojis a texto EN INGL√âS\n",
    "    # üòÇ -> :face_with_tears_of_joy:\n",
    "    text = emoji.demojize(text, language='en')\n",
    "    \n",
    "    # Eliminar URLs\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    \n",
    "    # Eliminar menciones (@usuario)\n",
    "    text = re.sub(r\"@\\w+\", \" \", text)\n",
    "    \n",
    "    # Convertir hashtags a palabras (#libertad -> libertad)\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "    \n",
    "    # OPCI√ìN 2: Mantener SOLO letras y espacios\n",
    "    # Esto convierte :face_with_tears_of_joy: -> face with tears of joy\n",
    "    # Mantiene espa√±ol (√°√©√≠√≥√∫√±√º) + ingl√©s (a-z)\n",
    "    text = re.sub(r\"[^a-z√°√©√≠√≥√∫√±√º\\s]\", \" \", text)\n",
    "    \n",
    "    # Reducir letras repetidas (hooola -> hola, soooo -> so)\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\", text)\n",
    "    \n",
    "    # Normalizar espacios m√∫ltiples y quitar espacios al inicio/fin\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Aplicar limpieza\n",
    "print(\"üîÑ Limpiando textos...\")\n",
    "df[\"text_clean\"] = df[\"Text\"].apply(clean_text)\n",
    "\n",
    "# Verificar que la limpieza funcion√≥\n",
    "print(f\"‚úÖ {len(df)} textos procesados correctamente\")\n",
    "\n",
    "# ==========================================\n",
    "# MOSTRAR EJEMPLOS DE TRANSFORMACI√ìN\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã EJEMPLOS DE TRANSFORMACI√ìN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"\\n[Ejemplo {i+1}]\")\n",
    "    original = df['Text'].iloc[i]\n",
    "    limpio = df['text_clean'].iloc[i]\n",
    "    \n",
    "    # Mostrar hasta 120 caracteres\n",
    "    print(f\"Original ({len(original)} chars): {original[:120]}\")\n",
    "    print(f\"Limpio   ({len(limpio)} chars): {limpio[:120]}\")\n",
    "    \n",
    "    # Mostrar diferencia de longitud\n",
    "    original_words = len(original.split())\n",
    "    clean_words = len(limpio.split())\n",
    "    print(f\"Palabras: {original_words} -> {clean_words} ({clean_words-original_words:+d})\")\n",
    "\n",
    "# ==========================================\n",
    "# DETECTAR Y ELIMINAR TEXTOS VAC√çOS\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üóëÔ∏è DETECCI√ìN DE TEXTOS VAC√çOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "empty_after_clean = df[df['text_clean'].str.strip() == '']\n",
    "print(f\"‚ö†Ô∏è Textos vac√≠os tras limpieza: {len(empty_after_clean)}\")\n",
    "\n",
    "if len(empty_after_clean) > 0:\n",
    "    print(\"\\nEjemplos de textos que quedaron vac√≠os:\")\n",
    "    for idx, original in empty_after_clean['Text'].head(5).items():\n",
    "        print(f\"  - '{original[:80]}'\")\n",
    "    \n",
    "    print(f\"\\nüóëÔ∏è Eliminando {len(empty_after_clean)} textos vac√≠os...\")\n",
    "    df = df[df['text_clean'].str.strip() != ''].reset_index(drop=True)\n",
    "    print(f\"‚úÖ Dataset final: {len(df)} filas\")\n",
    "else:\n",
    "    print(\"‚úÖ No hay textos vac√≠os\")\n",
    "\n",
    "# ==========================================\n",
    "# RECALCULAR LONGITUD DE TEXTOS LIMPIOS\n",
    "# ==========================================\n",
    "df['length_clean'] = df['text_clean'].str.split().str.len()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä ESTAD√çSTICAS DE TEXTOS LIMPIOS\")\n",
    "print(\"=\" * 60)\n",
    "print(df['length_clean'].describe())\n",
    "\n",
    "# ==========================================\n",
    "# AN√ÅLISIS DE TEXTOS MUY CORTOS\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚ö†Ô∏è AN√ÅLISIS DE TEXTOS CORTOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "very_short = df[df['length_clean'] <= 2]\n",
    "print(f\"Textos con ‚â§2 palabras: {len(very_short)} ({len(very_short)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(very_short) > 0:\n",
    "    print(\"\\nEjemplos de textos muy cortos:\")\n",
    "    for i, (original, clean) in enumerate(zip(very_short['Text'].head(5), \n",
    "                                               very_short['text_clean'].head(5)), 1):\n",
    "        print(f\"{i}. Original: '{original[:60]}'\")\n",
    "        print(f\"   Limpio  : '{clean}'\")\n",
    "\n",
    "# ==========================================\n",
    "# COMPARACI√ìN ORIGINAL VS LIMPIO\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìè COMPARACI√ìN: ORIGINAL VS LIMPIO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calcular longitud original\n",
    "df['length_original'] = df['Text'].str.split().str.len()\n",
    "\n",
    "print(f\"Longitud promedio ORIGINAL: {df['length_original'].mean():.2f} palabras\")\n",
    "print(f\"Longitud promedio LIMPIO  : {df['length_clean'].mean():.2f} palabras\")\n",
    "\n",
    "reduccion = ((df['length_original'].mean() - df['length_clean'].mean()) / df['length_original'].mean()) * 100\n",
    "print(f\"Reducci√≥n promedio        : {reduccion:.1f}%\")\n",
    "\n",
    "print(f\"\\nLongitud m√°xima ORIGINAL  : {df['length_original'].max()} palabras\")\n",
    "print(f\"Longitud m√°xima LIMPIA    : {df['length_clean'].max()} palabras\")\n",
    "\n",
    "print(f\"\\nLongitud m√≠nima ORIGINAL  : {df['length_original'].min()} palabras\")\n",
    "print(f\"Longitud m√≠nima LIMPIA    : {df['length_clean'].min()} palabras\")\n",
    "\n",
    "# ==========================================\n",
    "# VERIFICAR CONVERSI√ìN DE EMOJIS\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üòÄ VERIFICACI√ìN DE CONVERSI√ìN DE EMOJIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Buscar textos que originalmente ten√≠an emojis\n",
    "textos_con_emoji = df[df['Text'].str.contains(r'[^\\w\\s,.]', regex=True, na=False)]\n",
    "print(f\"Textos con posibles emojis: {len(textos_con_emoji)}\")\n",
    "\n",
    "if len(textos_con_emoji) > 0:\n",
    "    print(\"\\nEjemplos de conversi√≥n de emojis:\")\n",
    "    for i, (original, clean) in enumerate(zip(textos_con_emoji['Text'].head(3), \n",
    "                                               textos_con_emoji['text_clean'].head(3)), 1):\n",
    "        print(f\"\\n{i}. Original: {original[:100]}\")\n",
    "        print(f\"   Limpio  : {clean[:100]}\")\n",
    "\n",
    "# ==========================================\n",
    "# AN√ÅLISIS DE VOCABULARIO\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìö AN√ÅLISIS DE VOCABULARIO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Contar palabras √∫nicas\n",
    "all_words = ' '.join(df['text_clean']).split()\n",
    "unique_words = set(all_words)\n",
    "\n",
    "print(f\"Total de palabras (con repeticiones): {len(all_words):,}\")\n",
    "print(f\"Palabras √∫nicas (vocabulario)       : {len(unique_words):,}\")\n",
    "print(f\"Ratio de diversidad                 : {len(unique_words)/len(all_words):.4f}\")\n",
    "\n",
    "# Palabras m√°s frecuentes\n",
    "from collections import Counter\n",
    "word_freq = Counter(all_words)\n",
    "print(\"\\nüîù Top 20 palabras m√°s frecuentes:\")\n",
    "for i, (word, count) in enumerate(word_freq.most_common(20), 1):\n",
    "    print(f\"{i:2}. {word:15} -> {count:6,} veces ({count/len(all_words)*100:.2f}%)\")\n",
    "\n",
    "# ==========================================\n",
    "# RESUMEN FINAL\n",
    "# ==========================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ LIMPIEZA COMPLETADA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Dataset final        : {len(df):,} comentarios\")\n",
    "print(f\"üìù Vocabulario          : {len(unique_words):,} palabras √∫nicas\")\n",
    "print(f\"üìè Longitud promedio    : {df['length_clean'].mean():.2f} palabras\")\n",
    "print(f\"üìâ Reducci√≥n de tama√±o  : {reduccion:.1f}%\")\n",
    "print(f\"\\nüöÄ Datos listos para vectorizaci√≥n y modelado NLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52a329",
   "metadata": {},
   "source": [
    "3. Normalizaci√≥n biling√ºe con spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "163f5c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Normalizando textos biling√ºes con spaCy...\n",
      "\n",
      "[Ejemplo 1]\n",
      "Limpio     : if only people would just take a step back and not make this case about them because it wasn t about anyone except the two people in that situation to lump yourself into this mess and take matters into your own hands makes these kinds of protests selfish and without rational thought and investigation the guy in this video is heavily emotional and hyped up and wants to be heard and when he gets heard he just presses more and more he was never out to have a reasonable discussion kudos to the smerconish for keeping level the whole time and letting masri make himself out to be a fool how dare he and those that tore that city down in protest make this about themselves and to dishonor the entire incident with their own hate by the way since when did police brutality become an epidemic i wish everyone would just stop pretending like they were there and they knew exactly what was going on because there s no measurable amount of people that honestly witnessed this incident so none of us have a clue on which way this whole issue should have swung the grand jury were the most informed we have to trust the majority rule was the right course of action and let it be also thank you to the of police officers in america that actually serve protect even if you re a bit of a jerk when you pull me over i respect your job and know that someone has to do it and that many people are going to pout about being held accountable to their actions people hate police until they need an officer or two around in an emergency\n",
      "Normalizado: people step case wasn t people situation lump mess matter hand make kind protest selfish rational thought investigation guy video heavily emotional hype want hear get hear press reasonable discussion kudo smerconish keep level time let masri fool dare tear city protest dishonor entire incident hate way police brutality epidemic wish stop pretend like know exactly go s measurable people honestly witness incident clue way issue swing grand jury informed trust majority rule right course action let thank police officer america actually serve protect bit jerk pull respect job know people go pout hold accountable action people hate police need officer emergency\n",
      "\n",
      "[Ejemplo 2]\n",
      "Limpio     : law enforcement is not trained to shoot to apprehend they are trained to shoot to kill and i thank wilson for killing that punk bitch\n",
      "Normalizado: law enforcement train shoot apprehend train shoot kill thank wilson kill punk bitch\n",
      "\n",
      "[Ejemplo 3]\n",
      "Limpio     : dont you reckon them black lives matter banners being held by white cunts is kinda patronizing and ironically racist could they have not come up with somethin better or is it just what white folks do to give them selves pride o look at me im being nice for the black people why does it always have to be about race actually the whole world is pussyfootin around for fear of being racist its fuckin daft man\n",
      "Normalizado: not reckon black life matter banner hold white cunt kinda patronizing ironically racist come somethin well white folk self pride o look m nice black people race actually world pussyfootin fear racist fuckin daft man\n",
      "\n",
      "[Ejemplo 4]\n",
      "Limpio     : there are a very large number of people who do not like police officers they are called criminals and its the reason we have police officers the fact that criminals do not like police officers is a testament to the good work that police officers do in protecting the public when our children or our family are in danger we do not hessitate to call for help and we call the police its about time people stopped complaining and started to give the police some respect for the hard work and dedication that often requires them to put their lives on the line to serve the public\n",
      "Normalizado: large number people like police officer call criminal reason police officer fact criminal like police officer testament good work police officer protect public child family danger hessitate help police time people stop complain start police respect hard work dedication require life line serve public\n",
      "\n",
      "[Ejemplo 5]\n",
      "Limpio     : the arab dude is absolutely right he should have not been shot extra time shoot him once if hes attacking you and that would stop his attack shoot him twice if he s still attacking you but six times that is shoot to kill in my opinion\n",
      "Normalizado: arab dude absolutely right shoot extra time shoot s attack stop attack shoot twice s attack time shoot kill opinion\n",
      "\n",
      "[Ejemplo 1]\n",
      "Limpio     : if only people would just take a step back and not make this case about them because it wasn t about anyone except the two people in that situation to lump yourself into this mess and take matters into your own hands makes these kinds of protests selfish and without rational thought and investigation the guy in this video is heavily emotional and hyped up and wants to be heard and when he gets heard he just presses more and more he was never out to have a reasonable discussion kudos to the smerconish for keeping level the whole time and letting masri make himself out to be a fool how dare he and those that tore that city down in protest make this about themselves and to dishonor the entire incident with their own hate by the way since when did police brutality become an epidemic i wish everyone would just stop pretending like they were there and they knew exactly what was going on because there s no measurable amount of people that honestly witnessed this incident so none of us have a clue on which way this whole issue should have swung the grand jury were the most informed we have to trust the majority rule was the right course of action and let it be also thank you to the of police officers in america that actually serve protect even if you re a bit of a jerk when you pull me over i respect your job and know that someone has to do it and that many people are going to pout about being held accountable to their actions people hate police until they need an officer or two around in an emergency\n",
      "Normalizado: people step case wasn t people situation lump mess matter hand make kind protest selfish rational thought investigation guy video heavily emotional hype want hear get hear press reasonable discussion kudo smerconish keep level time let masri fool dare tear city protest dishonor entire incident hate way police brutality epidemic wish stop pretend like know exactly go s measurable people honestly witness incident clue way issue swing grand jury informed trust majority rule right course action let thank police officer america actually serve protect bit jerk pull respect job know people go pout hold accountable action people hate police need officer emergency\n",
      "\n",
      "[Ejemplo 2]\n",
      "Limpio     : law enforcement is not trained to shoot to apprehend they are trained to shoot to kill and i thank wilson for killing that punk bitch\n",
      "Normalizado: law enforcement train shoot apprehend train shoot kill thank wilson kill punk bitch\n",
      "\n",
      "[Ejemplo 3]\n",
      "Limpio     : dont you reckon them black lives matter banners being held by white cunts is kinda patronizing and ironically racist could they have not come up with somethin better or is it just what white folks do to give them selves pride o look at me im being nice for the black people why does it always have to be about race actually the whole world is pussyfootin around for fear of being racist its fuckin daft man\n",
      "Normalizado: not reckon black life matter banner hold white cunt kinda patronizing ironically racist come somethin well white folk self pride o look m nice black people race actually world pussyfootin fear racist fuckin daft man\n",
      "\n",
      "[Ejemplo 4]\n",
      "Limpio     : there are a very large number of people who do not like police officers they are called criminals and its the reason we have police officers the fact that criminals do not like police officers is a testament to the good work that police officers do in protecting the public when our children or our family are in danger we do not hessitate to call for help and we call the police its about time people stopped complaining and started to give the police some respect for the hard work and dedication that often requires them to put their lives on the line to serve the public\n",
      "Normalizado: large number people like police officer call criminal reason police officer fact criminal like police officer testament good work police officer protect public child family danger hessitate help police time people stop complain start police respect hard work dedication require life line serve public\n",
      "\n",
      "[Ejemplo 5]\n",
      "Limpio     : the arab dude is absolutely right he should have not been shot extra time shoot him once if hes attacking you and that would stop his attack shoot him twice if he s still attacking you but six times that is shoot to kill in my opinion\n",
      "Normalizado: arab dude absolutely right shoot extra time shoot s attack stop attack shoot twice s attack time shoot kill opinion\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelos spaCy\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "def normalize_text_spacy(text):\n",
    "    \"\"\"\n",
    "    Normalizaci√≥n biling√ºe con spaCy:\n",
    "    1. Detectar idioma\n",
    "    2. Tokenizar\n",
    "    3. Lematizar\n",
    "    4. Eliminar stopwords\n",
    "    5. Mantener solo tokens alfab√©ticos\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = \"en\"  # fallback si no detecta\n",
    "\n",
    "    doc = nlp_es(text) if lang == \"es\" else nlp_en(text)\n",
    "\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if token.is_alpha and not token.is_stop\n",
    "    ]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Aplicar normalizaci√≥n sobre tu columna ya limpia\n",
    "print(\"üîÑ Normalizando textos biling√ºes con spaCy...\")\n",
    "df[\"text_norm\"] = df[\"text_clean\"].apply(normalize_text_spacy)\n",
    "\n",
    "# Ejemplos\n",
    "for i in range(5):\n",
    "    print(f\"\\n[Ejemplo {i+1}]\")\n",
    "    print(\"Limpio     :\", df[\"text_clean\"].iloc[i])\n",
    "    print(\"Normalizado:\", df[\"text_norm\"].iloc[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa5003",
   "metadata": {},
   "source": [
    "5. Guardado del Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16d94ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset guardado despu√©s del Preprocesamiento en: ../../data/comentarios_preprocesados.pkl\n"
     ]
    }
   ],
   "source": [
    "# Guardar el dataset normalizado\n",
    "preprocessing_file = \"../../data/preprocessing.py\"\n",
    "output_file = \"../../data/comentarios_preprocesados.pkl\"\n",
    "\n",
    "df.to_pickle(output_file)\n",
    "\n",
    "print(f\"‚úÖ Dataset guardado despu√©s del Preprocesamiento en: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
