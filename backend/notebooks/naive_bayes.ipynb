{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f890d50f",
   "metadata": {},
   "source": [
    "# Clasificaci√≥n de comentarios t√≥xicos con Naive Bayes\n",
    "\n",
    "En este notebook entrenamos un modelo cl√°sico de **Naive Bayes (MultinomialNB)** para detectar\n",
    "comentarios t√≥xicos (`IsToxic`) en el dataset preprocesado:\n",
    "\n",
    "\n",
    "**Objetivo principal:**\n",
    "- Entrenar un modelo **Multinomial Naive Bayes** usando:\n",
    "  - Dataset limpio: `data/preprocessing_data/youtoxic_english_1000_clean.csv`\n",
    "  - Texto procesado (`text_classic`)\n",
    "  - *Features* num√©ricas ya calculadas en el preprocesado:\n",
    "    - `text_len_classic`\n",
    "    - `word_count_classic`\n",
    "    - `uppercase_ratio`\n",
    "    - `exclamation_count`\n",
    "    - `hate_words_count`\n",
    "- Predecir la columna objetivo: **`IsToxic`**\n",
    "\n",
    "Al final del notebook:\n",
    "\n",
    "- Entrenaremos el modelo con un **train/test split (80/20)**.\n",
    "- Calcularemos m√©tricas: accuracy, precision, recall, F1, ROC-AUC y matriz de confusi√≥n.\n",
    "- Guardaremos:\n",
    "  - El modelo entrenado (`.pkl`) en `backend/models/`\n",
    "  - Un fichero de resultados (`.json`) en `data/results/` siguiendo el formato acordado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0253f8e",
   "metadata": {},
   "source": [
    "### 1 Importaci√≥n de librer√≠as y configuraci√≥n\n",
    "\n",
    "En esta celda:\n",
    "- Importamos las librer√≠as necesarias para:\n",
    "  - Carga de datos (`pandas`, `pathlib`)\n",
    "  - Modelado cl√°sico (`scikit-learn`)\n",
    "  - C√°lculo de m√©tricas\n",
    "  - Guardado del modelo (`joblib`)\n",
    "  - Guardado de resultados en JSON\n",
    "- Definimos el nombre del modelo, la columna objetivo y las columnas de texto y num√©ricas que vamos a usar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfc388c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "# === 1. Imports libraries ======================================\n",
    "\n",
    "import json  # To save metrics in JSON format\n",
    "from datetime import datetime  # To generate ISO timestamp\n",
    "from pathlib import Path  # To handle file system paths\n",
    "\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # DataFrame handling\n",
    "\n",
    "# Machine Leearning: Scikit-learn: data split, preprocessing and modeling\n",
    "from sklearn.model_selection import train_test_split  # Train/test split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Text vectorization\n",
    "from sklearn.compose import ColumnTransformer  # Combine text + numeric features\n",
    "from sklearn.preprocessing import FunctionTransformer  # For numeric features\n",
    "from sklearn.pipeline import Pipeline  # Build end-to-end ML pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB  # Naive Bayes classifier (text)\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    precision_recall_fscore_support,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Persistence\n",
    "import joblib\n",
    "\n",
    "import warnings  # To ignore some sklearn warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a64463",
   "metadata": {},
   "source": [
    "## 2. Carga del dataset preprocesado\n",
    "\n",
    "En esta secci√≥n:\n",
    "\n",
    "- Localizamos la ra√≠z del proyecto.\n",
    "- Cargamos el fichero limpio: `data/preprocessing_data/youtoxic_english_1000_clean.csv`.\n",
    "- Verificamos dimensiones y algunas columnas clave.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99256db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Ra√≠z del proyecto: c:\\dev\\proyectos\\PX_NLP_G4\n",
      "üìÑ Cargando dataset desde: c:\\dev\\proyectos\\PX_NLP_G4\\data\\preprocessing_data\\youtoxic_english_1000_clean.csv\n",
      "\n",
      "üìä Dimensiones del dataset limpio:\n",
      "   Filas:    997\n",
      "   Columnas: 18\n",
      "\n",
      "üîç Primeras filas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentId</th>\n",
       "      <th>VideoId</th>\n",
       "      <th>Text</th>\n",
       "      <th>IsToxic</th>\n",
       "      <th>IsAbusive</th>\n",
       "      <th>IsThreat</th>\n",
       "      <th>IsProvocative</th>\n",
       "      <th>IsObscene</th>\n",
       "      <th>IsHatespeech</th>\n",
       "      <th>IsRacist</th>\n",
       "      <th>IsReligiousHate</th>\n",
       "      <th>text_basic</th>\n",
       "      <th>text_classic</th>\n",
       "      <th>text_len_classic</th>\n",
       "      <th>word_count_classic</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>exclamation_count</th>\n",
       "      <th>hate_words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugg2KwwX0V8-aXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>If only people would just take a step back and...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>If only people would just take a step back and...</td>\n",
       "      <td>people would take step back make case wasnt an...</td>\n",
       "      <td>850</td>\n",
       "      <td>129</td>\n",
       "      <td>0.014121</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ugg2s5AzSPioEXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>Law enforcement is not trained to shoot to app...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Law enforcement is not trained to shoot to app...</td>\n",
       "      <td>law enforcement trained shoot apprehend traine...</td>\n",
       "      <td>90</td>\n",
       "      <td>13</td>\n",
       "      <td>0.036232</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugg3dWTOxryFfHgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>\\r\\nDont you reckon them 'black lives matter' ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Dont you reckon them 'black lives matter' bann...</td>\n",
       "      <td>dont reckon black life matter banner held whit...</td>\n",
       "      <td>252</td>\n",
       "      <td>40</td>\n",
       "      <td>0.002375</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CommentId      VideoId  \\\n",
       "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
       "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
       "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
       "\n",
       "                                                Text  IsToxic  IsAbusive  \\\n",
       "0  If only people would just take a step back and...    False      False   \n",
       "1  Law enforcement is not trained to shoot to app...     True       True   \n",
       "2  \\r\\nDont you reckon them 'black lives matter' ...     True       True   \n",
       "\n",
       "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  \\\n",
       "0     False          False      False         False     False   \n",
       "1     False          False      False         False     False   \n",
       "2     False          False       True         False     False   \n",
       "\n",
       "   IsReligiousHate                                         text_basic  \\\n",
       "0            False  If only people would just take a step back and...   \n",
       "1            False  Law enforcement is not trained to shoot to app...   \n",
       "2            False  Dont you reckon them 'black lives matter' bann...   \n",
       "\n",
       "                                        text_classic  text_len_classic  \\\n",
       "0  people would take step back make case wasnt an...               850   \n",
       "1  law enforcement trained shoot apprehend traine...                90   \n",
       "2  dont reckon black life matter banner held whit...               252   \n",
       "\n",
       "   word_count_classic  uppercase_ratio  exclamation_count  hate_words_count  \n",
       "0                 129         0.014121                  0                 2  \n",
       "1                  13         0.036232                  0                 3  \n",
       "2                  40         0.002375                  0                 1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. CARGA DEL DATASET PREPROCESADO\n",
    "# =============================================================================\n",
    "\n",
    "# Detect project root (assuming this notebook is in backend/notebooks)\n",
    "\n",
    "##################\n",
    "# Detect project root\n",
    "notebook_dir = Path.cwd()\n",
    "\n",
    "if notebook_dir.name == \"notebooks\":\n",
    "    project_root = notebook_dir.parent.parent\n",
    "elif notebook_dir.name == \"backend\":\n",
    "    project_root = notebook_dir.parent\n",
    "else:\n",
    "    project_root = notebook_dir\n",
    "\n",
    "data_path = project_root / \"data\" / \"preprocessing_data\" / \"youtoxic_english_1000_clean.csv\"\n",
    "\n",
    "print(f\"üìÇ Ra√≠z del proyecto: {project_root}\")\n",
    "print(f\"üìÑ Cargando dataset desde: {data_path}\")\n",
    "\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found at {data_path}.\\n\"\n",
    "        \"Comprueba que la carpeta 'data' est√° en la ra√≠z del proyecto y que\\n\"\n",
    "        \"el fichero 'youtoxic_english_1000.csv' est√° dentro de ella.\"\n",
    "    )\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(\"\\nüìä Dimensiones del dataset limpio:\")\n",
    "print(f\"   Filas:    {df.shape[0]}\")\n",
    "print(f\"   Columnas: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\nüîç Primeras filas:\")\n",
    "display(df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d6bec",
   "metadata": {},
   "source": [
    "## 3. Definici√≥n de columnas de entrada y variable objetivo\n",
    "\n",
    "En este notebook vamos a:\n",
    "\n",
    "- Usar el texto preprocesado `text_classic` como **feature textual principal**.\n",
    "- A√±adir las siguientes **features num√©ricas**:\n",
    "  - `text_len_classic` ‚Üí longitud de texto\n",
    "  - `word_count_classic` ‚Üí n√∫mero de palabras\n",
    "  - `uppercase_ratio` ‚Üí porcentaje de letras en may√∫scula\n",
    "  - `exclamation_count` ‚Üí n√∫mero de signos de exclamaci√≥n\n",
    "  - `hate_words_count` ‚Üí n√∫mero de palabras de odio encontradas\n",
    "- Predecir la variable binaria **`IsToxic`** como objetivo.\n",
    "\n",
    "Si otro compa√±ero quiere entrenar el modelo para otra etiqueta (`IsHatespeech`, `IsAbusive`, etc.),\n",
    "solo tendr√≠a que cambiar el nombre de `TARGET_COL`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fd55a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Columna objetivo: IsToxic\n",
      "üìù Columna de texto: text_classic\n",
      "üî¢ Features num√©ricas: ['text_len_classic', 'word_count_classic', 'uppercase_ratio', 'exclamation_count', 'hate_words_count']\n",
      "\n",
      "‚úÖ Todas las columnas necesarias est√°n presentes en el dataset\n",
      "\n",
      "üìä Distribuci√≥n de la variable objetivo (IsToxic):\n",
      "            ratio\n",
      "IsToxic          \n",
      "False    0.539619\n",
      "True     0.460381\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. DEFINICI√ìN DE FEATURES Y TARGET\n",
    "# =============================================================================\n",
    "\n",
    "# Target column for this notebook\n",
    "TARGET_COL = \"IsToxic\"  # Change this if you want to model another label\n",
    "TEXT_COL = \"text_classic\"\n",
    "\n",
    "# Numeric features already prepared in the preprocessing step\n",
    "numeric_features = [\n",
    "    \"text_len_classic\",\n",
    "    \"word_count_classic\",\n",
    "    \"uppercase_ratio\",\n",
    "    \"exclamation_count\",\n",
    "    \"hate_words_count\",\n",
    "]\n",
    "\n",
    "print(\"üéØ Columna objetivo:\", TARGET_COL)\n",
    "print(\"üìù Columna de texto:\", TEXT_COL)\n",
    "print(\"üî¢ Features num√©ricas:\", numeric_features)\n",
    "\n",
    "# Check that all needed columns exist\n",
    "required_cols = [TEXT_COL, TARGET_COL] + numeric_features\n",
    "missing_cols = [c for c in required_cols if c not in df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"‚ùå Faltan columnas en el dataset: {missing_cols}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Todas las columnas necesarias est√°n presentes en el dataset\")\n",
    "\n",
    "# Quick check of target distribution\n",
    "print(\"\\nüìä Distribuci√≥n de la variable objetivo (IsToxic):\")\n",
    "print(df[TARGET_COL].value_counts(normalize=True).rename(\"ratio\").to_frame())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe5321",
   "metadata": {},
   "source": [
    "## 4. Partici√≥n Train/Test\n",
    "\n",
    "Hacemos un **train/test split 80/20**, estratificando por la variable objetivo para mantener\n",
    "la misma proporci√≥n de clases en ambos subconjuntos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd6e5dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Tama√±os de los conjuntos:\n",
      "   X_train: (797, 6)\n",
      "   X_test:  (200, 6)\n",
      "   y_train: (797,)\n",
      "   y_test:  (200,)\n",
      "\n",
      "üìä Proporci√≥n de clase positiva (IsToxic = 1):\n",
      "   Train: 0.460\n",
      "   Test:  0.460\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. TRAIN/TEST SPLIT\n",
    "# =============================================================================\n",
    "\n",
    "# Features (X) and target (y)\n",
    "X = df[[TEXT_COL] + numeric_features]\n",
    "y = df[TARGET_COL].astype(int)  # Make sure it's 0/1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,  # Keep class distribution\n",
    ")\n",
    "\n",
    "print(\"üìö Tama√±os de los conjuntos:\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   X_test:  {X_test.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "print(f\"   y_test:  {y_test.shape}\")\n",
    "\n",
    "print(\"\\nüìä Proporci√≥n de clase positiva (IsToxic = 1):\")\n",
    "print(f\"   Train: {y_train.mean():.3f}\")\n",
    "print(f\"   Test:  {y_test.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cbdad1",
   "metadata": {},
   "source": [
    "## 5. Pipeline: TF-IDF + features num√©ricas + Naive Bayes\n",
    "\n",
    "Construimos un **Pipeline de scikit-learn** que:\n",
    "\n",
    "1. Aplica `TfidfVectorizer` sobre la columna `text_classic` (unigrams + bigrams).\n",
    "2. A√±ade las columnas num√©ricas en bruto (ya normalizadas o acotadas en el preprocesado).\n",
    "3. Entrena un modelo `MultinomialNB` con todos esos features combinados.\n",
    "\n",
    "De esta forma:\n",
    "\n",
    "- Tenemos un √∫nico objeto (`Pipeline`) que incluye preprocesado + modelo.\n",
    "- Es m√°s f√°cil guardar y reutilizar el modelo despu√©s (`.pkl`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b78fb859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline definido correctamente\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. DEFINICI√ìN DEL PIPELINE (TF-IDF + NUM FEATURES + NAIVE BAYES)\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer  # Optional, if needed\n",
    "\n",
    "# Define TF-IDF vectorizer for text\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),       # unigrams + bigrams\n",
    "    max_features=10000,       # limit vocabulary size\n",
    "    min_df=2,                 # ignore very rare terms\n",
    "    strip_accents=\"unicode\",  # normalize accents\n",
    ")\n",
    "\n",
    "# ColumnTransformer to combine text and numeric features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Apply TF-IDF on text column\n",
    "        (\"text\", tfidf_vectorizer, TEXT_COL),\n",
    "        # Pass numeric features as they are (they are already non-negative and reasonable)\n",
    "        (\"num\", \"passthrough\", numeric_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build Pipeline: preprocessor + Naive Bayes classifier\n",
    "nb_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Pipeline definido correctamente\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f30d5",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento y evaluaci√≥n del modelo\n",
    "\n",
    "Entrenamos el pipeline completo y calculamos las siguientes m√©tricas sobre el conjunto de test:\n",
    "\n",
    "- **Accuracy**\n",
    "- **Precision**\n",
    "- **Recall**\n",
    "- **F1-Score**\n",
    "- **ROC-AUC**\n",
    "- **Matriz de confusi√≥n** (TN, FP, FN, TP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99d0ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Entrenando modelo Naive Bayes...\n",
      "‚úÖ Entrenamiento completado\n",
      "\n",
      "üìä M√âTRICAS EN TEST (Naive Bayes - IsToxic):\n",
      "   Accuracy : 0.760\n",
      "   Precision: 0.833\n",
      "   Recall   : 0.598\n",
      "   F1-Score : 0.696\n",
      "   ROC-AUC  : 0.801\n",
      "\n",
      "üìå Matriz de confusi√≥n:\n",
      "   TN: 97   FP: 11\n",
      "   FN: 37   TP: 55\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. ENTRENAMIENTO Y EVALUACI√ìN\n",
    "# =============================================================================\n",
    "\n",
    "# Fit the pipeline on training data\n",
    "print(\"‚è≥ Entrenando modelo Naive Bayes...\")\n",
    "nb_pipeline.fit(X_train, y_train)\n",
    "print(\"‚úÖ Entrenamiento completado\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = nb_pipeline.predict(X_test)\n",
    "y_proba = nb_pipeline.predict_proba(X_test)[:, 1]  # Probabilities for positive class\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average=\"binary\", zero_division=0\n",
    ")\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "print(\"\\nüìä M√âTRICAS EN TEST (Naive Bayes - IsToxic):\")\n",
    "print(f\"   Accuracy : {accuracy:.3f}\")\n",
    "print(f\"   Precision: {precision:.3f}\")\n",
    "print(f\"   Recall   : {recall:.3f}\")\n",
    "print(f\"   F1-Score : {f1:.3f}\")\n",
    "print(f\"   ROC-AUC  : {roc_auc:.3f}\")\n",
    "\n",
    "print(\"\\nüìå Matriz de confusi√≥n:\")\n",
    "print(f\"   TN: {tn}   FP: {fp}\")\n",
    "print(f\"   FN: {fn}   TP: {tp}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c8827",
   "metadata": {},
   "source": [
    "## 7. Generaci√≥n del JSON de resultados y guardado del modelo\n",
    "\n",
    "Para poder comparar modelos de forma homog√©nea:\n",
    "\n",
    "- Construimos un diccionario con la misma estructura de JSON para **todos los modelos**.\n",
    "- Guardamos ese JSON en `data/results/<model_name>.json`.\n",
    "- Guardamos el modelo (`Pipeline` completo) en `backend/models/<model_name>.pkl`.\n",
    "\n",
    "De esta forma, el notebook de comparaci√≥n solo tendr√° que leer los `.json` de `data/results`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b3881b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Archivos guardados:\n",
      "   üìÅ JSON resultados: c:\\dev\\proyectos\\PX_NLP_G4\\data\\results\\naive_bayes_toxic_v1.json\n",
      "   üìÅ Modelo (.pkl)   : c:\\dev\\proyectos\\PX_NLP_G4\\backend\\models\\naive_bayes_toxic_v1.pkl\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 7. CREACI√ìN DE JSON DE RESULTADOS Y GUARDADO\n",
    "# =============================================================================\n",
    "\n",
    "from joblib import dump  # To save the trained model\n",
    "\n",
    "model_name = \"naive_bayes_toxic_v1\"\n",
    "\n",
    "# Get TF-IDF feature count after fitting\n",
    "fitted_tfidf = nb_pipeline.named_steps[\"preprocessor\"].named_transformers_[\"text\"]\n",
    "n_features_text = len(fitted_tfidf.get_feature_names_out())\n",
    "n_features_numeric = len(numeric_features)\n",
    "n_samples = df.shape[0]\n",
    "\n",
    "results_dict = {\n",
    "    \"model_name\": model_name,\n",
    "    \"task\": \"binary_classification\",\n",
    "    \"target_label\": TARGET_COL,\n",
    "    \"data\": {\n",
    "        \"n_samples\": int(n_samples),\n",
    "        \"n_features_text\": int(n_features_text),\n",
    "        \"n_features_numeric\": int(n_features_numeric),\n",
    "        \"train_size\": float(len(X_train) / len(df)),\n",
    "        \"test_size\": float(len(X_test) / len(df)),\n",
    "        \"random_state\": 42,\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"roc_auc\": float(roc_auc),\n",
    "    },\n",
    "    \"confusion_matrix\": {\n",
    "        \"tn\": int(tn),\n",
    "        \"fp\": int(fp),\n",
    "        \"fn\": int(fn),\n",
    "        \"tp\": int(tp),\n",
    "    },\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"notes\": \"Naive Bayes + TF-IDF (1,2) + 5 numeric features on text_classic\",\n",
    "}\n",
    "\n",
    "# Paths for saving\n",
    "results_dir = project_root / \"data\" / \"results\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "models_dir = project_root / \"backend\" / \"models\"\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "json_path = results_dir / f\"{model_name}.json\"\n",
    "model_path = models_dir / f\"{model_name}.pkl\"\n",
    "\n",
    "# Save JSON\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save model\n",
    "dump(nb_pipeline, model_path)\n",
    "\n",
    "print(\"\\nüíæ Archivos guardados:\")\n",
    "print(f\"   üìÅ JSON resultados: {json_path}\")\n",
    "print(f\"   üìÅ Modelo (.pkl)   : {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
