{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3903391f",
   "metadata": {},
   "source": [
    "# Red neuronal recurrente para toxicidad (GRU vs BiGRU)\n",
    "\n",
    "En este notebook entrenamos dos modelos secuenciales:\n",
    "\n",
    "1. **GRU**\n",
    "2. **Bidirectional GRU (BiGRU)**\n",
    "\n",
    "Partimos del dataset limpio:\n",
    "\n",
    "- Texto: `text_classic`\n",
    "- Etiqueta objetivo: `IsToxic`\n",
    "\n",
    "Incluimos:\n",
    "\n",
    "- TokenizaciÃ³n con Keras\n",
    "- Padding\n",
    "- Entrenamiento con validaciÃ³n\n",
    "- MÃ©tricas en train/test\n",
    "- Control de overfitting (< 5%)\n",
    "- SelecciÃ³n del mejor modelo\n",
    "- Guardado de:\n",
    "  - Modelo neuronal en `.keras`\n",
    "  - MÃ©tricas en `.json` (formato estÃ¡ndar del equipo)\n",
    "\n",
    "Este JSON se usa directamente en `comparison_models.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82ff38",
   "metadata": {},
   "source": [
    "### 1. ImportaciÃ³n de librerÃ­as y configuraciÃ³n\n",
    "\n",
    "En esta celda:\n",
    "\n",
    "- Importamos:\n",
    "  - `pandas`, `numpy`\n",
    "  - `tensorflow/keras` para la red neuronal\n",
    "  - mÃ©tricas de `sklearn` para mantener el mismo estÃ¡ndar del equipo\n",
    "- Definimos:\n",
    "  - semilla\n",
    "  - nombre base del modelo\n",
    "  - parÃ¡metros de tokenizaciÃ³n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1a220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Imports and config ===================================================\n",
    "\n",
    "import json  # To save metrics in JSON format\n",
    "from datetime import datetime  # To generate ISO timestamps\n",
    "from pathlib import Path  # To handle project paths\n",
    "\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # DataFrame handling\n",
    "\n",
    "import tensorflow as tf  # Deep learning framework\n",
    "from tensorflow.keras import layers, models  # Keras high-level API\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # Data split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas correctamente\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TARGET_COL = \"IsToxic\"\n",
    "\n",
    "T# Text column selection for RNN\n",
    "# We default to text_classic for consistency with the cleaned dataset.\n",
    "# You can switch to text_basic to test a less normalized variant.\n",
    "TEXT_COL = \"text_classic\"  # or \"text_basic\"\n",
    "\n",
    "model_name_base = \"bigru_IsToxic\"\n",
    "\n",
    "# Tokenization parameters\n",
    "MAX_WORDS = 20000\n",
    "MAX_LEN = 200\n",
    "\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a667210",
   "metadata": {},
   "source": [
    "### 2. Funciones auxiliares comunes\n",
    "\n",
    "Incluimos:\n",
    "\n",
    "- detecciÃ³n de raÃ­z del proyecto\n",
    "- creaciÃ³n de carpetas\n",
    "- cÃ¡lculo de mÃ©tricas estÃ¡ndar\n",
    "- control de overfitting\n",
    "- guardado del JSON\n",
    "\n",
    "Nota:\n",
    "- aquÃ­ **no guardamos `.pkl`** porque en redes neuronales\n",
    "  el artefacto correcto es `.keras`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150d104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Helper functions (NN version) =======================================\n",
    "\n",
    "def get_project_root() -> Path:\n",
    "    \"\"\"Return PX_NLP_G4 project root folder, robust from notebooks.\"\"\"\n",
    "    cwd = Path.cwd()\n",
    "    parts = cwd.parts\n",
    "    if \"notebooks\" in parts:\n",
    "        return cwd.parents[1]\n",
    "    if cwd.name == \"backend\":\n",
    "        return cwd.parent\n",
    "    return cwd\n",
    "\n",
    "\n",
    "def build_project_dirs(project_root: Path) -> dict:\n",
    "    \"\"\"Create and return standard directories used across notebooks.\"\"\"\n",
    "    data_dir = project_root / \"data\"\n",
    "    preproc_dir = data_dir / \"preprocessing_data\"\n",
    "    results_dir = data_dir / \"results\"\n",
    "    models_dir = project_root / \"backend\" / \"models\"\n",
    "\n",
    "    preproc_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    return {\n",
    "        \"data_dir\": data_dir,\n",
    "        \"preproc_dir\": preproc_dir,\n",
    "        \"results_dir\": results_dir,\n",
    "        \"models_dir\": models_dir\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_binary_metrics(y_true, y_pred, y_proba=None) -> dict:\n",
    "    \"\"\"Compute standard binary metrics for JSON consistency.\"\"\"\n",
    "    out = {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "    if y_proba is not None:\n",
    "        try:\n",
    "            out[\"roc_auc\"] = float(roc_auc_score(y_true, y_proba))\n",
    "        except Exception:\n",
    "            out[\"roc_auc\"] = None\n",
    "    else:\n",
    "        out[\"roc_auc\"] = None\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_confusion_dict(y_true, y_pred) -> dict:\n",
    "    \"\"\"Return confusion matrix values tn, fp, fn, tp.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        tn = fp = fn = tp = 0\n",
    "    return {\"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp)}\n",
    "\n",
    "\n",
    "def overfitting_check(train_metrics: dict, test_metrics: dict, threshold: float = 0.05) -> dict:\n",
    "    \"\"\"Flag overfitting risk if train - test > threshold.\"\"\"\n",
    "    report = {\"threshold\": threshold, \"flags\": {}}\n",
    "    for k in [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]:\n",
    "        tr = train_metrics.get(k)\n",
    "        te = test_metrics.get(k)\n",
    "        if tr is None or te is None:\n",
    "            report[\"flags\"][k] = None\n",
    "            continue\n",
    "        diff = float(tr - te)\n",
    "        report[\"flags\"][k] = {\n",
    "            \"train\": float(tr),\n",
    "            \"test\": float(te),\n",
    "            \"diff\": float(diff),\n",
    "            \"overfit_risk\": bool(diff > threshold)\n",
    "        }\n",
    "    return report\n",
    "\n",
    "\n",
    "def build_results_json_nn(\n",
    "    model_name: str,\n",
    "    target_label: str,\n",
    "    n_samples: int,\n",
    "    vocab_size: int,\n",
    "    train_size: float,\n",
    "    test_size: float,\n",
    "    random_state: int,\n",
    "    test_metrics: dict,\n",
    "    cm_dict: dict,\n",
    "    notes: str = \"\"\n",
    ") -> dict:\n",
    "    \"\"\"Standard team JSON for NN models.\"\"\"\n",
    "    return {\n",
    "        \"model_name\": model_name,\n",
    "        \"task\": \"binary_classification\",\n",
    "        \"target_label\": target_label,\n",
    "        \"data\": {\n",
    "            \"n_samples\": int(n_samples),\n",
    "            \"n_features_text\": int(vocab_size),\n",
    "            \"n_features_numeric\": 0,\n",
    "            \"train_size\": float(train_size),\n",
    "            \"test_size\": float(test_size),\n",
    "            \"random_state\": int(random_state),\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": test_metrics.get(\"accuracy\"),\n",
    "            \"precision\": test_metrics.get(\"precision\"),\n",
    "            \"recall\": test_metrics.get(\"recall\"),\n",
    "            \"f1\": test_metrics.get(\"f1\"),\n",
    "            \"roc_auc\": test_metrics.get(\"roc_auc\"),\n",
    "        },\n",
    "        \"confusion_matrix\": cm_dict,\n",
    "        \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"notes\": notes\n",
    "    }\n",
    "\n",
    "\n",
    "def save_results_json(results: dict, results_dir: Path, model_name: str) -> Path:\n",
    "    \"\"\"Save JSON results using model_name as filename.\"\"\"\n",
    "    json_path = results_dir / f\"{model_name}.json\"\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    return json_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b58fd8",
   "metadata": {},
   "source": [
    "### 3. Carga del dataset limpio y split\n",
    "\n",
    "En esta celda:\n",
    "\n",
    "- Cargamos el CSV limpio.\n",
    "- Verificamos columnas.\n",
    "- Dividimos en train/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86fbb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Load dataset and split ==============================================\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "project_root = get_project_root()\n",
    "dirs = build_project_dirs(project_root)\n",
    "\n",
    "preproc_dir = dirs[\"preproc_dir\"]\n",
    "results_dir = dirs[\"results_dir\"]\n",
    "models_dir = dirs[\"models_dir\"]\n",
    "\n",
    "csv_path = preproc_dir / \"youtoxic_english_1000_clean.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "if TEXT_COL not in df.columns:\n",
    "    raise ValueError(f\"La columna seleccionada '{TEXT_COL}' no existe en el dataset limpio.\")\n",
    "\n",
    "missing_cols = [c for c in [TEXT_COL, TARGET_COL] if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Faltan columnas en el dataset limpio: {missing_cols}\")\n",
    "\n",
    "X = df[TEXT_COL].astype(str).values\n",
    "y = df[TARGET_COL].astype(int).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "**âœ… Dataset cargado y split completado**\n",
    "\n",
    "- Train: **{len(X_train)}**\n",
    "- Test: **{len(X_test)}**\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd43efd",
   "metadata": {},
   "source": [
    "### 4. TokenizaciÃ³n y padding\n",
    "\n",
    "En esta celda:\n",
    "\n",
    "- Creamos un `Tokenizer`.\n",
    "- Convertimos textos a secuencias.\n",
    "- Aplicamos padding.\n",
    "\n",
    "Esto convierte texto a formato usable por GRU/BiGRU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Tokenize and pad sequences ==========================================\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=MAX_WORDS,\n",
    "    oov_token=\"<OOV>\"\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_train_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "X_test_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_test_seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\"\n",
    ")\n",
    "\n",
    "vocab_size = min(MAX_WORDS, len(tokenizer.word_index) + 1)\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "**âœ… TokenizaciÃ³n completada**\n",
    "\n",
    "- TamaÃ±o vocabulario usado: **{vocab_size}**\n",
    "- Longitud mÃ¡xima secuencia: **{MAX_LEN}**\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7857d",
   "metadata": {},
   "source": [
    "### 5. DefiniciÃ³n de modelos GRU y BiGRU\n",
    "\n",
    "En esta celda:\n",
    "\n",
    "- Definimos dos arquitecturas simples y comparables.\n",
    "- Usamos:\n",
    "  - Embedding\n",
    "  - GRU (normal)\n",
    "  - GRU bidireccional\n",
    "\n",
    "Buscamos equilibrio entre rendimiento y evitar overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aed6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Build GRU and BiGRU models ==========================================\n",
    "\n",
    "def build_gru_model(vocab_size: int) -> tf.keras.Model:\n",
    "    \"\"\"Build a simple GRU binary classifier.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, 128, input_length=MAX_LEN),\n",
    "        layers.GRU(64),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_bigru_model(vocab_size: int) -> tf.keras.Model:\n",
    "    \"\"\"Build a Bidirectional GRU binary classifier.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Embedding(vocab_size, 128, input_length=MAX_LEN),\n",
    "        layers.Bidirectional(layers.GRU(64)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "gru_model = build_gru_model(vocab_size)\n",
    "bigru_model = build_bigru_model(vocab_size)\n",
    "\n",
    "display(Markdown(\"**ðŸ§  Modelos GRU y BiGRU definidos y compilados**\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9e70a",
   "metadata": {},
   "source": [
    "### 6. Entrenamiento de GRU\n",
    "\n",
    "Entrenamos con validaciÃ³n interna.\n",
    "\n",
    "Usamos `EarlyStopping` para:\n",
    "\n",
    "- reducir overfitting\n",
    "- cumplir el requisito de la rÃºbrica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Train GRU ============================================================\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "display(Markdown(\"### ðŸ‹ï¸ Entrenamiento GRU\"))\n",
    "\n",
    "history_gru = gru_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=8,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"âœ… Entrenamiento GRU completado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9568f96",
   "metadata": {},
   "source": [
    "### 7. Entrenamiento de BiGRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad610176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Train BiGRU ==========================================================\n",
    "\n",
    "display(Markdown(\"### ðŸ‹ï¸ Entrenamiento BiGRU\"))\n",
    "\n",
    "history_bigru = bigru_model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=8,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"âœ… Entrenamiento BiGRU completado\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04becb1b",
   "metadata": {},
   "source": [
    "### 7.1 Curvas de entrenamiento\n",
    "\n",
    "En esta celda:\n",
    "\n",
    "- Visualizamos la evoluciÃ³n de:\n",
    "  - `loss`\n",
    "  - `accuracy`\n",
    "\n",
    "para GRU y BiGRU.\n",
    "\n",
    "Esto nos ayuda a detectar seÃ±ales tempranas de overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c1e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7.1 Training curves =====================================================\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "display(Markdown(\"### ðŸ“‰ Curvas de entrenamiento â€” GRU vs BiGRU\"))\n",
    "\n",
    "def plot_history(history, title):\n",
    "    \"\"\"Plot training/validation loss and accuracy for a Keras model.\"\"\"\n",
    "    hist = history.history\n",
    "\n",
    "    # Loss\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 4))\n",
    "    ax.plot(hist.get(\"loss\", []), label=\"train_loss\")\n",
    "    ax.plot(hist.get(\"val_loss\", []), label=\"val_loss\")\n",
    "    ax.set_title(f\"Loss â€” {title}\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    fig, ax = plt.subplots(figsize=(6.5, 4))\n",
    "    ax.plot(hist.get(\"accuracy\", []), label=\"train_acc\")\n",
    "    ax.plot(hist.get(\"val_accuracy\", []), label=\"val_acc\")\n",
    "    ax.set_title(f\"Accuracy â€” {title}\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_history(history_gru, \"GRU\")\n",
    "plot_history(history_bigru, \"BiGRU\")\n",
    "\n",
    "display(Markdown(\"\"\"\n",
    "**ðŸ”Ž InterpretaciÃ³n**\n",
    "\n",
    "- Si `val_loss` sube mientras `train_loss` baja,\n",
    "  es una seÃ±al tÃ­pica de overfitting.\n",
    "- `EarlyStopping` suele mitigar este efecto en datasets pequeÃ±os.\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889028ae",
   "metadata": {},
   "source": [
    "### 8. EvaluaciÃ³n en train/test + control de overfitting\n",
    "\n",
    "En esta celda:\n",
    "\n",
    "- Generamos predicciones y probabilidades\n",
    "- Calculamos mÃ©tricas estÃ¡ndar\n",
    "- Comparamos train vs test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a84a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. Evaluate GRU and BiGRU ==============================================\n",
    "\n",
    "def eval_keras_model(model, X_tr, y_tr, X_te, y_te):\n",
    "    \"\"\"Compute train/test metrics for a Keras binary classifier.\"\"\"\n",
    "    # Train predictions\n",
    "    tr_proba = model.predict(X_tr, verbose=0).ravel()\n",
    "    tr_pred = (tr_proba >= 0.5).astype(int)\n",
    "    train_metrics = compute_binary_metrics(y_tr, tr_pred, tr_proba)\n",
    "\n",
    "    # Test predictions\n",
    "    te_proba = model.predict(X_te, verbose=0).ravel()\n",
    "    te_pred = (te_proba >= 0.5).astype(int)\n",
    "    test_metrics = compute_binary_metrics(y_te, te_pred, te_proba)\n",
    "\n",
    "    cm_dict = compute_confusion_dict(y_te, te_pred)\n",
    "    overfit = overfitting_check(train_metrics, test_metrics, threshold=0.05)\n",
    "\n",
    "    return train_metrics, test_metrics, cm_dict, overfit\n",
    "\n",
    "\n",
    "display(Markdown(\"### ðŸ“Š EvaluaciÃ³n GRU\"))\n",
    "train_m_gru, test_m_gru, cm_gru, overfit_gru = eval_keras_model(\n",
    "    gru_model, X_train_pad, y_train, X_test_pad, y_test\n",
    ")\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "**GRU â€” Test**\n",
    "- Accuracy: `{test_m_gru['accuracy']:.3f}`\n",
    "- Precision: `{test_m_gru['precision']:.3f}`\n",
    "- Recall: `{test_m_gru['recall']:.3f}`\n",
    "- F1: `{test_m_gru['f1']:.3f}`\n",
    "- ROC-AUC: `{test_m_gru['roc_auc']:.3f}`\n",
    "\"\"\"))\n",
    "\n",
    "display(Markdown(\"### ðŸ“Š EvaluaciÃ³n BiGRU\"))\n",
    "train_m_bigru, test_m_bigru, cm_bigru, overfit_bigru = eval_keras_model(\n",
    "    bigru_model, X_train_pad, y_train, X_test_pad, y_test\n",
    ")\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "**BiGRU â€” Test**\n",
    "- Accuracy: `{test_m_bigru['accuracy']:.3f}`\n",
    "- Precision: `{test_m_bigru['precision']:.3f}`\n",
    "- Recall: `{test_m_bigru['recall']:.3f}`\n",
    "- F1: `{test_m_bigru['f1']:.3f}`\n",
    "- ROC-AUC: `{test_m_bigru['roc_auc']:.3f}`\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ff39bc",
   "metadata": {},
   "source": [
    "### 9. SelecciÃ³n del mejor modelo neuronal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 9. Select best NN model =================================================\n",
    "\n",
    "candidates_nn = [\n",
    "    (\"gru\", gru_model, train_m_gru, test_m_gru, cm_gru),\n",
    "    (\"bigru\", bigru_model, train_m_bigru, test_m_bigru, cm_bigru),\n",
    "]\n",
    "\n",
    "candidates_nn_sorted = sorted(\n",
    "    candidates_nn,\n",
    "    key=lambda x: (x[3].get(\"f1\", 0), x[3].get(\"roc_auc\", 0)),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "best_tag, best_nn_model, best_train_m, best_test_m, best_cm = candidates_nn_sorted[0]\n",
    "\n",
    "final_model_name = f\"{model_name_base}_{best_tag}\"\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "**ðŸ¥‡ Mejor modelo neuronal:** `{final_model_name}`\n",
    "\n",
    "- F1 (test): `{best_test_m['f1']:.3f}`\n",
    "- ROC-AUC (test): `{best_test_m['roc_auc']:.3f}`\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86cb70",
   "metadata": {},
   "source": [
    "### 10. Guardado del modelo neuronal y JSON\n",
    "\n",
    "En esta celda:\n",
    "\n",
    "- Guardamos el modelo en formato Keras:\n",
    "  - `backend/models/<model_name>.keras`\n",
    "- Guardamos el JSON estÃ¡ndar en:\n",
    "  - `data/results/<model_name>.json`\n",
    "\n",
    "Esto permite comparaciÃ³n directa con modelos clÃ¡sicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 10. Save best NN model and JSON ========================================\n",
    "\n",
    "n_samples = df.shape[0]\n",
    "\n",
    "results_dict = build_results_json_nn(\n",
    "    model_name=final_model_name,\n",
    "    target_label=TARGET_COL,\n",
    "    n_samples=n_samples,\n",
    "    vocab_size=vocab_size,\n",
    "    train_size=len(X_train) / n_samples,\n",
    "    test_size=len(X_test) / n_samples,\n",
    "    random_state=RANDOM_STATE,\n",
    "    test_metrics=best_test_m,\n",
    "    cm_dict=best_cm,\n",
    "    notes=\"GRU vs BiGRU; text_classic; tokenizer + embedding\"\n",
    ")\n",
    "\n",
    "json_path = save_results_json(results_dict, results_dir, final_model_name)\n",
    "\n",
    "# Save Keras model\n",
    "keras_path = models_dir / f\"{final_model_name}.keras\"\n",
    "best_nn_model.save(keras_path)\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "**ðŸ’¾ Guardado completado**\n",
    "\n",
    "- Modelo neuronal: `{keras_path}`\n",
    "- JSON: `{json_path}`\n",
    "\n",
    "Este JSON es compatible con `comparison_models.ipynb`.\n",
    "\"\"\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
