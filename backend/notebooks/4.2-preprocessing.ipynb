{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eea0556",
   "metadata": {},
   "source": [
    "# 02 ‚Äì Preprocesamiento de texto para detecci√≥n de toxicidad\n",
    "\n",
    "En este notebook vamos a construir el pipeline de **preprocesamiento de texto** sobre el dataset\n",
    "`youtoxic_english_1000.csv` para dejarlo listo de cara a modelos **cl√°sicos** (TF-IDF + clasificadores).\n",
    "\n",
    "Concretamente:\n",
    "\n",
    "1. Cargaremos el dataset original.\n",
    "2. Definiremos una funci√≥n de limpieza b√°sica:\n",
    "   - Conversi√≥n de emojis a texto en ingl√©s.\n",
    "   - Paso a min√∫sculas.\n",
    "   - Eliminaci√≥n de URLs, menciones, hashtags, saltos de l√≠nea y espacios extra.\n",
    "3. Implementaremos un pipeline cl√°sico de NLP en ingl√©s:\n",
    "   - Tokenizaci√≥n.\n",
    "   - Eliminaci√≥n de stopwords.\n",
    "   - Lematizaci√≥n.\n",
    "4. Crearemos:\n",
    "   - `text_basic`: texto con limpieza ligera (apto tanto para modelos cl√°sicos como modernos).\n",
    "   - `text_classic`: texto procesado (tokenizado, sin stopwords, lematizado) para modelos cl√°sicos.\n",
    "5. Guardaremos un CSV preprocesado que se usar√° en el notebook de modelado.\n",
    "\n",
    "> Nota: la vectorizaci√≥n (TF-IDF, Bag-of-Words, etc.) se har√° en el notebook de modelado,\n",
    "> a partir de la columna `text_classic`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80284f",
   "metadata": {},
   "source": [
    "## Paso 1: Importar librer√≠as y configurar el entorno\n",
    "\n",
    "Importamos las librer√≠as necesarias para el preprocesamiento y definimos las rutas del fichero de entrada y de salida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d1f9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Import libraries for preprocessing\n",
    "\n",
    "import pandas as pd                      # Data manipulation with DataFrames\n",
    "import numpy as np                       # Numerical utilities (if needed)\n",
    "import re                                # Regular expressions for text cleaning and tokenization\n",
    "import emoji                             # Emoji handling: convert emojis to text\n",
    "import nltk                              # Classic NLP library: stopwords, lemmatization, etc.\n",
    "from nltk.corpus import stopwords        # Stopword lists for multiple languages (we use English)\n",
    "from nltk.stem import WordNetLemmatizer  # Lemmatizer based on WordNet (English)\n",
    "from IPython.display import Markdown, display  # Display Markdown explanations from code cells\n",
    "\n",
    "# Paths for input (original dataset) and output (preprocessed dataset)\n",
    "INPUT_PATH = \"../../data/youtoxic_english_1000.csv\"\n",
    "OUTPUT_PATH = \"data/youtoxic_english_1000_clean.csv\"\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Salida:**\n",
    "\n",
    "Esta celda importa las librer√≠as necesarias para el preprocesamiento y define las rutas\n",
    "del fichero de entrada y de salida. Todav√≠a no se ha cargado ni transformado el dataset.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b46db",
   "metadata": {},
   "source": [
    "## Paso 2: Descargar recursos de NLTK (si es necesario)\n",
    "\n",
    "NLTK necesita descargar algunos recursos la primera vez:\n",
    "\n",
    "- `stopwords` ‚Üí lista de stopwords en ingl√©s.\n",
    "- `wordnet`, `omw-1.4` ‚Üí recursos para lematizaci√≥n.\n",
    "\n",
    "Esta celda se ejecuta normalmente una sola vez por entorno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56707a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Download required NLTK resources (run once per environment)\n",
    "\n",
    "nltk.download(\"stopwords\")   # English stopwords list\n",
    "nltk.download(\"wordnet\")     # WordNet lexical database for lemmatization\n",
    "nltk.download(\"omw-1.4\")     # Additional WordNet data\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "Esta celda descarga (si es necesario) los recursos de NLTK que se usar√°n para:\n",
    "\n",
    "- Obtener la lista de stopwords en ingl√©s.\n",
    "- Lematizar palabras mediante WordNet.\n",
    "\n",
    "Si ya estaban descargados, NLTK los reutiliza y la descarga ser√° muy r√°pida.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fa827",
   "metadata": {},
   "source": [
    "## Paso 3: Configurar stopwords y lematizador\n",
    "\n",
    "En esta celda:\n",
    "\n",
    "- Creamos el conjunto de stopwords en ingl√©s.\n",
    "- Instanciamos el lematizador de WordNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a35b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Configure English stopwords and lemmatizer\n",
    "\n",
    "# English stopwords set (e.g. \"the\", \"is\", \"at\", \"which\", \"on\", ...)\n",
    "stopwords_en = set(stopwords.words(\"english\"))\n",
    "\n",
    "# WordNet-based lemmatizer for English\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "- `stopwords_en` contiene la lista de stopwords en ingl√©s, que utilizaremos para\n",
    "  eliminar palabras muy frecuentes y poco informativas.\n",
    "- `lemmatizer` nos permitir√° obtener el **lema** (forma base) de cada palabra\n",
    "  en ingl√©s (por ejemplo, *running* ‚Üí *run*).\n",
    "\n",
    "Estas herramientas se usar√°n dentro del pipeline de preprocesamiento cl√°sico.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b77cdb",
   "metadata": {},
   "source": [
    "## Paso 4: Cargar el dataset original\n",
    "\n",
    "Vamos a cargar el CSV original con comentarios y etiquetas, tal y como viene de origen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cae311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Load the original dataset\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)  # Load CSV with comments and labels\n",
    "\n",
    "display(df.head())\n",
    "print(f\"\\nN√∫mero de filas: {df.shape[0]}\")\n",
    "print(f\"N√∫mero de columnas: {df.shape[1]}\")\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "- Se muestran las primeras filas del dataset original.\n",
    "- Se confirma el n√∫mero de filas y columnas.\n",
    "\n",
    "Este ser√° el punto de partida para aplicar el preprocesamiento de texto.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace819d8",
   "metadata": {},
   "source": [
    "## Paso 5: Definir limpieza b√°sica de texto (incluyendo emojis)\n",
    "\n",
    "Definiremos una funci√≥n `basic_clean` que haga:\n",
    "\n",
    "1. Conversi√≥n de emojis a texto en ingl√©s (`emoji.demojize`).\n",
    "2. Paso a min√∫sculas.\n",
    "3. Eliminaci√≥n de:\n",
    "   - URLs.\n",
    "   - Menciones (`@usuario`) y hashtags (`#tema`).\n",
    "   - Saltos de l√≠nea.\n",
    "4. Normalizaci√≥n de espacios (evitar espacios m√∫ltiples).\n",
    "\n",
    "El resultado ser√° la columna `text_basic`, que es una versi√≥n del texto con ruido reducido\n",
    "y que puede ser com√∫n tanto para modelos cl√°sicos como para modelos modernos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e736aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5: Define a basic text cleaning function (including emojis)\n",
    "\n",
    "# Regular expression patterns for cleaning\n",
    "url_pattern = r\"http\\S+|www\\S+\"            # URLs (http://, https://, www.)\n",
    "mention_hashtag_pattern = r\"(@\\w+)|(#\\w+)\" # Mentions (@user) and hashtags (#topic)\n",
    "newline_pattern = r\"[\\r\\n]+\"               # Newline characters\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply basic cleaning to a text comment:\n",
    "    1. Convert emojis to text (in English).\n",
    "    2. Convert to lowercase.\n",
    "    3. Remove URLs.\n",
    "    4. Remove mentions and hashtags.\n",
    "    5. Remove newlines.\n",
    "    6. Normalize multiple spaces.\n",
    "    \"\"\"\n",
    "    # Ensure we work with a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # 1) Convert emojis to text, e.g. \"I ‚ù§Ô∏è you\" -> \"I :red_heart: you\"\n",
    "    text = emoji.demojize(text, language=\"en\")\n",
    "\n",
    "    # 2) Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 3) Remove URLs\n",
    "    text = re.sub(url_pattern, \" \", text)\n",
    "\n",
    "    # 4) Remove mentions and hashtags\n",
    "    text = re.sub(mention_hashtag_pattern, \" \", text)\n",
    "\n",
    "    # 5) Remove newline characters\n",
    "    text = re.sub(newline_pattern, \" \", text)\n",
    "\n",
    "    # 6) Normalize whitespace (collapse multiple spaces into one)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "No se muestra nada todav√≠a, pero hemos definido `basic_clean`, una funci√≥n que:\n",
    "\n",
    "- Convierte emojis a texto (en ingl√©s), preservando la informaci√≥n emocional.\n",
    "- Normaliza el texto a min√∫sculas.\n",
    "- Elimina URLs, menciones, hashtags y saltos de l√≠nea.\n",
    "- Limpia espacios extra.\n",
    "\n",
    "Esta limpieza es relativamente ligera y puede usarse tanto antes de modelos cl√°sicos como modernos.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457797b0",
   "metadata": {},
   "source": [
    "## Paso 6: Tokenizaci√≥n, eliminaci√≥n de stopwords y lematizaci√≥n (pipeline cl√°sico)\n",
    "\n",
    "Ahora definiremos funciones para el preprocesamiento cl√°sico:\n",
    "\n",
    "1. `tokenize(text)`  \n",
    "   - Convierte un string en una lista de tokens (palabras) usando una expresi√≥n regular simple.\n",
    "2. `remove_stopwords(tokens)`  \n",
    "   - Elimina stopwords en ingl√©s.\n",
    "3. `lemmatize_tokens(tokens)`  \n",
    "   - Aplica lematizaci√≥n para reducir cada palabra a su forma base.\n",
    "\n",
    "Estas funciones ser√°n espec√≠ficas para el pipeline de **modelos cl√°sicos**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8443bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6: Define tokenization, stopword removal, and lemmatization functions (classic NLP pipeline)\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text into a list of word tokens using a simple regex.\n",
    "    Only alphabetic characters and apostrophes are kept (English-focused).\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r\"[a-zA-Z']+\", text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Remove English stopwords from the list of tokens.\n",
    "    \"\"\"\n",
    "    filtered_tokens = [t for t in tokens if t not in stopwords_en]\n",
    "    return filtered_tokens\n",
    "\n",
    "def lemmatize_tokens(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Lemmatize tokens using WordNet lemmatizer.\n",
    "    (Simple version: no POS tagging, assumes default part-of-speech.)\n",
    "    \"\"\"\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "Hemos definido tres funciones clave para el pipeline cl√°sico:\n",
    "\n",
    "1. `tokenize(text)`  \n",
    "   - Tokeniza el texto en una lista de palabras usando una expresi√≥n regular orientada a ingl√©s.\n",
    "\n",
    "2. `remove_stopwords(tokens)`  \n",
    "   - Elimina las stopwords en ingl√©s (the, is, at, which, on, ...).\n",
    "\n",
    "3. `lemmatize_tokens(tokens)`  \n",
    "   - Aplica lematizaci√≥n para convertir cada palabra a su forma base usando WordNet.\n",
    "\n",
    "Estas funciones se encadenar√°n en un pipeline que generar√° un texto preparado\n",
    "para modelos cl√°sicos (TF-IDF, BoW, etc.).\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f46804",
   "metadata": {},
   "source": [
    "## Paso 7: Definir el pipeline completo para modelos cl√°sicos\n",
    "\n",
    "Crearemos una funci√≥n `preprocess_text_classic` que aplique, en este orden:\n",
    "\n",
    "1. `basic_clean` ‚Üí limpieza ligera (emojis, min√∫sculas, URLs, menciones...).\n",
    "2. `tokenize` ‚Üí transformar texto en lista de palabras.\n",
    "3. `remove_stopwords` ‚Üí eliminar palabras poco informativas.\n",
    "4. `lemmatize_tokens` ‚Üí normalizar palabras a su lema.\n",
    "5. Unir los tokens resultantes en un string final (`\"token1 token2 ...\"`).\n",
    "\n",
    "Esta funci√≥n generar√° la columna `text_classic`, pensada para vectorizaci√≥n cl√°sica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f7e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 7: Define full preprocessing pipeline function for classic models\n",
    "\n",
    "def preprocess_text_classic(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply the full preprocessing pipeline for classic models:\n",
    "    1. Basic cleaning (emojis -> text, lowercase, URLs/mentions removal, etc.).\n",
    "    2. Tokenization.\n",
    "    3. English stopword removal.\n",
    "    4. Lemmatization.\n",
    "    5. Join processed tokens into a single string.\n",
    "    \"\"\"\n",
    "    # 1) Basic cleaning\n",
    "    cleaned = basic_clean(text)\n",
    "\n",
    "    # 2) Tokenization\n",
    "    tokens = tokenize(cleaned)\n",
    "\n",
    "    # 3) Stopword removal\n",
    "    tokens_no_stop = remove_stopwords(tokens)\n",
    "\n",
    "    # 4) Lemmatization\n",
    "    tokens_lemma = lemmatize_tokens(tokens_no_stop)\n",
    "\n",
    "    # 5) Join tokens back into a single string\n",
    "    processed_text = \" \".join(tokens_lemma)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "La funci√≥n `preprocess_text_classic` encadena todo el pipeline cl√°sico:\n",
    "\n",
    "1. Limpieza b√°sica (`basic_clean`).\n",
    "2. Tokenizaci√≥n (`tokenize`).\n",
    "3. Eliminaci√≥n de stopwords (`remove_stopwords`).\n",
    "4. Lematizaci√≥n (`lemmatize_tokens`).\n",
    "5. Uni√≥n de tokens en un texto procesado.\n",
    "\n",
    "Este texto procesado es ideal para vectorizadores como `CountVectorizer` o `TfidfVectorizer`\n",
    "en el notebook de modelado.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95726a91",
   "metadata": {},
   "source": [
    "## Paso 8: Aplicar el preprocesamiento al dataset\n",
    "\n",
    "Vamos a crear dos columnas:\n",
    "\n",
    "- `text_basic` ‚Üí resultado de aplicar `basic_clean` (limpieza ligera).\n",
    "- `text_classic` ‚Üí resultado de aplicar `preprocess_text_classic` (pipeline cl√°sico completo).\n",
    "\n",
    "Luego mostraremos algunos ejemplos comparando:\n",
    "\n",
    "- `Text` (original).\n",
    "- `text_basic`.\n",
    "- `text_classic`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f63390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 8: Apply preprocessing to the whole dataset\n",
    "\n",
    "# Light-cleaned text (shared baseline)\n",
    "df[\"text_basic\"] = df[\"Text\"].apply(basic_clean)\n",
    "\n",
    "# Fully processed text for classic models\n",
    "df[\"text_classic\"] = df[\"Text\"].apply(preprocess_text_classic)\n",
    "\n",
    "# Show a few random examples to inspect the transformations\n",
    "examples = df[[\"Text\", \"text_basic\", \"text_classic\"]].sample(5, random_state=42)\n",
    "display(examples)\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "La tabla muestra, para varios comentarios:\n",
    "\n",
    "- `Text`: texto original sin preprocesar.\n",
    "- `text_basic`: texto tras la limpieza b√°sica (emojis a texto, min√∫sculas, sin URLs ni menciones).\n",
    "- `text_classic`: texto tras el pipeline cl√°sico completo (limpieza b√°sica + tokenizaci√≥n + stopwords + lemas).\n",
    "\n",
    "Esto permite comprobar visualmente que el preprocesamiento es razonable y que el texto\n",
    "sigue siendo interpretable despu√©s de las transformaciones.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13f300",
   "metadata": {},
   "source": [
    "## Paso 9 (opcional): Longitud del texto preprocesado\n",
    "\n",
    "Podemos a√±adir:\n",
    "\n",
    "- `text_len_classic`: longitud en caracteres de `text_classic`.\n",
    "- `word_count_classic`: n√∫mero de palabras en `text_classic`.\n",
    "\n",
    "Estas columnas pueden servir como features adicionales o para an√°lisis posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 9: Compute length metrics on processed text (optional)\n",
    "\n",
    "df[\"text_len_classic\"] = df[\"text_classic\"].str.len()\n",
    "df[\"word_count_classic\"] = df[\"text_classic\"].str.split().str.len()\n",
    "\n",
    "display(df[[\"text_classic\", \"text_len_classic\", \"word_count_classic\"]].head())\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "- `text_len_classic`: n√∫mero de caracteres del texto ya preprocesado.\n",
    "- `word_count_classic`: n√∫mero de palabras del texto preprocesado.\n",
    "\n",
    "Estas columnas ayudan a entender c√≥mo el pipeline de preprocesamiento ha afectado\n",
    "a la longitud de los comentarios y pueden utilizarse como variables adicionales\n",
    "en los modelos si se considera √∫til.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238ad0c",
   "metadata": {},
   "source": [
    "## Paso 10: Guardar el dataset preprocesado\n",
    "\n",
    "Guardaremos un nuevo CSV con:\n",
    "\n",
    "- Columnas originales.\n",
    "- Columnas de preprocesamiento:\n",
    "  - `text_basic`\n",
    "  - `text_classic`\n",
    "  - `text_len_classic`\n",
    "  - `word_count_classic`\n",
    "\n",
    "Este fichero ser√° el punto de partida en el notebook de modelado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ec500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 10: Save preprocessed dataset to CSV\n",
    "\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Dataset preprocesado guardado en: {OUTPUT_PATH}\")\n",
    "\n",
    "display(Markdown(\n",
    "f\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "Se ha guardado el dataset preprocesado en:\n",
    "\n",
    "`{OUTPUT_PATH}`\n",
    "\n",
    "El fichero contiene:\n",
    "- Todas las columnas originales (texto, IDs y etiquetas `Is...`).\n",
    "- `text_basic`: texto con limpieza ligera (√∫til para pipelines cl√°sicos y modernos).\n",
    "- `text_classic`: texto preprocesado para modelos cl√°sicos (tokenizaci√≥n + stopwords + lemas).\n",
    "- `text_len_classic` y `word_count_classic`: m√©tricas de longitud del texto procesado.\n",
    "\n",
    "El siguiente paso ser√° usar este fichero en el notebook de **modelado**,\n",
    "donde aplicaremos vectorizaci√≥n (TF-IDF, BoW, etc.) y entrenaremos modelos de clasificaci√≥n.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7285333c",
   "metadata": {},
   "source": [
    "# Resumen del notebook de preprocesamiento\n",
    "\n",
    "En este notebook hemos:\n",
    "\n",
    "1. Cargado el dataset original de comentarios de YouTube.\n",
    "2. Definido una funci√≥n de **limpieza b√°sica** (`basic_clean`) que:\n",
    "   - Convierte emojis a texto en ingl√©s.\n",
    "   - Pasa a min√∫sculas.\n",
    "   - Elimina URLs, menciones, hashtags y saltos de l√≠nea.\n",
    "   - Normaliza espacios.\n",
    "3. Configurado un pipeline cl√°sico de NLP en ingl√©s:\n",
    "   - `tokenize` ‚Üí tokeniza el texto.\n",
    "   - `remove_stopwords` ‚Üí elimina stopwords en ingl√©s.\n",
    "   - `lemmatize_tokens` ‚Üí lematiza las palabras usando WordNet.\n",
    "4. Implementado `preprocess_text_classic` para encadenar todos los pasos y obtener:\n",
    "   - `text_classic`: texto limpio, tokenizado, sin stopwords y lematizado.\n",
    "5. Creado una versi√≥n de limpieza ligera (`text_basic`) que puede servir tambi√©n como entrada\n",
    "   para modelos modernos si en el futuro se usan transformers.\n",
    "6. A√±adido columnas de longitud sobre el texto procesado.\n",
    "7. Guardado un CSV preprocesado (`youtoxic_english_1000_clean.csv`) listo para el notebook de modelado.\n",
    "\n",
    "---\n",
    "\n",
    "## Pr√≥ximos pasos\n",
    "\n",
    "En el notebook de **modelado** (por ejemplo, `4.3-modeling-baseline.ipynb`):\n",
    "\n",
    "1. Cargar `youtoxic_english_1000_clean.csv`.\n",
    "2. Elegir la etiqueta objetivo (p. ej. `IsToxic`).\n",
    "3. Hacer un **train/test split**.\n",
    "4. Vectorizar `text_classic` con:\n",
    "   - `CountVectorizer` o `TfidfVectorizer`.\n",
    "5. Entrenar modelos cl√°sicos:\n",
    "   - Logistic Regression, Linear SVM, Naive Bayes, etc.\n",
    "6. Calcular m√©tricas (accuracy, precision, recall, F1, matriz de confusi√≥n).\n",
    "7. Comparar modelos y, a partir de ah√≠, avanzar a ensembles, tuning o incluso modelos modernos.\n",
    "\n",
    "Con estos dos notebooks (4.2-eda y 4.2-preprocessing) ya tienes la base s√≥lida de datos listos para modelar. üí™\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
