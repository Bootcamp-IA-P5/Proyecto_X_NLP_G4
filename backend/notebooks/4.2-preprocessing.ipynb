{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eea0556",
   "metadata": {},
   "source": [
    "# 02 ‚Äì Preprocesamiento de texto para detecci√≥n de toxicidad\n",
    "\n",
    "En este notebook vamos a construir el pipeline de **preprocesamiento de texto** sobre el dataset\n",
    "`youtoxic_english_1000.csv` para dejarlo listo de cara a modelos **cl√°sicos** (TF-IDF + clasificadores).\n",
    "\n",
    "Concretamente:\n",
    "\n",
    "1. Cargaremos el dataset original.\n",
    "2. Definiremos una funci√≥n de limpieza b√°sica:\n",
    "   - Conversi√≥n de emojis a texto en ingl√©s.\n",
    "   - Paso a min√∫sculas.\n",
    "   - Eliminaci√≥n de URLs, menciones, hashtags, saltos de l√≠nea y espacios extra.\n",
    "3. Implementaremos un pipeline cl√°sico de NLP en ingl√©s:\n",
    "   - Tokenizaci√≥n.\n",
    "   - Eliminaci√≥n de stopwords.\n",
    "   - Lematizaci√≥n.\n",
    "4. Crearemos:\n",
    "   - `text_basic`: texto con limpieza ligera (apto tanto para modelos cl√°sicos como modernos).\n",
    "   - `text_classic`: texto procesado (tokenizado, sin stopwords, lematizado) para modelos cl√°sicos.\n",
    "5. Guardaremos un CSV preprocesado que se usar√° en el notebook de modelado.\n",
    "\n",
    "> Nota: la vectorizaci√≥n (TF-IDF, Bag-of-Words, etc.) se har√° en el notebook de modelado,\n",
    "> a partir de la columna `text_classic`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80284f",
   "metadata": {},
   "source": [
    "## Paso 1: Importar librer√≠as y configurar el entorno\n",
    "\n",
    "Importamos las librer√≠as necesarias para el preprocesamiento y definimos las rutas del fichero de entrada y de salida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d1f9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Salida:**\n",
       "\n",
       "Esta celda importa las librer√≠as necesarias para el preprocesamiento y define las rutas\n",
       "del fichero de entrada y de salida. Todav√≠a no se ha cargado ni transformado el dataset.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paso 1: Import libraries for preprocessing\n",
    "\n",
    "import pandas as pd                      # Data manipulation with DataFrames\n",
    "import numpy as np                       # Numerical utilities (if needed)\n",
    "import re                                # Regular expressions for text cleaning and tokenization\n",
    "import emoji                             # Emoji handling: convert emojis to text\n",
    "import nltk                              # Classic NLP library: stopwords, lemmatization, etc.\n",
    "from nltk.corpus import stopwords        # Stopword lists for multiple languages (we use English)\n",
    "from nltk.stem import WordNetLemmatizer  # Lemmatizer based on WordNet (English)\n",
    "from IPython.display import Markdown, display  # Display Markdown explanations from code cells\n",
    "\n",
    "# Paths for input (original dataset) and output (preprocessed dataset)\n",
    "INPUT_PATH = \"../../data/youtoxic_english_1000.csv\"\n",
    "OUTPUT_PATH = \"../../data/youtoxic_english_1000_clean.csv\"\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Salida:**\n",
    "\n",
    "Esta celda importa las librer√≠as necesarias para el preprocesamiento y define las rutas\n",
    "del fichero de entrada y de salida. Todav√≠a no se ha cargado ni transformado el dataset.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b46db",
   "metadata": {},
   "source": [
    "## Paso 2: Descargar recursos de NLTK (si es necesario)\n",
    "\n",
    "NLTK necesita descargar algunos recursos la primera vez:\n",
    "\n",
    "- `stopwords` ‚Üí lista de stopwords en ingl√©s.\n",
    "- `wordnet`, `omw-1.4` ‚Üí recursos para lematizaci√≥n.\n",
    "\n",
    "Esta celda se ejecuta normalmente una sola vez por entorno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56707a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alfbb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alfbb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\alfbb\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Explicaci√≥n de la salida:**\n",
       "\n",
       "Esta celda descarga (si es necesario) los recursos de NLTK que se usar√°n para:\n",
       "\n",
       "- Obtener la lista de stopwords en ingl√©s.\n",
       "- Lematizar palabras mediante WordNet.\n",
       "\n",
       "Si ya estaban descargados, NLTK los reutiliza y la descarga ser√° muy r√°pida.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paso 2: Download required NLTK resources (run once per environment)\n",
    "\n",
    "nltk.download(\"stopwords\")   # English stopwords list\n",
    "nltk.download(\"wordnet\")     # WordNet lexical database for lemmatization\n",
    "nltk.download(\"omw-1.4\")     # Additional WordNet data\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "Esta celda descarga (si es necesario) los recursos de NLTK que se usar√°n para:\n",
    "\n",
    "- Obtener la lista de stopwords en ingl√©s.\n",
    "- Lematizar palabras mediante WordNet.\n",
    "\n",
    "Si ya estaban descargados, NLTK los reutiliza y la descarga ser√° muy r√°pida.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0fa827",
   "metadata": {},
   "source": [
    "## Paso 3: Configurar stopwords y lematizador\n",
    "\n",
    "En esta celda:\n",
    "\n",
    "- Creamos el conjunto de stopwords en ingl√©s.\n",
    "- Instanciamos el lematizador de WordNet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a35b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Explicaci√≥n de la salida:**\n",
       "\n",
       "- `stopwords_en` contiene la lista de stopwords en ingl√©s, que utilizaremos para\n",
       "  eliminar palabras muy frecuentes y poco informativas.\n",
       "- `lemmatizer` nos permitir√° obtener el **lema** (forma base) de cada palabra\n",
       "  en ingl√©s (por ejemplo, *running* ‚Üí *run*).\n",
       "\n",
       "Estas herramientas se usar√°n dentro del pipeline de preprocesamiento cl√°sico.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paso 3: Configure English stopwords and lemmatizer\n",
    "\n",
    "# English stopwords set (e.g. \"the\", \"is\", \"at\", \"which\", \"on\", ...)\n",
    "stopwords_en = set(stopwords.words(\"english\"))\n",
    "\n",
    "# WordNet-based lemmatizer for English\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "- `stopwords_en` contiene la lista de stopwords en ingl√©s, que utilizaremos para\n",
    "  eliminar palabras muy frecuentes y poco informativas.\n",
    "- `lemmatizer` nos permitir√° obtener el **lema** (forma base) de cada palabra\n",
    "  en ingl√©s (por ejemplo, *running* ‚Üí *run*).\n",
    "\n",
    "Estas herramientas se usar√°n dentro del pipeline de preprocesamiento cl√°sico.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b77cdb",
   "metadata": {},
   "source": [
    "## Paso 4: Cargar el dataset original\n",
    "\n",
    "Vamos a cargar el CSV original con comentarios y etiquetas, tal y como viene de origen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0cae311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CommentId</th>\n",
       "      <th>VideoId</th>\n",
       "      <th>Text</th>\n",
       "      <th>IsToxic</th>\n",
       "      <th>IsAbusive</th>\n",
       "      <th>IsThreat</th>\n",
       "      <th>IsProvocative</th>\n",
       "      <th>IsObscene</th>\n",
       "      <th>IsHatespeech</th>\n",
       "      <th>IsRacist</th>\n",
       "      <th>IsNationalist</th>\n",
       "      <th>IsSexist</th>\n",
       "      <th>IsHomophobic</th>\n",
       "      <th>IsReligiousHate</th>\n",
       "      <th>IsRadicalism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ugg2KwwX0V8-aXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>If only people would just take a step back and...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ugg2s5AzSPioEXgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>Law enforcement is not trained to shoot to app...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ugg3dWTOxryFfHgCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>\\r\\nDont you reckon them 'black lives matter' ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ugg7Gd006w1MPngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>There are a very large number of people who do...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ugg8FfTbbNF8IngCoAEC</td>\n",
       "      <td>04kJtp6pVXI</td>\n",
       "      <td>The Arab dude is absolutely right, he should h...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              CommentId      VideoId  \\\n",
       "0  Ugg2KwwX0V8-aXgCoAEC  04kJtp6pVXI   \n",
       "1  Ugg2s5AzSPioEXgCoAEC  04kJtp6pVXI   \n",
       "2  Ugg3dWTOxryFfHgCoAEC  04kJtp6pVXI   \n",
       "3  Ugg7Gd006w1MPngCoAEC  04kJtp6pVXI   \n",
       "4  Ugg8FfTbbNF8IngCoAEC  04kJtp6pVXI   \n",
       "\n",
       "                                                Text  IsToxic  IsAbusive  \\\n",
       "0  If only people would just take a step back and...    False      False   \n",
       "1  Law enforcement is not trained to shoot to app...     True       True   \n",
       "2  \\r\\nDont you reckon them 'black lives matter' ...     True       True   \n",
       "3  There are a very large number of people who do...    False      False   \n",
       "4  The Arab dude is absolutely right, he should h...    False      False   \n",
       "\n",
       "   IsThreat  IsProvocative  IsObscene  IsHatespeech  IsRacist  IsNationalist  \\\n",
       "0     False          False      False         False     False          False   \n",
       "1     False          False      False         False     False          False   \n",
       "2     False          False       True         False     False          False   \n",
       "3     False          False      False         False     False          False   \n",
       "4     False          False      False         False     False          False   \n",
       "\n",
       "   IsSexist  IsHomophobic  IsReligiousHate  IsRadicalism  \n",
       "0     False         False            False         False  \n",
       "1     False         False            False         False  \n",
       "2     False         False            False         False  \n",
       "3     False         False            False         False  \n",
       "4     False         False            False         False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N√∫mero de filas: 1000\n",
      "N√∫mero de columnas: 15\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Explicaci√≥n de la salida:**\n",
       "\n",
       "- Se muestran las primeras filas del dataset original.\n",
       "- Se confirma el n√∫mero de filas y columnas.\n",
       "\n",
       "Este ser√° el punto de partida para aplicar el preprocesamiento de texto.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paso 4: Load the original dataset\n",
    "\n",
    "df = pd.read_csv(INPUT_PATH)  # Load CSV with comments and labels\n",
    "\n",
    "display(df.head())\n",
    "print(f\"\\nN√∫mero de filas: {df.shape[0]}\")\n",
    "print(f\"N√∫mero de columnas: {df.shape[1]}\")\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "- Se muestran las primeras filas del dataset original.\n",
    "- Se confirma el n√∫mero de filas y columnas.\n",
    "\n",
    "Este ser√° el punto de partida para aplicar el preprocesamiento de texto.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace819d8",
   "metadata": {},
   "source": [
    "## Paso 5: Definir limpieza b√°sica de texto (incluyendo emojis)\n",
    "\n",
    "Definiremos una funci√≥n `basic_clean` que haga:\n",
    "\n",
    "1. Conversi√≥n de emojis a texto en ingl√©s (`emoji.demojize`).\n",
    "2. Paso a min√∫sculas.\n",
    "3. Eliminaci√≥n de:\n",
    "   - URLs.\n",
    "   - Menciones (`@usuario`) y hashtags (`#tema`).\n",
    "   - Saltos de l√≠nea.\n",
    "4. Normalizaci√≥n de espacios (evitar espacios m√∫ltiples).\n",
    "\n",
    "El resultado ser√° la columna `text_basic`, que es una versi√≥n del texto con ruido reducido\n",
    "y que puede ser com√∫n tanto para modelos cl√°sicos como para modelos modernos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e736aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Explicaci√≥n de la salida:**\n",
       "\n",
       "No se muestra nada todav√≠a, pero hemos definido `basic_clean`, una funci√≥n que:\n",
       "\n",
       "- Convierte emojis a texto (en ingl√©s), preservando la informaci√≥n emocional.\n",
       "- Normaliza el texto a min√∫sculas.\n",
       "- Elimina URLs, menciones, hashtags y saltos de l√≠nea.\n",
       "- Limpia espacios extra.\n",
       "\n",
       "Esta limpieza es relativamente ligera y puede usarse tanto antes de modelos cl√°sicos como modernos.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paso 5: Define a basic text cleaning function (including emojis)\n",
    "\n",
    "# Regular expression patterns for cleaning\n",
    "url_pattern = r\"http\\S+|www\\S+\"            # URLs (http://, https://, www.)\n",
    "mention_hashtag_pattern = r\"(@\\w+)|(#\\w+)\" # Mentions (@user) and hashtags (#topic)\n",
    "newline_pattern = r\"[\\r\\n]+\"               # Newline characters\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply basic cleaning to a text comment:\n",
    "    1. Convert emojis to text (in English).\n",
    "    2. Convert to lowercase.\n",
    "    3. Remove URLs.\n",
    "    4. Remove mentions and hashtags.\n",
    "    5. Remove newlines.\n",
    "    6. Normalize multiple spaces.\n",
    "    \"\"\"\n",
    "    # Ensure we work with a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # 1) Convert emojis to text, e.g. \"I ‚ù§Ô∏è you\" -> \"I :red_heart: you\"\n",
    "    text = emoji.demojize(text, language=\"en\")\n",
    "\n",
    "    # 2) Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 3) Remove URLs\n",
    "    text = re.sub(url_pattern, \" \", text)\n",
    "\n",
    "    # 4) Remove mentions and hashtags\n",
    "    text = re.sub(mention_hashtag_pattern, \" \", text)\n",
    "\n",
    "    # 5) Remove newline characters\n",
    "    text = re.sub(newline_pattern, \" \", text)\n",
    "\n",
    "    # 6) Normalize whitespace (collapse multiple spaces into one)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "No se muestra nada todav√≠a, pero hemos definido `basic_clean`, una funci√≥n que:\n",
    "\n",
    "- Convierte emojis a texto (en ingl√©s), preservando la informaci√≥n emocional.\n",
    "- Normaliza el texto a min√∫sculas.\n",
    "- Elimina URLs, menciones, hashtags y saltos de l√≠nea.\n",
    "- Limpia espacios extra.\n",
    "\n",
    "Esta limpieza es relativamente ligera y puede usarse tanto antes de modelos cl√°sicos como modernos.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457797b0",
   "metadata": {},
   "source": [
    "## Paso 6: Tokenizaci√≥n, eliminaci√≥n de stopwords y lematizaci√≥n (pipeline cl√°sico)\n",
    "\n",
    "Ahora definiremos funciones para el preprocesamiento cl√°sico:\n",
    "\n",
    "1. `tokenize(text)`  \n",
    "   - Convierte un string en una lista de tokens (palabras) usando una expresi√≥n regular simple.\n",
    "2. `remove_stopwords(tokens)`  \n",
    "   - Elimina stopwords en ingl√©s.\n",
    "3. `lemmatize_tokens(tokens)`  \n",
    "   - Aplica lematizaci√≥n para reducir cada palabra a su forma base.\n",
    "\n",
    "Estas funciones ser√°n espec√≠ficas para el pipeline de **modelos cl√°sicos**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8443bdac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Explicaci√≥n de la salida:**\n",
       "\n",
       "Hemos definido tres funciones clave para el pipeline cl√°sico:\n",
       "\n",
       "1. `tokenize(text)`  \n",
       "   - Tokeniza el texto en una lista de palabras usando una expresi√≥n regular orientada a ingl√©s.\n",
       "\n",
       "2. `remove_stopwords(tokens)`  \n",
       "   - Elimina las stopwords en ingl√©s (the, is, at, which, on, ...).\n",
       "\n",
       "3. `lemmatize_tokens(tokens)`  \n",
       "   - Aplica lematizaci√≥n para convertir cada palabra a su forma base usando WordNet.\n",
       "\n",
       "Estas funciones se encadenar√°n en un pipeline que generar√° un texto preparado\n",
       "para modelos cl√°sicos (TF-IDF, BoW, etc.).\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paso 6: Define tokenization, stopword removal, and lemmatization functions (classic NLP pipeline)\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text into a list of word tokens using a simple regex.\n",
    "    Only alphabetic characters and apostrophes are kept (English-focused).\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r\"[a-zA-Z']+\", text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Remove English stopwords from the list of tokens.\n",
    "    \"\"\"\n",
    "    filtered_tokens = [t for t in tokens if t not in stopwords_en]\n",
    "    return filtered_tokens\n",
    "\n",
    "def lemmatize_tokens(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Lemmatize tokens using WordNet lemmatizer.\n",
    "    (Simple version: no POS tagging, assumes default part-of-speech.)\n",
    "    \"\"\"\n",
    "    lemmatized = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "Hemos definido tres funciones clave para el pipeline cl√°sico:\n",
    "\n",
    "1. `tokenize(text)`  \n",
    "   - Tokeniza el texto en una lista de palabras usando una expresi√≥n regular orientada a ingl√©s.\n",
    "\n",
    "2. `remove_stopwords(tokens)`  \n",
    "   - Elimina las stopwords en ingl√©s (the, is, at, which, on, ...).\n",
    "\n",
    "3. `lemmatize_tokens(tokens)`  \n",
    "   - Aplica lematizaci√≥n para convertir cada palabra a su forma base usando WordNet.\n",
    "\n",
    "Estas funciones se encadenar√°n en un pipeline que generar√° un texto preparado\n",
    "para modelos cl√°sicos (TF-IDF, BoW, etc.).\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f46804",
   "metadata": {},
   "source": [
    "## Paso 7: Definir el pipeline completo para modelos cl√°sicos\n",
    "\n",
    "Crearemos una funci√≥n `preprocess_text_classic` que aplique, en este orden:\n",
    "\n",
    "1. `basic_clean` ‚Üí limpieza ligera (emojis, min√∫sculas, URLs, menciones...).\n",
    "2. `tokenize` ‚Üí transformar texto en lista de palabras.\n",
    "3. `remove_stopwords` ‚Üí eliminar palabras poco informativas.\n",
    "4. `lemmatize_tokens` ‚Üí normalizar palabras a su lema.\n",
    "5. Unir los tokens resultantes en un string final (`\"token1 token2 ...\"`).\n",
    "\n",
    "Esta funci√≥n generar√° la columna `text_classic`, pensada para vectorizaci√≥n cl√°sica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47f7e20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Explicaci√≥n de la salida:**\n",
       "\n",
       "La funci√≥n `preprocess_text_classic` encadena todo el pipeline cl√°sico:\n",
       "\n",
       "1. Limpieza b√°sica (`basic_clean`).\n",
       "2. Tokenizaci√≥n (`tokenize`).\n",
       "3. Eliminaci√≥n de stopwords (`remove_stopwords`).\n",
       "4. Lematizaci√≥n (`lemmatize_tokens`).\n",
       "5. Uni√≥n de tokens en un texto procesado.\n",
       "\n",
       "Este texto procesado es ideal para vectorizadores como `CountVectorizer` o `TfidfVectorizer`\n",
       "en el notebook de modelado.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paso 7: Define full preprocessing pipeline function for classic models\n",
    "\n",
    "def preprocess_text_classic(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Apply the full preprocessing pipeline for classic models:\n",
    "    1. Basic cleaning (emojis -> text, lowercase, URLs/mentions removal, etc.).\n",
    "    2. Tokenization.\n",
    "    3. English stopword removal.\n",
    "    4. Lemmatization.\n",
    "    5. Join processed tokens into a single string.\n",
    "    \"\"\"\n",
    "    # 1) Basic cleaning\n",
    "    cleaned = basic_clean(text)\n",
    "\n",
    "    # 2) Tokenization\n",
    "    tokens = tokenize(cleaned)\n",
    "\n",
    "    # 3) Stopword removal\n",
    "    tokens_no_stop = remove_stopwords(tokens)\n",
    "\n",
    "    # 4) Lemmatization\n",
    "    tokens_lemma = lemmatize_tokens(tokens_no_stop)\n",
    "\n",
    "    # 5) Join tokens back into a single string\n",
    "    processed_text = \" \".join(tokens_lemma)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "La funci√≥n `preprocess_text_classic` encadena todo el pipeline cl√°sico:\n",
    "\n",
    "1. Limpieza b√°sica (`basic_clean`).\n",
    "2. Tokenizaci√≥n (`tokenize`).\n",
    "3. Eliminaci√≥n de stopwords (`remove_stopwords`).\n",
    "4. Lematizaci√≥n (`lemmatize_tokens`).\n",
    "5. Uni√≥n de tokens en un texto procesado.\n",
    "\n",
    "Este texto procesado es ideal para vectorizadores como `CountVectorizer` o `TfidfVectorizer`\n",
    "en el notebook de modelado.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95726a91",
   "metadata": {},
   "source": [
    "## Paso 8: Aplicar el preprocesamiento al dataset\n",
    "\n",
    "Vamos a crear dos columnas:\n",
    "\n",
    "- `text_basic` ‚Üí resultado de aplicar `basic_clean` (limpieza ligera).\n",
    "- `text_classic` ‚Üí resultado de aplicar `preprocess_text_classic` (pipeline cl√°sico completo).\n",
    "\n",
    "Luego mostraremos algunos ejemplos comparando:\n",
    "\n",
    "- `Text` (original).\n",
    "- `text_basic`.\n",
    "- `text_classic`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f63390f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>text_basic</th>\n",
       "      <th>text_classic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>You call yourself an anarchist but defend a co...</td>\n",
       "      <td>you call yourself an anarchist but defend a co...</td>\n",
       "      <td>call anarchist defend cop shooting unarmed civ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>My mother told me the same thing.¬† God Bless t...</td>\n",
       "      <td>my mother told me the same thing. god bless th...</td>\n",
       "      <td>mother told thing god bless woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>Love it I same the saem thing Go Peggy!  #stup...</td>\n",
       "      <td>love it i same the saem thing go peggy! ya kil...</td>\n",
       "      <td>love saem thing go peggy ya killing ya self qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>Next time they do that, line up some cars and ...</td>\n",
       "      <td>next time they do that, line up some cars and ...</td>\n",
       "      <td>next time line car start making burnout smoke ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>He was Robbing the Store and Being a Big Man ....</td>\n",
       "      <td>he was robbing the store and being a big man ....</td>\n",
       "      <td>robbing store big man play fire get burnt poli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text  \\\n",
       "521  You call yourself an anarchist but defend a co...   \n",
       "737  My mother told me the same thing.¬† God Bless t...   \n",
       "740  Love it I same the saem thing Go Peggy!  #stup...   \n",
       "660  Next time they do that, line up some cars and ...   \n",
       "411  He was Robbing the Store and Being a Big Man ....   \n",
       "\n",
       "                                            text_basic  \\\n",
       "521  you call yourself an anarchist but defend a co...   \n",
       "737  my mother told me the same thing. god bless th...   \n",
       "740  love it i same the saem thing go peggy! ya kil...   \n",
       "660  next time they do that, line up some cars and ...   \n",
       "411  he was robbing the store and being a big man ....   \n",
       "\n",
       "                                          text_classic  \n",
       "521  call anarchist defend cop shooting unarmed civ...  \n",
       "737                  mother told thing god bless woman  \n",
       "740  love saem thing go peggy ya killing ya self qu...  \n",
       "660  next time line car start making burnout smoke ...  \n",
       "411  robbing store big man play fire get burnt poli...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Explicaci√≥n de la salida:**\n",
       "\n",
       "La tabla muestra, para varios comentarios:\n",
       "\n",
       "- `Text`: texto original sin preprocesar.\n",
       "- `text_basic`: texto tras la limpieza b√°sica (emojis a texto, min√∫sculas, sin URLs ni menciones).\n",
       "- `text_classic`: texto tras el pipeline cl√°sico completo (limpieza b√°sica + tokenizaci√≥n + stopwords + lemas).\n",
       "\n",
       "Esto permite comprobar visualmente que el preprocesamiento es razonable y que el texto\n",
       "sigue siendo interpretable despu√©s de las transformaciones.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paso 8: Apply preprocessing to the whole dataset\n",
    "\n",
    "# Light-cleaned text (shared baseline)\n",
    "df[\"text_basic\"] = df[\"Text\"].apply(basic_clean)\n",
    "\n",
    "# Fully processed text for classic models\n",
    "df[\"text_classic\"] = df[\"Text\"].apply(preprocess_text_classic)\n",
    "\n",
    "# Show a few random examples to inspect the transformations\n",
    "examples = df[[\"Text\", \"text_basic\", \"text_classic\"]].sample(5, random_state=42)\n",
    "display(examples)\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "La tabla muestra, para varios comentarios:\n",
    "\n",
    "- `Text`: texto original sin preprocesar.\n",
    "- `text_basic`: texto tras la limpieza b√°sica (emojis a texto, min√∫sculas, sin URLs ni menciones).\n",
    "- `text_classic`: texto tras el pipeline cl√°sico completo (limpieza b√°sica + tokenizaci√≥n + stopwords + lemas).\n",
    "\n",
    "Esto permite comprobar visualmente que el preprocesamiento es razonable y que el texto\n",
    "sigue siendo interpretable despu√©s de las transformaciones.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13f300",
   "metadata": {},
   "source": [
    "## Paso 9 (opcional): Longitud del texto preprocesado\n",
    "\n",
    "Podemos a√±adir:\n",
    "\n",
    "- `text_len_classic`: longitud en caracteres de `text_classic`.\n",
    "- `word_count_classic`: n√∫mero de palabras en `text_classic`.\n",
    "\n",
    "Estas columnas pueden servir como features adicionales o para an√°lisis posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "627e8c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_classic</th>\n",
       "      <th>text_len_classic</th>\n",
       "      <th>word_count_classic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people would take step back make case anyone e...</td>\n",
       "      <td>840</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>law enforcement trained shoot apprehend traine...</td>\n",
       "      <td>90</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dont reckon 'black life matter' banner held wh...</td>\n",
       "      <td>256</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>large number people like police officer called...</td>\n",
       "      <td>339</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arab dude absolutely right shot extra time sho...</td>\n",
       "      <td>135</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        text_classic  text_len_classic  \\\n",
       "0  people would take step back make case anyone e...               840   \n",
       "1  law enforcement trained shoot apprehend traine...                90   \n",
       "2  dont reckon 'black life matter' banner held wh...               256   \n",
       "3  large number people like police officer called...               339   \n",
       "4  arab dude absolutely right shot extra time sho...               135   \n",
       "\n",
       "   word_count_classic  \n",
       "0                 127  \n",
       "1                  13  \n",
       "2                  40  \n",
       "3                  49  \n",
       "4                  22  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Explicaci√≥n de la salida:**\n",
       "\n",
       "- `text_len_classic`: n√∫mero de caracteres del texto ya preprocesado.\n",
       "- `word_count_classic`: n√∫mero de palabras del texto preprocesado.\n",
       "\n",
       "Estas columnas ayudan a entender c√≥mo el pipeline de preprocesamiento ha afectado\n",
       "a la longitud de los comentarios y pueden utilizarse como variables adicionales\n",
       "en los modelos si se considera √∫til.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paso 9: Compute length metrics on processed text (optional)\n",
    "\n",
    "df[\"text_len_classic\"] = df[\"text_classic\"].str.len()\n",
    "df[\"word_count_classic\"] = df[\"text_classic\"].str.split().str.len()\n",
    "\n",
    "display(df[[\"text_classic\", \"text_len_classic\", \"word_count_classic\"]].head())\n",
    "\n",
    "display(Markdown(\n",
    "\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "- `text_len_classic`: n√∫mero de caracteres del texto ya preprocesado.\n",
    "- `word_count_classic`: n√∫mero de palabras del texto preprocesado.\n",
    "\n",
    "Estas columnas ayudan a entender c√≥mo el pipeline de preprocesamiento ha afectado\n",
    "a la longitud de los comentarios y pueden utilizarse como variables adicionales\n",
    "en los modelos si se considera √∫til.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238ad0c",
   "metadata": {},
   "source": [
    "## Paso 10: Guardar el dataset preprocesado\n",
    "\n",
    "Guardaremos un nuevo CSV con:\n",
    "\n",
    "- Columnas originales.\n",
    "- Columnas de preprocesamiento:\n",
    "  - `text_basic`\n",
    "  - `text_classic`\n",
    "  - `text_len_classic`\n",
    "  - `word_count_classic`\n",
    "\n",
    "Este fichero ser√° el punto de partida en el notebook de modelado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "306ec500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preprocesado guardado en: ../../data/youtoxic_english_1000_clean.csv\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Explicaci√≥n de la salida:**\n",
       "\n",
       "Se ha guardado el dataset preprocesado en:\n",
       "\n",
       "`../../data/youtoxic_english_1000_clean.csv`\n",
       "\n",
       "El fichero contiene:\n",
       "- Todas las columnas originales (texto, IDs y etiquetas `Is...`).\n",
       "- `text_basic`: texto con limpieza ligera (√∫til para pipelines cl√°sicos y modernos).\n",
       "- `text_classic`: texto preprocesado para modelos cl√°sicos (tokenizaci√≥n + stopwords + lemas).\n",
       "- `text_len_classic` y `word_count_classic`: m√©tricas de longitud del texto procesado.\n",
       "\n",
       "El siguiente paso ser√° usar este fichero en el notebook de **modelado**,\n",
       "donde aplicaremos vectorizaci√≥n (TF-IDF, BoW, etc.) y entrenaremos modelos de clasificaci√≥n.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paso 10: Save preprocessed dataset to CSV\n",
    "\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Dataset preprocesado guardado en: {OUTPUT_PATH}\")\n",
    "\n",
    "display(Markdown(\n",
    "f\"\"\"\n",
    "**Explicaci√≥n de la salida:**\n",
    "\n",
    "Se ha guardado el dataset preprocesado en:\n",
    "\n",
    "`{OUTPUT_PATH}`\n",
    "\n",
    "El fichero contiene:\n",
    "- Todas las columnas originales (texto, IDs y etiquetas `Is...`).\n",
    "- `text_basic`: texto con limpieza ligera (√∫til para pipelines cl√°sicos y modernos).\n",
    "- `text_classic`: texto preprocesado para modelos cl√°sicos (tokenizaci√≥n + stopwords + lemas).\n",
    "- `text_len_classic` y `word_count_classic`: m√©tricas de longitud del texto procesado.\n",
    "\n",
    "El siguiente paso ser√° usar este fichero en el notebook de **modelado**,\n",
    "donde aplicaremos vectorizaci√≥n (TF-IDF, BoW, etc.) y entrenaremos modelos de clasificaci√≥n.\n",
    "\"\"\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7285333c",
   "metadata": {},
   "source": [
    "# Resumen del notebook de preprocesamiento\n",
    "\n",
    "En este notebook hemos:\n",
    "\n",
    "1. Cargado el dataset original de comentarios de YouTube.\n",
    "2. Definido una funci√≥n de **limpieza b√°sica** (`basic_clean`) que:\n",
    "   - Convierte emojis a texto en ingl√©s.\n",
    "   - Pasa a min√∫sculas.\n",
    "   - Elimina URLs, menciones, hashtags y saltos de l√≠nea.\n",
    "   - Normaliza espacios.\n",
    "3. Configurado un pipeline cl√°sico de NLP en ingl√©s:\n",
    "   - `tokenize` ‚Üí tokeniza el texto.\n",
    "   - `remove_stopwords` ‚Üí elimina stopwords en ingl√©s.\n",
    "   - `lemmatize_tokens` ‚Üí lematiza las palabras usando WordNet.\n",
    "4. Implementado `preprocess_text_classic` para encadenar todos los pasos y obtener:\n",
    "   - `text_classic`: texto limpio, tokenizado, sin stopwords y lematizado.\n",
    "5. Creado una versi√≥n de limpieza ligera (`text_basic`) que puede servir tambi√©n como entrada\n",
    "   para modelos modernos si en el futuro se usan transformers.\n",
    "6. A√±adido columnas de longitud sobre el texto procesado.\n",
    "7. Guardado un CSV preprocesado (`youtoxic_english_1000_clean.csv`) listo para el notebook de modelado.\n",
    "\n",
    "---\n",
    "\n",
    "## Pr√≥ximos pasos\n",
    "\n",
    "En el notebook de **modelado** (por ejemplo, `4.3-modeling-baseline.ipynb`):\n",
    "\n",
    "1. Cargar `youtoxic_english_1000_clean.csv`.\n",
    "2. Elegir la etiqueta objetivo (p. ej. `IsToxic`).\n",
    "3. Hacer un **train/test split**.\n",
    "4. Vectorizar `text_classic` con:\n",
    "   - `CountVectorizer` o `TfidfVectorizer`.\n",
    "5. Entrenar modelos cl√°sicos:\n",
    "   - Logistic Regression, Linear SVM, Naive Bayes, etc.\n",
    "6. Calcular m√©tricas (accuracy, precision, recall, F1, matriz de confusi√≥n).\n",
    "7. Comparar modelos y, a partir de ah√≠, avanzar a ensembles, tuning o incluso modelos modernos.\n",
    "\n",
    "Con estos dos notebooks (4.2-eda y 4.2-preprocessing) ya tienes la base s√≥lida de datos listos para modelar. üí™\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
